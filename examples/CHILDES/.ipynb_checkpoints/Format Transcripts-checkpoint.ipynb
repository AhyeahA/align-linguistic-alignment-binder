{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, re, glob, numpy\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_PATH = '/Users/nduran/Desktop/GitProjects/align-linguistic-alignment/examples/CHILDES/transcripts/'\n",
    "OPUT_PATH = '/Users/nduran/Desktop/GitProjects/align-linguistic-alignment/examples/CHILDES/childes-original/'\n",
    "\n",
    "file_list = os.listdir(INPUT_PATH)\n",
    "for fileName in file_list:  \n",
    "    getFilepart = fileName.split(\".\")\n",
    "    outname = OPUT_PATH + \"time\" + getFilepart[0] + \"-cond\" + \"1\" + \".txt\"\n",
    "    fh = open(outname, \"w\")\n",
    "    fh.writelines([\"participant\",\"\\t\",\"content\",\"\\n\"])\n",
    "#     df = pd.read_csv(INPUT_PATH + fileName, sep='\\t',header=None) \n",
    "    df = pd.read_csv(INPUT_PATH + fileName, sep='\\t') \n",
    "    for value in df.values:  \n",
    "        value = value.tolist()\n",
    "        line = re.sub('1cgv', 'cgv', value[0])\n",
    "        line = re.sub('1kid', 'kid', line)\n",
    "        line1 = line.split(\" \")\n",
    "        line2 = line1[2]\n",
    "        line3 = line1[3:]\n",
    "        line4 = (\" \".join(line3))        \n",
    "        finalList = [line2,\"\\t\",line4,\"\\n\"]\n",
    "        fh.writelines(finalList)\n",
    "    fh.close()      \n",
    "      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas\n",
    "# from pandas import *\n",
    "# import nltk, string, re, math, os, numpy, csv\n",
    "# ##// no need for string if using new whitelist method\n",
    "\n",
    "# INPUT_PATH = '/Users/nduran/Dropbox (ASU)/DA/DA_linguisticAlignment/working/ALIGN_DA_ANALYSIS/'\n",
    "# # FILE_NAME = 'DA_transcripts_agreement.txt' ## ROUND 2\n",
    "# FILE_NAME = 'DA_transcripts_disagreement.txt' ## ROUND 1\n",
    "\n",
    "# ##// this is just to get the data prepped for the starting format in paper\n",
    "# def get_unique(dataframe):\n",
    "#     uniqueDyad = dataframe.dyadNumber.unique()\n",
    "#     uniqueConvo = dataframe.convoCode.unique()\n",
    "#     return uniqueDyad, uniqueConvo\n",
    "\n",
    "# ##// this is just to get the data prepped for the starting format in paper\n",
    "# def order_turns(df1,convo,dyad): \n",
    "#     dA1 = df1.loc[(df1['type'] == 1) & (df1['dyadNumber'] == int(dyad)) & (df1['convoCode'] == int(convo))]    \n",
    "#     dA1 = dA1.sort_values(['begin'], ascending=[True])                \n",
    "#     return dA1\n",
    "\n",
    "# ##// this is just to get the data prepped for the starting format in paper\n",
    "# def remove_fillers(dataframe):\n",
    "# #     WHITELIST = set('abcdefghijklmnopqrstuvwxy ABCDEFGHIJKLMNOPQRSTUVWXYZ') # only include letters and spaces (no numbers, no punctuation, no apostrophes, etc.)\n",
    "#     clean = []\n",
    "#     for value in dataframe['content'].values:        \n",
    "# #         pattern = re.compile('^[m]\\s[mh]+\\s+')\n",
    "# #         clean1 = pattern.match(value)\n",
    "# #         if clean1:\n",
    "# #             print clean1.group(0)\n",
    "            \n",
    "# #         removefiller = re.sub('\\*\\w+\\s[uhmo]+', ' ', value) ##new get rid of all two word sequences starting with an asterisk\n",
    "# #         removefiller = re.sub('\\*\\w+\\s', ' ', removefiller) ##new get rid of all words starting with an asterisk\n",
    "# #         removefiller = re.sub('\\*mm', ' ', removefiller) ##new weirdness for DA data\n",
    "# #         removefiller = re.sub('Mm hm', ' ', removefiller) ##new weirdness for DA data\n",
    "# #         removefiller = re.sub('Mm...', ' ', removefiller) ##new weirdness for DA data\n",
    "                \n",
    "# #         removefiller = re.sub('^[m]+\\s[mh]+', ' ', removefiller) ##new start of string: get rid of all \"m hm\" or \"mm hmm\" sort of sequences\n",
    "# #         removefiller = re.sub('^[m]\\s[mh]+\\s+', ' ', removefiller) ##new start of string: get rid of all \"m hm\" or \"mm hmm\" sort of sequences, but with trailing white spaces\n",
    "        \n",
    "# #         removefiller = re.sub('\\s[m]+\\s[mh]+', ' ', removefiller) ##new within string: get rid of all \"m hm\" or \"mm hmm\" sort of sequences\n",
    "# #         removefiller = re.sub('\\s[m]\\s[mh]+\\s+', ' ', removefiller) ##new within string: get rid of all \"m hm\" or \"mm hmm\" sort of sequences, but with trailing white spaces\n",
    "\n",
    "# #         removefiller = \" \".join(removefiller.split()) ##new            \n",
    "        \n",
    "# #         clean.append(removefiller)\n",
    "#         clean.append(value) \n",
    "        \n",
    "#     dataframe['cleantext'] = clean\n",
    "#     dataframe = dataframe.iloc[:, [9,14]]\n",
    "#     dataframe = dataframe.rename(columns={'speakerDeceive': 'participant', 'cleantext': 'content'})\n",
    "#     return dataframe\n",
    "\n",
    "# def removeNullLines(dataframe,minwords):     \n",
    "#     utteranceLen = []\n",
    "#     for value in dataframe['content'].values:\n",
    "#         if isinstance(value,basestring):\n",
    "#             utteranceLen.append(len(value.split(\" \"))) \n",
    "#         else:\n",
    "#             utteranceLen.append(0)       \n",
    "#     dataframe['utteranceLen'] = utteranceLen        \n",
    "#     dataframe2 = dataframe.loc[(dataframe['utteranceLen'] != 0) & (dataframe['utteranceLen'] > int(minwords))]\n",
    "# #     return dataframe2\n",
    "#     return dataframe2.iloc[:, [0,1]]\n",
    "\n",
    "# dataframe1=read_csv(INPUT_PATH + FILE_NAME, sep='\\t',encoding='utf-8')\n",
    "# [dya, conv] = get_unique(dataframe1)   \n",
    "# for con in conv:\n",
    "#     for dy in dya:\n",
    "#         df2 = order_turns(dataframe1,con,dy)\n",
    "#         if len(df2) > 0: \n",
    "            \n",
    "#             deceptionconvo = df2['convoDeceive'].iloc[1]\n",
    "#             roundconvo = df2['round'].iloc[1]\n",
    "            \n",
    "#             df3 = remove_fillers(df2)\n",
    "# #             df4 = removeNullLines(df3,1)\n",
    "#     #         df5 = AdjacentMerge(df4)\n",
    "            \n",
    "# #             outname = INPUT_PATH + \"TRANSCRIPTS/\" + str(dy) + \"_\" + str(roundconvo) + \"-\" + str(deceptionconvo) + \"-\" + str(con) + \".txt\"\n",
    "# #             df4.to_csv(outname,encoding='utf-8',index=False,sep='\\t') \n",
    "# #             outname = INPUT_PATH + \"TRANSCRIPTS/\" + \"dyad\" + str(dy) + \"_\" + \"cond\" + str(roundconvo) + \"-\" + str(deceptionconvo) + \"-\" + str(con) + \".txt\"\n",
    "#             outname = INPUT_PATH + \"TRANSCRIPTS/\" + str(dy) + \"_\" + str(roundconvo) + \"-\" + str(deceptionconvo) + \"-\" + str(con) + \".txt\"\n",
    "#             df3.to_csv(outname,encoding='utf-8',index=False,sep='\\t')       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
