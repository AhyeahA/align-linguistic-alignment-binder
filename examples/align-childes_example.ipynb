{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALIGN Tutorial Notebook: CHILDES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an introduction to **ALIGN**, \n",
    "a tool for quantifying multi-level linguistic similarity \n",
    "between speakers, using parent-child transcript data from \n",
    "the Kuczaj Corpus \n",
    "(https://childes.talkbank.org/access/Eng-NA/Kuczaj.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method was introduced in \"ALIGN: Analyzing Linguistic Interactions with Generalizable techNiques\" (Duran, Paxton, & Fusaroli, *submitted*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While many studies of interpersonal linguistic alignment\n",
    "compare alignment between different dyads across different\n",
    "conditions (i.e., typically a within- or between-dyads\n",
    "design in which each dyad contributes only one or two \n",
    "conversations), there may also be interest in understanding\n",
    "longer-scale temporal dynamics *within* a given dyad.\n",
    "This tutorial provides an example of how ALIGN may be used\n",
    "to just that end: analyzing how a single dyad's multilevel\n",
    "alignment changes across different conversations held\n",
    "at different points over a longer range of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, this tutorial walks users throuh an analysis of\n",
    "conversations from a single English corpus from the CHILDES \n",
    "database  (MacWhinney, 2000)---specifically, Kuczajâ€™s Abe \n",
    "corpus (Kuczaj, 1976), used under a Creative Commons \n",
    "Attribution-ShareAlike 3.0 Unported License (see GitHub\n",
    "repository or `data/CHILDES` directory for license). \n",
    "We analyze the last 20 conversations in the corpus in order\n",
    "to explore how ALIGN can be used to track multi-level\n",
    "linguistic alignment between a parent and child over time,\n",
    "which may be of interest to developmental language\n",
    "researchers. Specifically, we explore how alignment between a parent\n",
    "and a child changes over a brief span of developmental\n",
    "trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for this tutorial are shipped with the `align`\n",
    "package on PyPI (https://pypi.python.org/pypi/align) and GitHub\n",
    "(https://github.com/nickduran/align-linguistic-alignment/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Getting Started](#Getting-Started)\n",
    "    * [Prerequisites](#Prerequisites)\n",
    "    * [Preparing input data](#Preparing-input-data)\n",
    "    * [Filename conventions](#Filename-conventions)\n",
    "    * [Highest-level functions](#Highest-level-functions)\n",
    "* [Setup](#Setup)\n",
    "    * [Import libraries](#Import-libraries)\n",
    "    * [Specify ALIGN path settings](#Specify-ALIGN-path-settings)\n",
    "* [Phase 1: Prepare transcripts](#Phase-1:-Prepare-transcripts)\n",
    "    * [Preparation settings](#Preparation-settings)\n",
    "    * [Run preparation phase](#Run-preparation-phase)\n",
    "* [Phase 2: Calculate alignment](#Phase-2:-Calculate-alignment)\n",
    "    * [For real data: Alignment calculation settings](#For-real-data:-Alignment-calculation-settings)\n",
    "    * [For real data: Run alignment calculation](#For-real-data:-Run-alignment-calculation)\n",
    "    * [For surrogate data: Alignment calculation settings](#For-surrogate-data:-Alignment-calculation-settings)\n",
    "    * [For surrogate data: Run alignment calculation](#For-surrogate-data:-Run-alignment-calculation)\n",
    "* [ALIGN output overview](#ALIGN-output-overview)\n",
    "    * [Speed calculations](#Speed-calculations)\n",
    "    * [Printouts!](#Printouts!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each input text file needs to contain a single conversation organized in an `N x 2` matrix\n",
    "    * Text file must be tab-delimited.\n",
    "* Each row must correspond to a single conversational turn from a speaker.\n",
    "    * Rows must be temporally ordered based on their occurrence in the conversation.\n",
    "    * Rows must alternate between speakers.\n",
    "* Speaker identifier and content for each turn are divided across two columns.\n",
    "    * Column 1 must have the header `participant`.\n",
    "        * Each cell specifies the speaker.\n",
    "        * Each speaker must have a unique label (e.g., `P1` and `P2`, `0` and `1`).\n",
    "    * Column 2 must have the header `content`.\n",
    "        * Each cell corresponds to the transcribed utterance from the speaker.\n",
    "        * Each cell must end with a newline character: `\\n`\n",
    "* See folder `examples > toy_data-original` in Github repository for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filename conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each conversation text file must be regularly formatted, including a prefix for dyad and a prefix for conversation prior to the identifier for each that are separated by a unique character. By default, ALIGN looks for patterns that follow this convention: `dyad1-condA.txt`\n",
    "    * However, users may choose to include any label for dyad or condition so long as the two labels are distinct from one another and are not subsets of any possible dyad or condition labels. Users may also use any character as a separator so long as it does not occur anywhere else in the filename.\n",
    "    * The chosen file format **must** be used when saving **all** files for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest-level functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given appropriately prepared transcript files, ALIGN can be run in 3 high-level functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`prepare_transcripts`**: Pre-process each standardized \n",
    "conversation, checking it conforms to the requirements. \n",
    "Each utterance is tokenized and lemmatized and has \n",
    "POS tags added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`calculate_alignment`**: Generates turn-level and \n",
    "conversation-level alignment scores (lexical, \n",
    "conceptual, and syntactic) across a range of \n",
    "*n*-gram sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`calculate_baseline_alignment`**: Generate a surrogate corpus\n",
    "and run alignment analysis (using identical specifications \n",
    "from `calculate_alignment`) on it to produce a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages we'll need to run ALIGN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import align, os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `time` so that we can get a sense of how\n",
    "long the ALIGN pipeline takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `warnings` to flag us if required files aren't provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the `rpy2.ipython` Jupyter notebook extension so\n",
    "that we can run R analysis in notebook with a Python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify ALIGN path settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALIGN will need to know where the raw transcripts are stored, where to store the processed data, and where to read in any additional files needed for optional ALIGN parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this tutorial, specify a base path that will serve as our jumping-off point for our saved data. All of the CHILDES data and shipped data will be called from the package directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`BASE_PATH`**: Containing directory for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`CHILDES_EXAMPLE`**: Subdirectories for output and other\n",
    "files for this tutorial. (We'll create a default directory\n",
    "if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHILDES_EXAMPLE = os.path.join(BASE_PATH,\n",
    "                              'examples/CHILDES/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CHILDES_EXAMPLE):\n",
    "    os.makedirs(CHILDES_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`TRANSCRIPTS`**: Path to raw transcript files. Automatically provided by `align`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPTS = align.datasets.CHILDES_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`PREPPED_TRANSCRIPTS`**: Set variable for folder name \n",
    "(as string) for relative location of folder into which \n",
    "prepared transcript files will be saved. (We'll create\n",
    "a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREPPED_TRANSCRIPTS = os.path.join(CHILDES_EXAMPLE,\n",
    "                                   'childes-prepped/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PREPPED_TRANSCRIPTS):\n",
    "    os.makedirs(PREPPED_TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ANALYSIS_READY`**: Set variable for folder name \n",
    "(as string) for relative location of folder into \n",
    "which analysis-ready dataframe files will be saved.\n",
    "(We'll create a default directory if one doesn't\n",
    "already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ANALYSIS_READY = os.path.join(CHILDES_EXAMPLE,\n",
    "                              'childes-analysis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ANALYSIS_READY):\n",
    "    os.makedirs(ANALYSIS_READY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`SURROGATE_TRANSCRIPTS`**: Set variable for folder name \n",
    "(as string) for relative location of folder into which all\n",
    "prepared surrogate transcript files will be saved. (We'll\n",
    "create a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SURROGATE_TRANSCRIPTS = os.path.join(CHILDES_EXAMPLE,\n",
    "                                     'childes-surrogate/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SURROGATE_TRANSCRIPTS):\n",
    "    os.makedirs(SURROGATE_TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths for optional parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`OPTIONAL_PATHS`**: If using Stanford POS tagger or\n",
    "pretrained vectors, the path to these files. If these\n",
    "files are provided in other locations, be sure to\n",
    "change the file paths for them. (We'll create a default\n",
    "directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIONAL_PATHS = os.path.join(CHILDES_EXAMPLE,\n",
    "                             'optional_directories/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OPTIONAL_PATHS):\n",
    "    os.makedirs(OPTIONAL_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stanford POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stanford POS tagger **will not be used** by \n",
    "default in this example. However, you may use them\n",
    "by uncommenting and providing the requested file \n",
    "paths in the cells in this section and then changing \n",
    "the relevant parameters in the ALIGN calls below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired, we could use the Standford part-of-speech \n",
    "tagger along with the Penn part-of-speech tagger\n",
    "(which is always used in ALIGN). To do so, the files\n",
    "will need to be downloaded separately: \n",
    "https://nlp.stanford.edu/software/tagger.shtml#Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`STANFORD_POS_PATH`**: If using Stanford POS tagger\n",
    "with the Penn POS tagger, path to Stanford directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STANFORD_POS_PATH = os.path.join(OPTIONAL_PATHS,\n",
    "#                                  'stanford-postagger-full-2017-06-09/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not STANFORD_POS_PATH:\n",
    "#     warnings.warn('Stanford POS directory not found at the specified '\n",
    "#                   'location. Please update the file path or comment '\n",
    "#                   'out the `STANFORD_POS_PATH` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`STANFORD_LANGUAGE`**: If using Stanford tagger,\n",
    "set language model to be used for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STANFORD_LANGUAGE = os.path.join(OPTIONAL_PATHS,\n",
    "#                                  'english-left3words-distsim.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not STANFORD_LANGUAGE:\n",
    "#     warnings.warn('Stanford tagger language not found at the specified '\n",
    "#                   'location. Please update the file path or comment '\n",
    "#                   'out the `STANFORD_POS_PATH` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News pretrained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google News pretrained vectors **will be used**\n",
    "by default in this example. The file is available for\n",
    "download here: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired, researchers may choose to read in pretrained\n",
    "`word2vec` vectors rather than creating a semantic space\n",
    "from the corpus provided. This may be especially useful \n",
    "for small corpora (i.e., fewer than 30k unique words),\n",
    "although the choice of semantic space corpus should be\n",
    "made with careful consideration about the nature of the\n",
    "linguistic context (for further discussion, see Duran, \n",
    "Paxton, & Fusaroli, *submitted*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`PRETRAINED_INPUT_FILE`**: If using pretrained vectors, path\n",
    "to pretrained vector files. You may choose to download the file\n",
    "directly to this path or to change the path to a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_INPUT_FILE = os.path.join(OPTIONAL_PATHS,\n",
    "                            'GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRETRAINED_INPUT_FILE:\n",
    "    warnings.warn('Google News vector not found at the specified '\n",
    "                  'location. Please update the file path or comment '\n",
    "                  'out the `PRETRAINED_INPUT_FILE` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Phase 1: Prepare transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Phase 1, we take our raw transcripts and get them ready\n",
    "for later ALIGN analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameters that we can set for the\n",
    "`prepare_transcripts()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Prepare transcripts for similarity analysis.\n",
      "\n",
      "    Given individual .txt files of conversations,\n",
      "    return a completely prepared dataframe of transcribed\n",
      "    conversations for later ALIGN analysis, including: text\n",
      "    cleaning, merging adjacent turns, spell-checking,\n",
      "    tokenization, lemmatization, and part-of-speech tagging.\n",
      "    The output serve as the input for later ALIGN\n",
      "    analysis.\n",
      "\n",
      "    input_files : str (directory name) or list of str (file names)\n",
      "        Cleaned files to be analyzed. Behavior governed by `input_as_directory`\n",
      "        parameter as well.\n",
      "\n",
      "    output_file_directory : str\n",
      "        Name of directory where output for individual conversations will be\n",
      "        saved.\n",
      "\n",
      "    training_dictionary : str, optional (default: None)\n",
      "        Specify whether to train the spell-checking dictionary using a\n",
      "        provided file name (str) or the default Project\n",
      "        Gutenberg corpus [http://www.gutenberg.org] (None).\n",
      "\n",
      "    minwords : int, optional (2)\n",
      "        Specify the minimum number of words in a turn. Any turns with fewer\n",
      "        than the minimum number of words will be removed from the corpus.\n",
      "\n",
      "    use_filler_list : list of str, optional (default: None)\n",
      "        Specify whether words should be filtered from all conversations using a\n",
      "        list of filler words (list of str) or using regular expressions to\n",
      "        filter out common filler words (None). Behavior governed by\n",
      "        `filler_regex_and_list` parameter as well.\n",
      "\n",
      "    filler_regex_and_list : boolean, optional (default: False)\n",
      "        If providing a list to `use_filler_list` parameter, specify whether to\n",
      "        use only the provided list (False) or to use both the provided list and\n",
      "        the regular expression filter (True).\n",
      "\n",
      "    add_stanford_tags : boolean, optional (default: False)\n",
      "        Specify whether to return part-of-speech similarity scores based on\n",
      "        Stanford POS tagger in addition to the Penn POS tagger (True) or to\n",
      "        return only POS similarity scores from the Penn tagger (False).\n",
      "\n",
      "    stanford_pos_path : str, optional (default: None)\n",
      "        If Stanford POS tagging is desired, specify local path to Stanford POS\n",
      "        tagger.\n",
      "\n",
      "    stanford_language_path : str, optional (default: None)\n",
      "        If Stanford POS tagging is desired, specify local path to Stanford POS\n",
      "        tagger for the desired language (str) or use the default English tagger\n",
      "        (None).\n",
      "\n",
      "    input_as_directory : boolean, optional (default: True)\n",
      "        Specify whether the value passed to `input_files` parameter should\n",
      "        be read as a directory (True) or a list of files to be processed\n",
      "        (False).\n",
      "\n",
      "    save_concatenated_dataframe : boolean, optional (default: True)\n",
      "        Specify whether to save the individual conversation output data only\n",
      "        as individual files in the `output_file_directory` (False) or to save\n",
      "        the individual files as well as a single concatenated dataframe (True).\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print align.prepare_transcripts.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this demonstration, we'll keep everything as\n",
    "defaults. Among other parameters, this means that:\n",
    "* any turns fewer than 2 words will be removed from the corpus\n",
    " (`minwords=2`),\n",
    "* we'll be using regex to strip out any filler words\n",
    " (e.g., \"uh,\" \"um,\" \"huh\"; `use_filler_list=None`),\n",
    "* we'll be using the Project Gutenberg corpus to create our \n",
    " spell-checker algorithm (`training_dictionary=None`),\n",
    "* we'll rely only on the Penn POS tagger \n",
    " (`add_stanford_tags=False`), and\n",
    "* our data will be saved both as individual conversation files\n",
    " and as a master dataframe of all conversation outputs\n",
    " (`save_concatenated_dataframe=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run preparation phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we prepare our transcripts by reading in individual `.txt`\n",
    "files for each conversation, clean up undesired text and turns,\n",
    "spell-check, tokenize, lemmatize, and add POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_store = align.prepare_transcripts(\n",
    "                        input_files=TRANSCRIPTS,\n",
    "                        output_file_directory=PREPPED_TRANSCRIPTS,\n",
    "                        minwords=2,\n",
    "                        use_filler_list=None,\n",
    "                        training_dictionary=None,\n",
    "                        add_stanford_tags=False,\n",
    "                        save_concatenated_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Calculate alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For real data: Alignment calculation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameters that we can set for the\n",
    "`calculate_alignment()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Calculate lexical, syntactic, and conceptual alignment between speakers.\n",
      "\n",
      "    Given a directory of individual .txt files and the\n",
      "    vocabulary list that have been generated by the `prepare_transcripts`\n",
      "    preparation stage, return multi-level alignment\n",
      "    scores with turn-by-turn and conversation-level metrics.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    input_files : str (directory name) or list of str (file names)\n",
      "        Cleaned files to be analyzed. Behavior governed by `input_as_directory`\n",
      "        parameter as well.\n",
      "\n",
      "    output_file_directory : str\n",
      "        Name of directory where output for individual conversations will be\n",
      "        saved.\n",
      "\n",
      "    semantic_model_input_file : str\n",
      "        Name of file to be used for creating the semantic model. A compatible\n",
      "        file will be saved as an output of `prepare_transcripts()`.\n",
      "\n",
      "    pretrained_input_file : str or None\n",
      "        If using a pretrained vector to create the semantic model, use\n",
      "        name of model here. If not, use None. Behavior governed by\n",
      "        `use_pretrained_vectors` parameter as well.\n",
      "\n",
      "    high_sd_cutoff : int, optional (default: 3)\n",
      "        High-frequency cutoff (in SD over the mean) for lexical items\n",
      "        when creating the semantic model.\n",
      "\n",
      "    low_n_cutoff : int, optional (default: 1)\n",
      "        Low-frequency cutoff (in raw frequency) for lexical items when\n",
      "        creating the semantic models. Items with frequency less than or\n",
      "        equal to the number provided here will be removed. To remove the\n",
      "        low-frequency cutoff, set to 0.\n",
      "\n",
      "    delay : int, optional (default: 1)\n",
      "        Delay (or lag) at which to calculate similarity. A lag of 1 (default)\n",
      "        considers only adjacent turns.\n",
      "\n",
      "    maxngram : int, optional (default: 2)\n",
      "        Maximum n-gram size for calculations. Similarity scores for n-grams\n",
      "        from unigrams to the maximum size specified here will be calculated.\n",
      "\n",
      "    use_pretrained_vectors : boolean, optional (default: True)\n",
      "        Specify whether to use a pretrained gensim model for word2vec\n",
      "        analysis (True) or to construct a new model from the provided corpus\n",
      "        (False). If True, the file name of a valid model must be\n",
      "        provided to the `pretrained_input_file` parameter.\n",
      "\n",
      "    ignore_duplicates : boolean, optional (default: True)\n",
      "        Specify whether to remove exact duplicates when calculating\n",
      "        part-of-speech similarity scores (True) or to retain perfectly\n",
      "        mimicked lexical items for POS similarity calculation (False).\n",
      "\n",
      "    add_stanford_tags : boolean, optional (default: False)\n",
      "        Specify whether to return part-of-speech similarity scores based on\n",
      "        Stanford POS tagger in addition to the Penn POS tagger (True) or to\n",
      "        return only POS similarity scores from the Penn tagger (False).\n",
      "\n",
      "    input_as_directory : boolean, optional (default: True)\n",
      "        Specify whether the value passed to `input_files` parameter should\n",
      "        be read as a directory (True) or a list of files to be processed\n",
      "        (False).\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print align.calculate_alignment.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this tutorial, we'll keep everything as\n",
    "defaults. Among other parameters, this means that we'll:\n",
    "* use only unigrams and bigrams for our *n*-grams\n",
    " (`maxngram=2`),\n",
    "* use pretrained vectors instead of creating our own\n",
    " semantic space, since our tutorial corpus is quite\n",
    " small (`use_pretrained_vectors=True` and\n",
    " `pretrained_file_directory=PRETRAINED_INPUT_FILE`),\n",
    "* ignore exact lexical duplicates when calculating\n",
    " syntactic alignment,\n",
    "* we'll rely only on the Penn POS tagger \n",
    " (`add_stanford_tags=False`), and\n",
    "* implement high- and low-frequency cutoffs to clean\n",
    " our transcript data (`high_sd_cutoff=3` and \n",
    " `low_n_cutoff=1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we calculate a baseline level of alignment,\n",
    "we need to include the same parameter values for any\n",
    "parameters that are present in both `calculate_alignment()`\n",
    "(this step) and `calculate_baseline_alignment()`\n",
    "(next step). As a result, we'll specify these here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set standards to be used for real and surrogate\n",
    "INPUT_FILES = PREPPED_TRANSCRIPTS\n",
    "MAXNGRAM = 2\n",
    "USE_PRETRAINED_VECTORS = True\n",
    "SEMANTIC_MODEL_INPUT_FILE = os.path.join(CHILDES_EXAMPLE,\n",
    "                                         'align_concatenated_dataframe.txt')\n",
    "PRETRAINED_FILE_DRIRECTORY = PRETRAINED_INPUT_FILE\n",
    "ADD_STANFORD_TAGS = False\n",
    "IGNORE_DUPLICATES = True\n",
    "HIGH_SD_CUTOFF = 3\n",
    "LOW_N_CUTOFF = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## For real data: Run alignment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "[turn_real,convo_real]= calculate_alignment(\n",
    "                            input_files=INPUT_FILES,\n",
    "                            maxngram=MAXNGRAM,   \n",
    "                            use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                            pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                            semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                            output_file_directory=ANALYSIS_READY,\n",
    "                            add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                            ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                            high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                            low_n_cutoff=LOW_N_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For surrogate data: Alignment calculation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the surrogate or baseline data, we have many of the same\n",
    "parameters for `calculate_baseline_alignment()` as we do for\n",
    "`calculate_alignment()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Calculate baselines for lexical, syntactic, and conceptual\n",
      "    alignment between speakers.\n",
      "\n",
      "    Given a directory of individual .txt files and the\n",
      "    vocab list that have been generated by the `prepare_transcripts`\n",
      "    preparation stage, return multi-level alignment\n",
      "    scores with turn-by-turn and conversation-level metrics\n",
      "    for surrogate baseline conversations.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    input_files : str (directory name) or list of str (file names)\n",
      "        Cleaned files to be analyzed. Behavior governed by `input_as_directory`\n",
      "        parameter as well.\n",
      "\n",
      "    surrogate_file_directory : str\n",
      "        Name of directory where raw surrogate data will be saved.\n",
      "\n",
      "    output_file_directory : str\n",
      "        Name of directory where output for individual surrogate\n",
      "        conversations will be saved.\n",
      "\n",
      "    semantic_model_input_file : str\n",
      "        Name of file to be used for creating the semantic model. A compatible\n",
      "        file will be saved as an output of `prepare_transcripts()`.\n",
      "\n",
      "    pretrained_input_file : str or None\n",
      "        If using a pretrained vector to create the semantic model, use\n",
      "        name of model here. If not, use None. Behavior governed by\n",
      "        `use_pretrained_vectors` parameter as well.\n",
      "\n",
      "    high_sd_cutoff : int, optional (default: 3)\n",
      "        High-frequency cutoff (in SD over the mean) for lexical items\n",
      "        when creating the semantic model.\n",
      "\n",
      "    low_n_cutoff : int, optional (default: 1)\n",
      "        Low-frequency cutoff (in raw frequency) for lexical items when\n",
      "        creating the semantic models. Items with frequency less than or\n",
      "        equal to the number provided here will be removed. To remove the\n",
      "        low-frequency cutoff, set to 0.\n",
      "\n",
      "    id_separator : str, optional (default: '\\-')\n",
      "        Character separator between the dyad and condition IDs in\n",
      "        original data file names.\n",
      "\n",
      "    condition_label : str, optional (default: 'cond')\n",
      "        String preceding ID for each unique condition. Anything after this\n",
      "        label will be identified as a unique condition ID.\n",
      "\n",
      "    dyad_label : str, optional (default: 'dyad')\n",
      "        String preceding ID for each unique dyad. Anything after this label\n",
      "        will be identified as a unique dyad ID.\n",
      "\n",
      "    all_surrogates : boolean, optional (default: True)\n",
      "        Specify whether to generate all possible surrogates across original\n",
      "        dataset (True) or to generate only a subset of surrogates equal to\n",
      "        the real sample size drawn randomly from all possible surrogates\n",
      "        (False).\n",
      "\n",
      "    keep_original_turn_order : boolean, optional (default: True)\n",
      "        Specify whether to retain original turn ordering when pairing surrogate\n",
      "        dyads (True) or to pair surrogate partners' turns in random order\n",
      "        (False).\n",
      "\n",
      "    delay : int, optional (default: 1)\n",
      "        Delay (or lag) at which to calculate similarity. A lag of 1 (default)\n",
      "        considers only adjacent turns.\n",
      "\n",
      "    maxngram : int, optional (default: 2)\n",
      "        Maximum n-gram size for calculations. Similarity scores for n-grams\n",
      "        from unigrams to the maximum size specified here will be calculated.\n",
      "\n",
      "    use_pretrained_vectors : boolean, optional (default: True)\n",
      "        Specify whether to use a pretrained gensim model for word2vec\n",
      "        analysis. If True, the file name of a valid model must be\n",
      "        provided to the `pretrained_input_file` parameter.\n",
      "\n",
      "    ignore_duplicates : boolean, optional (default: True)\n",
      "        Specify whether to remove exact duplicates when calculating\n",
      "        part-of-speech similarity scores. By default, ignore perfectly\n",
      "        mimicked lexical items for POS similarity calculation.\n",
      "\n",
      "    add_stanford_tags : boolean, optional (default: False)\n",
      "        Specify whether to return part-of-speech similarity scores\n",
      "        based on Stanford POS tagger (in addition to the Penn POS\n",
      "        tagger).\n",
      "\n",
      "    input_as_directory : boolean, optional (default: True)\n",
      "        Specify whether the value passed to `input_files` parameter should\n",
      "        be read as a directory or a list of files to be processed.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print align.calculate_baseline_alignment.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, when calculating the baseline, it is **vital** \n",
    "to include the *same* parameter values for any parameters that \n",
    "are included  in both `calculate_alignment()` and \n",
    "`calculate_baseline_alignment()`. As a result, we re-use those\n",
    "values here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate other possible uses for labels by setting \n",
    "`dyad_label = time`, allowing us to compare alignment over \n",
    "time across the same speakers. We also demonstrate how to \n",
    "generate a subset of surrogate pairings rather than all \n",
    "possible pairings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the parameters that we're re-using from\n",
    "the `calculate_alignment()` values (see above), we'll \n",
    "keep most parameters at their defaults by:\n",
    "* preserving the turn order when creating surrogate\n",
    " pairs (`keep_original_turn_order=True`),\n",
    "* specifying condition with `cond` prefix \n",
    " (`condition_label='cond'`), and\n",
    "* using a hyphen to separate the condition and\n",
    " dyad identifiers (`id_separator='\\-'`).\n",
    " \n",
    "However, we will also change some of these defaults,\n",
    "including:\n",
    "* generating only a subset of surrogate data equal\n",
    " to the size of the real data (`all_surrogates=False`)\n",
    " and\n",
    "* specifying that we'll be shuffling the baseline data\n",
    " by time instead of by dyad (`dyad_label='time'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For surrogate data: Run alignment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "[turn_surrogate,convo_surrogate] = calculate_baseline_alignment(\n",
    "                                    input_files=INPUT_FILES, \n",
    "                                    maxngram=MAXNGRAM,\n",
    "                                    use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                                    pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                                    semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                                    output_file_directory=ANALYSIS_READY,\n",
    "                                    add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                                    ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                                    high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                                    low_n_cutoff=LOW_N_CUTOFF,\n",
    "                                    surrogate_file_directory=SURROGATE_TRANSCRIPTS,\n",
    "                                    all_surrogates=False,\n",
    "                                    keep_original_turn_order=True,\n",
    "                                    id_separator='\\-',\n",
    "                                    dyad_label='time',\n",
    "                                    condition_label='cond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALIGN output overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, let's take a look at how long it takes to run each section. Time is given in seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.50847101211548"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_phase1 - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 2, real data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.47824192047119"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_phase2real - start_phase2real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 2, surrogate data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.40525507926941"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_phase2surrogate - start_phase2surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All phases:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188.39196801185608"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_phase2surrogate - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printouts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Before we go, let's take a look at the output from the real data analyzed at the turn level for each conversation (`turn_real`) and at the conversation level for each dyad (`convo_real`). We'll then look at our surrogate data, analyzed both at the turn level (`turn_surrogate`) and at the conversation level (`convo_surrogate`). In our next step, we would then take these data and plug them into our statistical model of choice, but we'll stop here for the sake of our tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>syntax_penn_tok2</th>\n",
       "      <th>syntax_penn_lem2</th>\n",
       "      <th>lexical_tok2</th>\n",
       "      <th>lexical_lem2</th>\n",
       "      <th>cosine_semanticL</th>\n",
       "      <th>partner_direction</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285198</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.37358</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.154303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.57782</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.672067</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.09245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.597504</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.617649</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168668</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.223091</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.323836</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.283156</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time syntax_penn_tok2 syntax_penn_lem2 lexical_tok2 lexical_lem2  \\\n",
       "0     0                0                0            0            0   \n",
       "1     1                0                0            0            0   \n",
       "2     2         0.154303                0            0            0   \n",
       "3     3                0                0            0            0   \n",
       "4     4         0.111111          0.09245            0            0   \n",
       "5     5         0.222222          0.27735            0            0   \n",
       "6     6                0                0            0            0   \n",
       "7     7                0                0            0            0   \n",
       "8     8                0                0            0            0   \n",
       "9     9                0                0            0            0   \n",
       "\n",
       "  cosine_semanticL partner_direction     condition_info  \n",
       "0         0.285198           cgv>kid  time197-cond1.txt  \n",
       "1          0.37358           kid>cgv  time197-cond1.txt  \n",
       "2          0.57782           cgv>kid  time197-cond1.txt  \n",
       "3         0.672067           kid>cgv  time197-cond1.txt  \n",
       "4         0.597504           cgv>kid  time197-cond1.txt  \n",
       "5         0.617649           kid>cgv  time197-cond1.txt  \n",
       "6         0.168668           cgv>kid  time197-cond1.txt  \n",
       "7         0.223091           kid>cgv  time197-cond1.txt  \n",
       "8         0.323836           cgv>kid  time197-cond1.txt  \n",
       "9         0.283156           kid>cgv  time197-cond1.txt  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>syntax_penn_tok2</th>\n",
       "      <th>syntax_penn_lem2</th>\n",
       "      <th>lexical_tok2</th>\n",
       "      <th>lexical_lem2</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.709110</td>\n",
       "      <td>0.709110</td>\n",
       "      <td>0.099848</td>\n",
       "      <td>0.186072</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.768490</td>\n",
       "      <td>0.768490</td>\n",
       "      <td>0.353514</td>\n",
       "      <td>0.435380</td>\n",
       "      <td>time202-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.744802</td>\n",
       "      <td>0.744802</td>\n",
       "      <td>0.309924</td>\n",
       "      <td>0.356673</td>\n",
       "      <td>time191-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.782399</td>\n",
       "      <td>0.782399</td>\n",
       "      <td>0.353604</td>\n",
       "      <td>0.401469</td>\n",
       "      <td>time209-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.810753</td>\n",
       "      <td>0.810753</td>\n",
       "      <td>0.192589</td>\n",
       "      <td>0.305209</td>\n",
       "      <td>time210-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.766315</td>\n",
       "      <td>0.766315</td>\n",
       "      <td>0.311128</td>\n",
       "      <td>0.365522</td>\n",
       "      <td>time204-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.670246</td>\n",
       "      <td>0.670246</td>\n",
       "      <td>0.164155</td>\n",
       "      <td>0.228145</td>\n",
       "      <td>time196-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.789571</td>\n",
       "      <td>0.789571</td>\n",
       "      <td>0.285261</td>\n",
       "      <td>0.317173</td>\n",
       "      <td>time203-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.741248</td>\n",
       "      <td>0.741248</td>\n",
       "      <td>0.319008</td>\n",
       "      <td>0.383271</td>\n",
       "      <td>time208-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.785440</td>\n",
       "      <td>0.785440</td>\n",
       "      <td>0.188816</td>\n",
       "      <td>0.229783</td>\n",
       "      <td>time205-cond1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   syntax_penn_tok2  syntax_penn_lem2  lexical_tok2  lexical_lem2  \\\n",
       "0          0.709110          0.709110      0.099848      0.186072   \n",
       "1          0.768490          0.768490      0.353514      0.435380   \n",
       "2          0.744802          0.744802      0.309924      0.356673   \n",
       "3          0.782399          0.782399      0.353604      0.401469   \n",
       "4          0.810753          0.810753      0.192589      0.305209   \n",
       "5          0.766315          0.766315      0.311128      0.365522   \n",
       "6          0.670246          0.670246      0.164155      0.228145   \n",
       "7          0.789571          0.789571      0.285261      0.317173   \n",
       "8          0.741248          0.741248      0.319008      0.383271   \n",
       "9          0.785440          0.785440      0.188816      0.229783   \n",
       "\n",
       "      condition_info  \n",
       "0  time197-cond1.txt  \n",
       "1  time202-cond1.txt  \n",
       "2  time191-cond1.txt  \n",
       "3  time209-cond1.txt  \n",
       "4  time210-cond1.txt  \n",
       "5  time204-cond1.txt  \n",
       "6  time196-cond1.txt  \n",
       "7  time203-cond1.txt  \n",
       "8  time208-cond1.txt  \n",
       "9  time205-cond1.txt  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>syntax_penn_tok2</th>\n",
       "      <th>syntax_penn_lem2</th>\n",
       "      <th>lexical_tok2</th>\n",
       "      <th>lexical_lem2</th>\n",
       "      <th>cosine_semanticL</th>\n",
       "      <th>partner_direction</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3444</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.210819</td>\n",
       "      <td>0.111803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.628921</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.105409</td>\n",
       "      <td>0.111803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.619997</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.13484</td>\n",
       "      <td>0.13484</td>\n",
       "      <td>0.686174</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.559554</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.309329</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.35275</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.535742</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.30474</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.456275</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time syntax_penn_tok2 syntax_penn_lem2 lexical_tok2 lexical_lem2  \\\n",
       "0     0                0         0.169031            0            0   \n",
       "1     1         0.210819         0.111803            0            0   \n",
       "2     2         0.105409         0.111803            0            0   \n",
       "3     3                0                0      0.13484      0.13484   \n",
       "4     4                0                0            0            0   \n",
       "5     5                0                0            0            0   \n",
       "6     6                0                0            0            0   \n",
       "7     7                0                0            0            0   \n",
       "8     8                0                0     0.353553     0.353553   \n",
       "9     9                0                0            0            0   \n",
       "\n",
       "  cosine_semanticL partner_direction         condition_info  \n",
       "0           0.3444           cgv>kid  time195-time204-cond1  \n",
       "1         0.628921           kid>cgv  time195-time204-cond1  \n",
       "2         0.619997           cgv>kid  time195-time204-cond1  \n",
       "3         0.686174           kid>cgv  time195-time204-cond1  \n",
       "4         0.559554           cgv>kid  time195-time204-cond1  \n",
       "5         0.309329           kid>cgv  time195-time204-cond1  \n",
       "6          0.35275           cgv>kid  time195-time204-cond1  \n",
       "7         0.535742           kid>cgv  time195-time204-cond1  \n",
       "8          0.30474           cgv>kid  time195-time204-cond1  \n",
       "9         0.456275           kid>cgv  time195-time204-cond1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_surrogate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>syntax_penn_tok2</th>\n",
       "      <th>syntax_penn_lem2</th>\n",
       "      <th>lexical_tok2</th>\n",
       "      <th>lexical_lem2</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.703404</td>\n",
       "      <td>0.703404</td>\n",
       "      <td>0.163967</td>\n",
       "      <td>0.251262</td>\n",
       "      <td>time195-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.767801</td>\n",
       "      <td>0.767801</td>\n",
       "      <td>0.129419</td>\n",
       "      <td>0.157256</td>\n",
       "      <td>time191-time201-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.747546</td>\n",
       "      <td>0.747546</td>\n",
       "      <td>0.102264</td>\n",
       "      <td>0.157979</td>\n",
       "      <td>time210-time197-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.806707</td>\n",
       "      <td>0.806707</td>\n",
       "      <td>0.124169</td>\n",
       "      <td>0.176623</td>\n",
       "      <td>time210-time201-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790831</td>\n",
       "      <td>0.790831</td>\n",
       "      <td>0.130467</td>\n",
       "      <td>0.211696</td>\n",
       "      <td>time204-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.686797</td>\n",
       "      <td>0.686797</td>\n",
       "      <td>0.135402</td>\n",
       "      <td>0.202031</td>\n",
       "      <td>time194-time210-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.727220</td>\n",
       "      <td>0.727220</td>\n",
       "      <td>0.078701</td>\n",
       "      <td>0.103854</td>\n",
       "      <td>time197-time210-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.736246</td>\n",
       "      <td>0.736246</td>\n",
       "      <td>0.081103</td>\n",
       "      <td>0.156581</td>\n",
       "      <td>time195-time197-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.808982</td>\n",
       "      <td>0.808982</td>\n",
       "      <td>0.120128</td>\n",
       "      <td>0.199052</td>\n",
       "      <td>time201-time210-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.710756</td>\n",
       "      <td>0.710756</td>\n",
       "      <td>0.069476</td>\n",
       "      <td>0.119855</td>\n",
       "      <td>time197-time195-cond1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   syntax_penn_tok2  syntax_penn_lem2  lexical_tok2  lexical_lem2  \\\n",
       "0          0.703404          0.703404      0.163967      0.251262   \n",
       "1          0.767801          0.767801      0.129419      0.157256   \n",
       "2          0.747546          0.747546      0.102264      0.157979   \n",
       "3          0.806707          0.806707      0.124169      0.176623   \n",
       "4          0.790831          0.790831      0.130467      0.211696   \n",
       "5          0.686797          0.686797      0.135402      0.202031   \n",
       "6          0.727220          0.727220      0.078701      0.103854   \n",
       "7          0.736246          0.736246      0.081103      0.156581   \n",
       "8          0.808982          0.808982      0.120128      0.199052   \n",
       "9          0.710756          0.710756      0.069476      0.119855   \n",
       "\n",
       "          condition_info  \n",
       "0  time195-time204-cond1  \n",
       "1  time191-time201-cond1  \n",
       "2  time210-time197-cond1  \n",
       "3  time210-time201-cond1  \n",
       "4  time204-time195-cond1  \n",
       "5  time194-time210-cond1  \n",
       "6  time197-time210-cond1  \n",
       "7  time195-time197-cond1  \n",
       "8  time201-time210-cond1  \n",
       "9  time197-time195-cond1  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_surrogate.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
