{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ALIGN Tutorial Notebook: CHILDES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook provides an introduction to **ALIGN**, \n",
    "a tool for quantifying multi-level linguistic similarity \n",
    "between speakers, using parent-child transcript data from \n",
    "the Kuczaj Corpus \n",
    "(https://childes.talkbank.org/access/Eng-NA/Kuczaj.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This method was introduced in \"ALIGN: Analyzing Linguistic Interactions with Generalizable techNiques\" (Duran, Paxton, & Fusaroli, *submitted*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tutorial Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While many studies of interpersonal linguistic alignment\n",
    "compare alignment between different dyads across different\n",
    "conditions (i.e., typically a within- or between-dyads\n",
    "design in which each dyad contributes only one or two \n",
    "conversations), there may also be interest in understanding\n",
    "longer-scale temporal dynamics *within* a given dyad.\n",
    "This tutorial provides an example of how ALIGN may be used\n",
    "to just that end: analyzing how a single dyad's multilevel\n",
    "alignment changes across different conversations held\n",
    "at different points over a longer range of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To do so, this tutorial walks users through an analysis of\n",
    "conversations from a single English corpus from the CHILDES \n",
    "database  (MacWhinney, 2000)---specifically, Kuczajâ€™s Abe \n",
    "corpus (Kuczaj, 1976), used under a Creative Commons \n",
    "Attribution-ShareAlike 3.0 Unported License (see GitHub\n",
    "repository or `data/CHILDES` directory for license). \n",
    "We analyze the last 20 conversations in the corpus in order\n",
    "to explore how ALIGN can be used to track multi-level\n",
    "linguistic alignment between a parent and child over time,\n",
    "which may be of interest to developmental language\n",
    "researchers. Specifically, we explore how alignment between a parent\n",
    "and a child changes over a brief span of developmental\n",
    "trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Data for this tutorial are shipped with the `align`\n",
    "package on PyPI (https://pypi.python.org/pypi/align) and GitHub\n",
    "(https://github.com/nickduran/align-linguistic-alignment/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* [Getting Started](#Getting-Started)\n",
    "    * [Prerequisites](#Prerequisites)\n",
    "    * [Preparing input data](#Preparing-input-data)\n",
    "    * [Filename conventions](#Filename-conventions)\n",
    "    * [Highest-level functions](#Highest-level-functions)\n",
    "* [Setup](#Setup)\n",
    "    * [Import libraries](#Import-libraries)\n",
    "    * [Specify ALIGN path settings](#Specify-ALIGN-path-settings)\n",
    "* [Phase 1: Prepare transcripts](#Phase-1:-Prepare-transcripts)\n",
    "    * [Preparation settings](#Preparation-settings)\n",
    "    * [Run preparation phase](#Run-preparation-phase)\n",
    "* [Phase 2: Calculate alignment](#Phase-2:-Calculate-alignment)\n",
    "    * [For real data: Alignment calculation settings](#For-real-data:-Alignment-calculation-settings)\n",
    "    * [For real data: Run alignment calculation](#For-real-data:-Run-alignment-calculation)\n",
    "    * [For surrogate data: Alignment calculation settings](#For-surrogate-data:-Alignment-calculation-settings)\n",
    "    * [For surrogate data: Run alignment calculation](#For-surrogate-data:-Run-alignment-calculation)\n",
    "* [ALIGN output overview](#ALIGN-output-overview)\n",
    "    * [Speed calculations](#Speed-calculations)\n",
    "    * [Printouts!](#Printouts!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Each input text file needs to contain a single conversation organized in an `N x 2` matrix\n",
    "    * Text file must be tab-delimited.\n",
    "* Each row must correspond to a single conversational turn from a speaker.\n",
    "    * Rows must be temporally ordered based on their occurrence in the conversation.\n",
    "    * Rows must alternate between speakers.\n",
    "* Speaker identifier and content for each turn are divided across two columns.\n",
    "    * Column 1 must have the header `participant`.\n",
    "        * Each cell specifies the speaker.\n",
    "        * Each speaker must have a unique label (e.g., `P1` and `P2`, `0` and `1`).\n",
    "    * Column 2 must have the header `content`.\n",
    "        * Each cell corresponds to the transcribed utterance from the speaker.\n",
    "        * Each cell must end with a newline character: `\\n`\n",
    "* See `examples` directory in Github repository for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filename conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Each conversation text file must be regularly formatted, including a prefix for dyad and a prefix for conversation prior to the identifier for each that are separated by a unique character. By default, ALIGN looks for patterns that follow this convention: `dyad1-condA.txt`\n",
    "    * However, users may choose to include any label for dyad or condition so long as the two labels are distinct from one another and are not subsets of any possible dyad or condition labels. Users may also use any character as a separator so long as it does not occur anywhere else in the filename.\n",
    "    * The chosen file format **must** be used when saving **all** files for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Highest-level functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Given appropriately prepared transcript files, ALIGN can be run in 3 high-level functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`prepare_transcripts`**: Pre-process each standardized \n",
    "conversation, checking it conforms to the requirements. \n",
    "Each utterance is tokenized and lemmatized and has \n",
    "POS tags added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`calculate_alignment`**: Generates turn-level and \n",
    "conversation-level alignment scores (lexical, \n",
    "conceptual, and syntactic) across a range of \n",
    "*n*-gram sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`calculate_baseline_alignment`**: Generate a surrogate corpus\n",
    "and run alignment analysis (using identical specifications \n",
    "from `calculate_alignment`) on it to produce a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import packages we'll need to run ALIGN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import align, os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import `time` so that we can get a sense of how\n",
    "long the ALIGN pipeline takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import `warnings` to flag us if required files aren't provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load in the `rpy2.ipython` Jupyter notebook extension so\n",
    "that we can run R analysis in notebook with a Python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALEX** I don't think this is necessary as the R analysis code is going to be hosted elsewhere. I don't want to deal setting it up to run here in the notebook. Maybe down the road... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Install additional NTLK packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Download some addition `nltk` packages for `align` to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nduran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nduran/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nduran/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALEX** Was throwing import error because of a typo in spelling in \"nltk\" - now fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Specify ALIGN path settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ALIGN will need to know where the raw transcripts are stored, where to store the processed data, and where to read in any additional files needed for optional ALIGN parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Required directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the sake of this tutorial, specify a base path that will serve as our jumping-off point for our saved data. All of the CHILDES data and shipped data will be called from the package directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`BASE_PATH`**: Containing directory for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`CHILDES_EXAMPLE`**: Subdirectories for output and other\n",
    "files for this tutorial. (We'll create a default directory\n",
    "if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CHILDES_EXAMPLE = os.path.join(BASE_PATH,\n",
    "                              'CHILDES/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(CHILDES_EXAMPLE):\n",
    "    os.makedirs(CHILDES_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`TRANSCRIPTS`**: Path to raw transcript files. Automatically provided by `align`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRANSCRIPTS = align.datasets.CHILDES_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`PREPPED_TRANSCRIPTS`**: Set variable for folder name \n",
    "(as string) for relative location of folder into which \n",
    "prepared transcript files will be saved. (We'll create\n",
    "a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PREPPED_TRANSCRIPTS = os.path.join(CHILDES_EXAMPLE,\n",
    "                                   'childes-prepped/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(PREPPED_TRANSCRIPTS):\n",
    "    os.makedirs(PREPPED_TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`ANALYSIS_READY`**: Set variable for folder name \n",
    "(as string) for relative location of folder into \n",
    "which analysis-ready dataframe files will be saved.\n",
    "(We'll create a default directory if one doesn't\n",
    "already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ANALYSIS_READY = os.path.join(CHILDES_EXAMPLE,\n",
    "                              'childes-analysis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(ANALYSIS_READY):\n",
    "    os.makedirs(ANALYSIS_READY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`SURROGATE_TRANSCRIPTS`**: Set variable for folder name \n",
    "(as string) for relative location of folder into which all\n",
    "prepared surrogate transcript files will be saved. (We'll\n",
    "create a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SURROGATE_TRANSCRIPTS = os.path.join(CHILDES_EXAMPLE,\n",
    "                                     'childes-surrogate/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(SURROGATE_TRANSCRIPTS):\n",
    "    os.makedirs(SURROGATE_TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Paths for optional parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`OPTIONAL_PATHS`**: If using Stanford POS tagger or\n",
    "pretrained vectors, the path to these files. If these\n",
    "files are provided in other locations, be sure to\n",
    "change the file paths for them. (We'll create a default\n",
    "directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "OPTIONAL_PATHS = os.path.join(CHILDES_EXAMPLE,\n",
    "                             'optional_directories/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(OPTIONAL_PATHS):\n",
    "    os.makedirs(OPTIONAL_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Stanford POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Stanford POS tagger **will not be used** by \n",
    "default in this example. However, you may use them\n",
    "by uncommenting and providing the requested file \n",
    "paths in the cells in this section and then changing \n",
    "the relevant parameters in the ALIGN calls below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If desired, we could use the Standford part-of-speech \n",
    "tagger along with the Penn part-of-speech tagger\n",
    "(which is always used in ALIGN). To do so, the files\n",
    "will need to be downloaded separately: \n",
    "https://nlp.stanford.edu/software/tagger.shtml#Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`STANFORD_POS_PATH`**: If using Stanford POS tagger\n",
    "with the Penn POS tagger, path to Stanford directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# STANFORD_POS_PATH = os.path.join(OPTIONAL_PATHS,\n",
    "#                                  'stanford-postagger-full-2017-06-09/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# if not STANFORD_POS_PATH:\n",
    "#     warnings.warn('Stanford POS directory not found at the specified '\n",
    "#                   'location. Please update the file path or comment '\n",
    "#                   'out the `STANFORD_POS_PATH` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`STANFORD_LANGUAGE`**: If using Stanford tagger,\n",
    "set language model to be used for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# STANFORD_LANGUAGE = os.path.join(OPTIONAL_PATHS,\n",
    "#                                  'english-left3words-distsim.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# if not STANFORD_LANGUAGE:\n",
    "#     warnings.warn('Stanford tagger language not found at the specified '\n",
    "#                   'location. Please update the file path or comment '\n",
    "#                   'out the `STANFORD_POS_PATH` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Google News pretrained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Google News pretrained vectors **will be used**\n",
    "by default in this example. The file is available for\n",
    "download here: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If desired, researchers may choose to read in pretrained\n",
    "`word2vec` vectors rather than creating a semantic space\n",
    "from the corpus provided. This may be especially useful \n",
    "for small corpora (i.e., fewer than 30k unique words),\n",
    "although the choice of semantic space corpus should be\n",
    "made with careful consideration about the nature of the\n",
    "linguistic context (for further discussion, see Duran, \n",
    "Paxton, & Fusaroli, *submitted*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`PRETRAINED_INPUT_FILE`**: If using pretrained vectors, path\n",
    "to pretrained vector files. You may choose to download the file\n",
    "directly to this path or change the path to a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PRETRAINED_INPUT_FILE = os.path.join(OPTIONAL_PATHS,\n",
    "                            'GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not PRETRAINED_INPUT_FILE:\n",
    "    warnings.warn('Google News vector not found at the specified '\n",
    "                  'location. Please update the file path or comment '\n",
    "                  'out the `PRETRAINED_INPUT_FILE` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phase 1: Prepare transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In Phase 1, we take our raw transcripts and get them ready\n",
    "for later ALIGN analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are a number of parameters that we can set for the\n",
    "`prepare_transcripts()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Prepare transcripts for similarity analysis.\n",
      "\n",
      "    Given individual .txt files of conversations,\n",
      "    return a completely prepared dataframe of transcribed\n",
      "    conversations for later ALIGN analysis, including: text\n",
      "    cleaning, merging adjacent turns, spell-checking,\n",
      "    tokenization, lemmatization, and part-of-speech tagging.\n",
      "    The output serve as the input for later ALIGN\n",
      "    analysis.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    input_files : str (directory name) or list of str (file names)\n",
      "        Raw files to be cleaned. Behavior governed by `input_as_directory`\n",
      "        parameter as well.\n",
      "\n",
      "    output_file_directory : str\n",
      "        Name of directory where output for individual conversations will be\n",
      "        saved.\n",
      "\n",
      "    training_dictionary : str, optional (default: None)\n",
      "        Specify whether to train the spell-checking dictionary using a\n",
      "        provided file name (str) or the default Project\n",
      "        Gutenberg corpus [http://www.gutenberg.org] (None).\n",
      "\n",
      "    minwords : int, optional (2)\n",
      "        Specify the minimum number of words in a turn. Any turns with fewer\n",
      "        than the minimum number of words will be removed from the corpus.\n",
      "        (Note: `minwords` must be equal to or greater than `maxngram` provided\n",
      "        to `calculate_alignment()` and `calculate_baseline_alignment` in later\n",
      "        steps.)\n",
      "\n",
      "    use_filler_list : list of str, optional (default: None)\n",
      "        Specify whether words should be filtered from all conversations using a\n",
      "        list of filler words (list of str) or using regular expressions to\n",
      "        filter out common filler words (None). Behavior governed by\n",
      "        `filler_regex_and_list` parameter as well.\n",
      "\n",
      "    filler_regex_and_list : boolean, optional (default: False)\n",
      "        If providing a list to `use_filler_list` parameter, specify whether to\n",
      "        use only the provided list (False) or to use both the provided list and\n",
      "        the regular expression filter (True).\n",
      "\n",
      "    add_stanford_tags : boolean, optional (default: False)\n",
      "        Specify whether to return part-of-speech similarity scores based on\n",
      "        Stanford POS tagger in addition to the Penn POS tagger (True) or to\n",
      "        return only POS similarity scores from the Penn tagger (False). (Note:\n",
      "        Including Stanford POS tags will lead to a significant increase in\n",
      "        processing time.)\n",
      "\n",
      "    stanford_pos_path : str, optional (default: None)\n",
      "        If Stanford POS tagging is desired, specify local path to Stanford POS\n",
      "        tagger.\n",
      "\n",
      "    stanford_language_path : str, optional (default: None)\n",
      "        If Stanford POS tagging is desired, specify local path to Stanford POS\n",
      "        tagger for the desired language (str) or use the default English tagger\n",
      "        (None).\n",
      "\n",
      "    input_as_directory : boolean, optional (default: True)\n",
      "        Specify whether the value passed to `input_files` parameter should\n",
      "        be read as a directory (True) or a list of files to be processed\n",
      "        (False).\n",
      "\n",
      "    save_concatenated_dataframe : boolean, optional (default: True)\n",
      "        Specify whether to save the individual conversation output data only\n",
      "        as individual files in the `output_file_directory` (False) or to save\n",
      "        the individual files as well as a single concatenated dataframe (True).\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "\n",
      "    prepped_df : Pandas DataFrame\n",
      "        A single concatenated dataframe of all transcripts, ready for\n",
      "        processing with `calculate_alignment()` and\n",
      "        `calculate_baseline_alignment()`.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print align.prepare_transcripts.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the sake of this demonstration, we'll keep everything as\n",
    "defaults. Among other parameters, this means that:\n",
    "* any turns fewer than 2 words will be removed from the corpus\n",
    " (`minwords=2`),\n",
    "* we'll be using regex to strip out any filler words\n",
    " (e.g., \"uh,\" \"um,\" \"huh\"; `use_filler_list=None`),\n",
    "* we'll be using the Project Gutenberg corpus to create our \n",
    " spell-checker algorithm (`training_dictionary=None`),\n",
    "* we'll rely only on the Penn POS tagger \n",
    " (`add_stanford_tags=False`), and\n",
    "* our data will be saved both as individual conversation files\n",
    " and as a master dataframe of all conversation outputs\n",
    " (`save_concatenated_dataframe=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run preparation phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, we prepare our transcripts by reading in individual `.txt`\n",
    "files for each conversation, clean up undesired text and turns,\n",
    "spell-check, tokenize, lemmatize, and add POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/nduran/anaconda/lib/python2.7/site-packages/align/data/gutenberg.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-7504781640a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                         \u001b[0mtraining_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0madd_stanford_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                         save_concatenated_dataframe=True)\n\u001b[0m",
      "\u001b[0;32m/Users/nduran/anaconda/lib/python2.7/site-packages/align/prepare_transcripts.pyc\u001b[0m in \u001b[0;36mprepare_transcripts\u001b[0;34m(input_files, output_file_directory, training_dictionary, minwords, use_filler_list, filler_regex_and_list, add_stanford_tags, stanford_pos_path, stanford_language_path, input_as_directory, save_concatenated_dataframe)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;31m# train our spell-checking model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m     \u001b[0mnwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[a-z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;31m# grab the appropriate files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/nduran/anaconda/lib/python2.7/site-packages/align/data/gutenberg.txt'"
     ]
    }
   ],
   "source": [
    "model_store = align.prepare_transcripts(\n",
    "                        input_files=TRANSCRIPTS,\n",
    "                        output_file_directory=PREPPED_TRANSCRIPTS,\n",
    "                        minwords=2,\n",
    "                        use_filler_list=None,\n",
    "                        training_dictionary=None,\n",
    "                        add_stanford_tags=False,\n",
    "                        save_concatenated_dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALEX** Do you know what is generating this error? It looks like the gutenberg.txt file is not saved with the align package on the pip install. Thanks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phase 2: Calculate alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## For real data: Alignment calculation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are a number of parameters that we can set for the\n",
    "`calculate_alignment()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print align.calculate_alignment.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the sake of this tutorial, we'll keep everything as\n",
    "defaults. Among other parameters, this means that we'll:\n",
    "* use only unigrams and bigrams for our *n*-grams\n",
    " (`maxngram=2`),\n",
    "* use pretrained vectors instead of creating our own\n",
    " semantic space, since our tutorial corpus is quite\n",
    " small (`use_pretrained_vectors=True` and\n",
    " `pretrained_file_directory=PRETRAINED_INPUT_FILE`),\n",
    "* ignore exact lexical duplicates when calculating\n",
    " syntactic alignment,\n",
    "* we'll rely only on the Penn POS tagger \n",
    " (`add_stanford_tags=False`), and\n",
    "* implement high- and low-frequency cutoffs to clean\n",
    " our transcript data (`high_sd_cutoff=3` and \n",
    " `low_n_cutoff=1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Whenever we calculate a baseline level of alignment,\n",
    "we need to include the same parameter values for any\n",
    "parameters that are present in both `calculate_alignment()`\n",
    "(this step) and `calculate_baseline_alignment()`\n",
    "(next step). As a result, we'll specify these here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set standards to be used for real and surrogate\n",
    "INPUT_FILES = PREPPED_TRANSCRIPTS\n",
    "MAXNGRAM = 2\n",
    "USE_PRETRAINED_VECTORS = True\n",
    "SEMANTIC_MODEL_INPUT_FILE = os.path.join(CHILDES_EXAMPLE,\n",
    "                                         'align_concatenated_dataframe.txt')\n",
    "PRETRAINED_FILE_DRIRECTORY = PRETRAINED_INPUT_FILE\n",
    "ADD_STANFORD_TAGS = False\n",
    "IGNORE_DUPLICATES = True\n",
    "HIGH_SD_CUTOFF = 3\n",
    "LOW_N_CUTOFF = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## For real data: Run alignment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[turn_real,convo_real] = align.calculate_alignment(\n",
    "                            input_files=INPUT_FILES,\n",
    "                            maxngram=MAXNGRAM,   \n",
    "                            use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                            pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                            semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                            output_file_directory=ANALYSIS_READY,\n",
    "                            add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                            ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                            high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                            low_n_cutoff=LOW_N_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## For surrogate data: Alignment calculation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the surrogate or baseline data, we have many of the same\n",
    "parameters for `calculate_baseline_alignment()` as we do for\n",
    "`calculate_alignment()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print align.calculate_baseline_alignment.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As mentioned above, when calculating the baseline, it is **vital** \n",
    "to include the *same* parameter values for any parameters that \n",
    "are included  in both `calculate_alignment()` and \n",
    "`calculate_baseline_alignment()`. As a result, we re-use those\n",
    "values here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We demonstrate other possible uses for labels by setting \n",
    "`dyad_label = time`, allowing us to compare alignment over \n",
    "time across the same speakers. We also demonstrate how to \n",
    "generate a subset of surrogate pairings rather than all \n",
    "possible pairings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In addition to the parameters that we're re-using from\n",
    "the `calculate_alignment()` values (see above), we'll \n",
    "keep most parameters at their defaults by:\n",
    "* preserving the turn order when creating surrogate\n",
    " pairs (`keep_original_turn_order=True`),\n",
    "* specifying condition with `cond` prefix \n",
    " (`condition_label='cond'`), and\n",
    "* using a hyphen to separate the condition and\n",
    " dyad identifiers (`id_separator='\\-'`).\n",
    " \n",
    "However, we will also change some of these defaults,\n",
    "including:\n",
    "* generating only a subset of surrogate data equal\n",
    " to the size of the real data (`all_surrogates=False`)\n",
    " and\n",
    "* specifying that we'll be shuffling the baseline data\n",
    " by time instead of by dyad (`dyad_label='time'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## For surrogate data: Run alignment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[turn_surrogate,convo_surrogate] = align.calculate_baseline_alignment(\n",
    "                                    input_files=INPUT_FILES, \n",
    "                                    maxngram=MAXNGRAM,\n",
    "                                    use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                                    pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                                    semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                                    output_file_directory=ANALYSIS_READY,\n",
    "                                    add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                                    ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                                    high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                                    low_n_cutoff=LOW_N_CUTOFF,\n",
    "                                    surrogate_file_directory=SURROGATE_TRANSCRIPTS,\n",
    "                                    all_surrogates=False,\n",
    "                                    keep_original_turn_order=True,\n",
    "                                    id_separator='\\-',\n",
    "                                    dyad_label='time',\n",
    "                                    condition_label='cond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ALIGN output overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Speed calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As promised, let's take a look at how long it takes to run each section. Time is given in seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Phase 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end_phase1 - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Phase 2, real data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end_phase2real - start_phase2real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Phase 2, surrogate data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end_phase2surrogate - start_phase2surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**All phases:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end_phase2surrogate - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Printouts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And that's it! Before we go, let's take a look at the output from the real data analyzed at the turn level for each conversation (`turn_real`) and at the conversation level for each dyad (`convo_real`). We'll then look at our surrogate data, analyzed both at the turn level (`turn_surrogate`) and at the conversation level (`convo_surrogate`). In our next step, we would then take these data and plug them into our statistical model of choice, but we'll stop here for the sake of our tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "turn_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convo_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "turn_surrogate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convo_surrogate.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
