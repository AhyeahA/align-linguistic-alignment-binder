{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ALIGN Tutorial Notebook: DEVIL'S ADVOCATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook provides an introduction to **ALIGN**, \n",
    "a tool for quantifying multi-level linguistic similarity \n",
    "between speakers, using the \"Devil's Advocate\" transcript data reported in Duran, Paxton, and Fusaroli: \"ALIGN: Analyzing Linguistic Interactions with Generalizable techNiques - a Python Library\".  This method was also introduced in Duran, Paxton, and Fusaroli (2019), which can be accessed here for reference: https://osf.io/kx8ur/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tutorial Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Devil's Advocate (\"DA\") study examines interpersonal linguistic alignment between dyads across two conversations where participants either agreed or disagreed with each other (as a randomly assigned between-dyads condition) and where one of the conversations involved the truth and the other deception (as a within-subjects condition), with order of conversations counterbalanced across dyads. \n",
    "\n",
    "**Transcript Data:**\n",
    "\n",
    "The complete de-identified dataset of raw conversational transcripts is hosted on a secure protected access data repository provided by the Inter-university Consortium for Political and Social Research (ICPSR). These transcripts need to be downloaded to use this tutorial. Please click on the link to the ICPSR repository to access: http://dx.doi.org/10.3886/ICPSR37124.v1. \n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "To replicate the results reported in Duran, Paxton, and Fusaroli (2019), or for an example of R code used to analzye the ALIGN output for this dataset, please visit the OSF repository for this project: https://osf.io/3TGUF/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* [Getting Started](#Getting-Started)\n",
    "    * [Prerequisites](#Prerequisites)\n",
    "    * [Preparing input data](#Preparing-input-data)\n",
    "    * [Filename conventions](#Filename-conventions)\n",
    "    * [Highest-level functions](#Highest-level-functions)\n",
    "* [Setup](#Setup)\n",
    "    * [Import libraries](#Import-libraries)\n",
    "    * [Specify ALIGN path settings](#Specify-ALIGN-path-settings)\n",
    "* [Phase 1: Prepare transcripts](#Phase-1:-Prepare-transcripts)\n",
    "    * [Preparation settings](#Preparation-settings)\n",
    "    * [Run preparation phase](#Run-preparation-phase)\n",
    "* [Phase 2: Calculate alignment](#Phase-2:-Calculate-alignment)\n",
    "    * [For real data: Alignment calculation settings](#For-real-data:-Alignment-calculation-settings)\n",
    "    * [For real data: Run alignment calculation](#For-real-data:-Run-alignment-calculation)\n",
    "    * [For surrogate data: Alignment calculation settings](#For-surrogate-data:-Alignment-calculation-settings)\n",
    "    * [For surrogate data: Run alignment calculation](#For-surrogate-data:-Run-alignment-calculation)\n",
    "* [ALIGN output overview](#ALIGN-output-overview)\n",
    "    * [Speed calculations](#Speed-calculations)\n",
    "    * [Printouts!](#Printouts!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**The transcript data used for this analysis adheres to the following requirements:**\n",
    "\n",
    "* Each input text file contains a single conversation organized in an `N x 2` matrix\n",
    "    * Text file must be tab-delimited.\n",
    "* Each row corresponds to a single conversational turn from a speaker.\n",
    "    * Rows must be temporally ordered based on their occurrence in the conversation.\n",
    "    * Rows must alternate between speakers.\n",
    "* Speaker identifier and content for each turn are divided across two columns.\n",
    "    * Column 1 must have the header `participant`.\n",
    "        * Each cell specifies the speaker.\n",
    "        * Each speaker must have a unique label (e.g., `P1` and `P2`, `0` and `1`).\n",
    "            * **NOTE: For the DA dataset, the label with a value of 0 indicates speaker did not receive any special assignment at the start of the experiment, a value of 1 indicates the speaker has been assigned the role of deceiver (i.e., “devil’s advocate) at the start of the experiment.**\n",
    "    * Column 2 must have the header `content`.\n",
    "        * Each cell corresponds to the transcribed utterance from the speaker.\n",
    "        * Each cell must end with a newline character: `\\n`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Filename conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Each conversation text file must be regularly formatted, including a prefix for dyad and a prefix for conversation prior to the identifier for each that are separated by a unique character. By default, ALIGN looks for patterns that follow this convention: `dyad1-condA.txt`\n",
    "    * However, users may choose to include any label for dyad or condition so long as the two labels are distinct from one another and are not subsets of any possible dyad or condition labels. Users may also use any character as a separator so long as it does not occur anywhere else in the filename.\n",
    "    * The chosen file format **must** be used when saving **all** files for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**NOTE: For the DA dataset, each conversation text file is saved in the format of: dyad_condX-Y-Z (e.g., dyad11_cond1-0-2).**\n",
    "\n",
    "Such that for X, Y, and Z condition codes:\n",
    "\n",
    "* X = Indicates whether the conversation involved dyads who agreed or disagreed with each other: value of 1 indicates a disagreement conversation, value of 2 indicates an agreement conversation (e.g., “cond1”)\n",
    "* Y = Indicates whether the conversation involved deception: value of 0 indicates truth, value of 1 indicates deception.\n",
    "* Z = Indicates conversation order. Given each dyad had two conversations: value of 2 indicates the conversation occurred first, value of 3 indicates the conversation occurred last.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Highest-level functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Given appropriately prepared transcript files, ALIGN can be run in 3 high-level functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`prepare_transcripts`**: Pre-process each standardized \n",
    "conversation, checking it conforms to the requirements. \n",
    "Each utterance is tokenized and lemmatized and has \n",
    "POS tags added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`calculate_alignment`**: Generates turn-level and \n",
    "conversation-level alignment scores (lexical, \n",
    "conceptual, and syntactic) across a range of \n",
    "*n*-gram sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`calculate_baseline_alignment`**: Generate a surrogate corpus\n",
    "and run alignment analysis (using identical specifications \n",
    "from `calculate_alignment`) on it to produce a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import packages we'll need to run ALIGN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import align, os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import `time` so that we can get a sense of how\n",
    "long the ALIGN pipeline takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import `warnings` to flag us if required files aren't provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Install additional NTLK packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Download some addition `nltk` packages for `align` to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nduran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nduran/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nduran/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Specify ALIGN path settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ALIGN will need to know where the raw transcripts are stored, where to store the processed data, and where to read in any additional files needed for optional ALIGN parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Required directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the sake of this tutorial, specify a base path that will serve as our jumping-off point for our saved data. All of the shipped data will be called from the package directory but the DA transcripts will need to be added manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`BASE_PATH`**: Containing directory for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`DA_EXAMPLE`**: Subdirectories for output and other\n",
    "files for this tutorial. (We'll create a default directory\n",
    "if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DA_EXAMPLE = os.path.join(BASE_PATH,\n",
    "                              'DA/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DA_EXAMPLE):\n",
    "    os.makedirs(DA_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`TRANSCRIPTS`**: Transcript text files must be first downloaded from the ICPSR repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, set variable for folder name (as string) for relative location of folder into which the downloaded transcript files need to be manually added. (We'll create a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRANSCRIPTS = os.path.join(DA_EXAMPLE,\n",
    "               'DA-transcripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(TRANSCRIPTS):\n",
    "    os.makedirs(TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.listdir(TRANSCRIPTS) :\n",
    "    warnings.warn('DA text files not found at the specified '\n",
    "                  'location. Please download from http://dx.doi.org/10.3886/ICPSR37124.v1 and add to directory.')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`PREPPED_TRANSCRIPTS`**: Set variable for folder name \n",
    "(as string) for relative location of folder into which \n",
    "prepared transcript files will be saved. (We'll create\n",
    "a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PREPPED_TRANSCRIPTS = os.path.join(DA_EXAMPLE,\n",
    "                                   'DA-prepped/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(PREPPED_TRANSCRIPTS):\n",
    "    os.makedirs(PREPPED_TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`ANALYSIS_READY`**: Set variable for folder name \n",
    "(as string) for relative location of folder into \n",
    "which analysis-ready dataframe files will be saved.\n",
    "(We'll create a default directory if one doesn't\n",
    "already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ANALYSIS_READY = os.path.join(DA_EXAMPLE,\n",
    "                              'DA-analysis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(ANALYSIS_READY):\n",
    "    os.makedirs(ANALYSIS_READY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`SURROGATE_TRANSCRIPTS`**: Set variable for folder name \n",
    "(as string) for relative location of folder into which all\n",
    "prepared surrogate transcript files will be saved. (We'll\n",
    "create a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SURROGATE_TRANSCRIPTS = os.path.join(DA_EXAMPLE,\n",
    "                                     'DA-surrogate/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(SURROGATE_TRANSCRIPTS):\n",
    "    os.makedirs(SURROGATE_TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Paths for optional parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`OPTIONAL_PATHS`**: If using Stanford POS tagger or\n",
    "pretrained vectors, the path to these files. If these\n",
    "files are provided in other locations, be sure to\n",
    "change the file paths for them. (We'll create a default\n",
    "directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "OPTIONAL_PATHS = os.path.join(DA_EXAMPLE,\n",
    "                             'optional_directories/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(OPTIONAL_PATHS):\n",
    "    os.makedirs(OPTIONAL_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Stanford POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Stanford POS tagger **will not be used** by \n",
    "default in this example. However, you may use them\n",
    "by uncommenting and providing the requested file \n",
    "paths in the cells in this section and then changing \n",
    "the relevant parameters in the ALIGN calls below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If desired, we could use the Standford part-of-speech \n",
    "tagger along with the Penn part-of-speech tagger\n",
    "(which is always used in ALIGN). To do so, the files\n",
    "will need to be downloaded separately: \n",
    "https://nlp.stanford.edu/software/tagger.shtml#Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`STANFORD_POS_PATH`**: If using Stanford POS tagger\n",
    "with the Penn POS tagger, path to Stanford directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "STANFORD_POS_PATH = os.path.join(OPTIONAL_PATHS,\n",
    "                                 'stanford-postagger-full-2017-06-09/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(STANFORD_POS_PATH) == False:\n",
    "    warnings.warn('Stanford POS directory not found at the specified location. '\n",
    "                      'Please update the file path with the folder that can be directly downloaded here: '\n",
    "                      'https://nlp.stanford.edu/software/stanford-postagger-full-2017-06-09.zip'\n",
    "                      ' . Alternatively, comment out the `STANFORD_POS_PATH` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`STANFORD_LANGUAGE`**: If using Stanford tagger,\n",
    "set language model to be used for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "STANFORD_LANGUAGE = os.path.join(STANFORD_POS_PATH,\n",
    "                                 'models/english-left3words-distsim.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(STANFORD_LANGUAGE) == False:\n",
    "    warnings.warn('Stanford tagger language not found at the specified '\n",
    "                      'location. Please update the file path or comment '\n",
    "                      'out the `STANFORD_POS_PATH` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Google News pretrained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Google News pretrained vectors **will be used**\n",
    "by default in this example. The file is available for\n",
    "download here: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If desired, researchers may choose to read in pretrained\n",
    "`word2vec` vectors rather than creating a semantic space\n",
    "from the corpus provided. This may be especially useful \n",
    "for small corpora (i.e., fewer than 30k unique words),\n",
    "although the choice of semantic space corpus should be\n",
    "made with careful consideration about the nature of the\n",
    "linguistic context (for further discussion, see Duran, \n",
    "Paxton, & Fusaroli, 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**`PRETRAINED_INPUT_FILE`**: If using pretrained vectors, path\n",
    "to pretrained vector files. You may choose to download the file\n",
    "directly to this path or change the path to a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PRETRAINED_INPUT_FILE = os.path.join(OPTIONAL_PATHS,\n",
    "                            'GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(PRETRAINED_INPUT_FILE) == False:\n",
    "    warnings.warn('Google News vector not found at the specified location. '\n",
    "                      'Please update the file path with the .bin file that can be accessed here: '\n",
    "                      'https://code.google.com/archive/p/word2vec/ '\n",
    "                      ' . Alternatively, comment out the `PRETRAINED_INPUT_FILE` information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phase 1: Prepare transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In Phase 1, we take our raw transcripts and get them ready\n",
    "for later ALIGN analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are a number of parameters that we can set for the\n",
    "`prepare_transcripts()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Prepare transcripts for similarity analysis.\n",
      "\n",
      "    Given individual .txt files of conversations,\n",
      "    return a completely prepared dataframe of transcribed\n",
      "    conversations for later ALIGN analysis, including: text\n",
      "    cleaning, merging adjacent turns, spell-checking,\n",
      "    tokenization, lemmatization, and part-of-speech tagging.\n",
      "    The output serve as the input for later ALIGN\n",
      "    analysis.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    input_files : str (directory name) or list of str (file names)\n",
      "        Raw files to be cleaned. Behavior governed by `input_as_directory`\n",
      "        parameter as well.\n",
      "\n",
      "    output_file_directory : str\n",
      "        Name of directory where output for individual conversations will be\n",
      "        saved.\n",
      "\n",
      "    training_dictionary : str, optional (default: None)\n",
      "        Specify whether to train the spell-checking dictionary using a\n",
      "        provided file name (str) or the default Project\n",
      "        Gutenberg corpus [http://www.gutenberg.org] (None).\n",
      "\n",
      "    minwords : int, optional (2)\n",
      "        Specify the minimum number of words in a turn. Any turns with fewer\n",
      "        than the minimum number of words will be removed from the corpus.\n",
      "        (Note: `minwords` must be equal to or greater than `maxngram` provided\n",
      "        to `calculate_alignment()` and `calculate_baseline_alignment` in later\n",
      "        steps.)\n",
      "\n",
      "    use_filler_list : list of str, optional (default: None)\n",
      "        Specify whether words should be filtered from all conversations using a\n",
      "        list of filler words (list of str) or using regular expressions to\n",
      "        filter out common filler words (None). Behavior governed by\n",
      "        `filler_regex_and_list` parameter as well.\n",
      "\n",
      "    filler_regex_and_list : boolean, optional (default: False)\n",
      "        If providing a list to `use_filler_list` parameter, specify whether to\n",
      "        use only the provided list (False) or to use both the provided list and\n",
      "        the regular expression filter (True).\n",
      "\n",
      "    add_stanford_tags : boolean, optional (default: False)\n",
      "        Specify whether to return part-of-speech similarity scores based on\n",
      "        Stanford POS tagger in addition to the Penn POS tagger (True) or to\n",
      "        return only POS similarity scores from the Penn tagger (False). (Note:\n",
      "        Including Stanford POS tags will lead to a significant increase in\n",
      "        processing time.)\n",
      "\n",
      "    stanford_pos_path : str, optional (default: None)\n",
      "        If Stanford POS tagging is desired, specify local path to Stanford POS\n",
      "        tagger.\n",
      "\n",
      "    stanford_language_path : str, optional (default: None)\n",
      "        If Stanford POS tagging is desired, specify local path to Stanford POS\n",
      "        tagger for the desired language (str) or use the default English tagger\n",
      "        (None).\n",
      "\n",
      "    input_as_directory : boolean, optional (default: True)\n",
      "        Specify whether the value passed to `input_files` parameter should\n",
      "        be read as a directory (True) or a list of files to be processed\n",
      "        (False).\n",
      "\n",
      "    save_concatenated_dataframe : boolean, optional (default: True)\n",
      "        Specify whether to save the individual conversation output data only\n",
      "        as individual files in the `output_file_directory` (False) or to save\n",
      "        the individual files as well as a single concatenated dataframe (True).\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "\n",
      "    prepped_df : Pandas DataFrame\n",
      "        A single concatenated dataframe of all transcripts, ready for\n",
      "        processing with `calculate_alignment()` and\n",
      "        `calculate_baseline_alignment()`.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print align.prepare_transcripts.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the sake of this demonstration, we'll keep everything as\n",
    "defaults. Among other parameters, this means that:\n",
    "* any turns fewer than 2 words will be removed from the corpus\n",
    " (`minwords=2`),\n",
    "* we'll be using regex to strip out any filler words\n",
    " (e.g., \"uh,\" \"um,\" \"huh\"; `use_filler_list=None`),\n",
    "* we'll be using the Project Gutenberg corpus to create our \n",
    " spell-checker algorithm (`training_dictionary=None`),\n",
    "* we'll rely only on the Penn POS tagger \n",
    " (`add_stanford_tags=False`), and\n",
    "* our data will be saved both as individual conversation files\n",
    " and as a master dataframe of all conversation outputs\n",
    " (`save_concatenated_dataframe=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run preparation phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, we prepare our transcripts by reading in individual `.txt`\n",
    "files for each conversation, clean up undesired text and turns,\n",
    "spell-check, tokenize, lemmatize, and add POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad65_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad25_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad61_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad18_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad36_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad21_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad42_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad11_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad7_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad55_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad15_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad46_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad56_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad12_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad41_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad38_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad45_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad35_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad66_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad71_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad62_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad26_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad75_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad31_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad9_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad55_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad7_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad46_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad15_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad11_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad42_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad18_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad36_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad58_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad21_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad65_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad25_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad61_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad75_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad31_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad9_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad66_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad35_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad71_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad62_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad26_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad41_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad38_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad45_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad56_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad12_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad74_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad27_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad8_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad67_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad34_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad70_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad30_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad40_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad13_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad53_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad44_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad57_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad54_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad69_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad43_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad60_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad33_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad19_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad64_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad73_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad24_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad44_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad57_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad13_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad40_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad53_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad34_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad67_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad70_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad30_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad27_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad74_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad8_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad73_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad37_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad24_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad33_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad60_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad19_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad64_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad43_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad69_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-transcripts/dyad54_cond2-0-2.txt\n"
     ]
    }
   ],
   "source": [
    "model_store = align.prepare_transcripts(\n",
    "                        input_files=TRANSCRIPTS,\n",
    "                        output_file_directory=PREPPED_TRANSCRIPTS,\n",
    "                        minwords=2,\n",
    "                        use_filler_list=None,\n",
    "                        training_dictionary=None,\n",
    "                        add_stanford_tags=False,\n",
    "                        save_concatenated_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phase 2: Calculate alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## For real data: Alignment calculation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are a number of parameters that we can set for the\n",
    "`calculate_alignment()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Calculate lexical, syntactic, and conceptual alignment between speakers.\n",
      "\n",
      "    Given a directory of individual .txt files and the\n",
      "    vocabulary list that have been generated by the `prepare_transcripts`\n",
      "    preparation stage, return multi-level alignment\n",
      "    scores with turn-by-turn and conversation-level metrics.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    input_files : str (directory name) or list of str (file names)\n",
      "        Cleaned files to be analyzed. Behavior governed by `input_as_directory`\n",
      "        parameter as well.\n",
      "\n",
      "    output_file_directory : str\n",
      "        Name of directory where output for individual conversations will be\n",
      "        saved.\n",
      "\n",
      "    semantic_model_input_file : str\n",
      "        Name of file to be used for creating the semantic model. A compatible\n",
      "        file will be saved as an output of `prepare_transcripts()`.\n",
      "\n",
      "    pretrained_input_file : str or None\n",
      "        If using a pretrained vector to create the semantic model, use\n",
      "        name of model here. If not, use None. Behavior governed by\n",
      "        `use_pretrained_vectors` parameter as well.\n",
      "\n",
      "    high_sd_cutoff : int, optional (default: 3)\n",
      "        High-frequency cutoff (in SD over the mean) for lexical items\n",
      "        when creating the semantic model.\n",
      "\n",
      "    low_n_cutoff : int, optional (default: 1)\n",
      "        Low-frequency cutoff (in raw frequency) for lexical items when\n",
      "        creating the semantic models. Items with frequency less than or\n",
      "        equal to the number provided here will be removed. To remove the\n",
      "        low-frequency cutoff, set to 0.\n",
      "\n",
      "    delay : int, optional (default: 1)\n",
      "        Delay (or lag) at which to calculate similarity. A lag of 1 (default)\n",
      "        considers only adjacent turns.\n",
      "\n",
      "    maxngram : int, optional (default: 2)\n",
      "        Maximum n-gram size for calculations. Similarity scores for n-grams\n",
      "        from unigrams to the maximum size specified here will be calculated.\n",
      "\n",
      "    use_pretrained_vectors : boolean, optional (default: True)\n",
      "        Specify whether to use a pretrained gensim model for word2vec\n",
      "        analysis (True) or to construct a new model from the provided corpus\n",
      "        (False). If True, the file name of a valid model must be\n",
      "        provided to the `pretrained_input_file` parameter.\n",
      "\n",
      "    ignore_duplicates : boolean, optional (default: True)\n",
      "        Specify whether to remove exact duplicates when calculating\n",
      "        part-of-speech similarity scores (True) or to retain perfectly\n",
      "        mimicked lexical items for POS similarity calculation (False).\n",
      "\n",
      "    add_stanford_tags : boolean, optional (default: False)\n",
      "        Specify whether to return part-of-speech similarity scores based on\n",
      "        Stanford POS tagger in addition to the Penn POS tagger (True) or to\n",
      "        return only POS similarity scores from the Penn tagger (False). (Note:\n",
      "        Including Stanford POS tags will lead to a significant increase in\n",
      "        processing time.)\n",
      "\n",
      "    input_as_directory : boolean, optional (default: True)\n",
      "        Specify whether the value passed to `input_files` parameter should\n",
      "        be read as a directory (True) or a list of files to be processed\n",
      "        (False).\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "\n",
      "    real_final_turn_df : Pandas DataFrame\n",
      "        A dataframe of lexical, syntactic, and conceptual alignment scores\n",
      "        between turns at specified delay. `NaN` values will be returned for\n",
      "        turns in which the speaker only produced words that were removed\n",
      "        from the corpus (e.g., too rare or too common words) or words that were\n",
      "        present in the corpus but not in the semantic model.\n",
      "\n",
      "    real_final_convo_df : Pandas DataFrame\n",
      "        A dataframe of lexical, syntactic, and conceptual alignment scores\n",
      "        between participants across the entire conversation.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print align.calculate_alignment.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the sake of this tutorial, we'll keep everything as\n",
    "defaults. Among other parameters, this means that we'll:\n",
    "* use only unigrams and bigrams for our *n*-grams\n",
    " (`maxngram=2`),\n",
    "* use pretrained vectors instead of creating our own\n",
    " semantic space, since our tutorial corpus is quite\n",
    " small (`use_pretrained_vectors=True` and\n",
    " `pretrained_file_directory=PRETRAINED_INPUT_FILE`),\n",
    "* ignore exact lexical duplicates when calculating\n",
    " syntactic alignment,\n",
    "* we'll rely only on the Penn POS tagger \n",
    " (`add_stanford_tags=False`), and\n",
    "* implement high- and low-frequency cutoffs to clean\n",
    " our transcript data (`high_sd_cutoff=3` and \n",
    " `low_n_cutoff=1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Whenever we calculate a baseline level of alignment,\n",
    "we need to include the same parameter values for any\n",
    "parameters that are present in both `calculate_alignment()`\n",
    "(this step) and `calculate_baseline_alignment()`\n",
    "(next step). As a result, we'll specify these here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set standards to be used for real and surrogate\n",
    "INPUT_FILES = PREPPED_TRANSCRIPTS\n",
    "MAXNGRAM = 2\n",
    "USE_PRETRAINED_VECTORS = True\n",
    "SEMANTIC_MODEL_INPUT_FILE = os.path.join(DA_EXAMPLE,\n",
    "                                         'align_concatenated_dataframe.txt')\n",
    "PRETRAINED_FILE_DRIRECTORY = PRETRAINED_INPUT_FILE\n",
    "ADD_STANFORD_TAGS = False\n",
    "IGNORE_DUPLICATES = True\n",
    "HIGH_SD_CUTOFF = 3\n",
    "LOW_N_CUTOFF = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## For real data: Run alignment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad65_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad25_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad61_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad18_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad36_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad21_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad42_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad11_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad7_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad55_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad15_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad46_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad56_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad12_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad41_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad38_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad45_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad35_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad66_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad71_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad62_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad26_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad75_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad31_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad9_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad55_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad7_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad46_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad15_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad11_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad42_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad18_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad36_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad58_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad21_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad65_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad25_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad61_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad75_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad31_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad9_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad66_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad35_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad71_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad62_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad26_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad41_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad38_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad45_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad56_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad12_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad74_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad27_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad8_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad67_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad34_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad70_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad30_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad40_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad13_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad53_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad44_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad57_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad54_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad69_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad43_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad60_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad33_cond1-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad19_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad64_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad73_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad24_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad44_cond2-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad57_cond2-0-3.txt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nduran/anaconda/lib/python2.7/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n",
      "/Users/nduran/anaconda/lib/python2.7/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad13_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad40_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad53_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad34_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad67_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad70_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad30_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad27_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad74_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad8_cond1-0-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad73_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad37_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad24_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad33_cond1-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad60_cond2-1-2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad19_cond1-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad64_cond2-1-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad43_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad69_cond2-0-3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment-testing/examples/DA/DA-prepped/dyad54_cond2-0-2.txt\n"
     ]
    }
   ],
   "source": [
    "[turn_real,convo_real] = align.calculate_alignment(\n",
    "                            input_files=INPUT_FILES,\n",
    "                            maxngram=MAXNGRAM,   \n",
    "                            use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                            pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                            semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                            output_file_directory=ANALYSIS_READY,\n",
    "                            add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                            ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                            high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                            low_n_cutoff=LOW_N_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## For surrogate data: Alignment calculation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the surrogate or baseline data, we have many of the same\n",
    "parameters for `calculate_baseline_alignment()` as we do for\n",
    "`calculate_alignment()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Calculate baselines for lexical, syntactic, and conceptual\n",
      "    alignment between speakers.\n",
      "\n",
      "    Given a directory of individual .txt files and the\n",
      "    vocab list that have been generated by the `prepare_transcripts`\n",
      "    preparation stage, return multi-level alignment\n",
      "    scores with turn-by-turn and conversation-level metrics\n",
      "    for surrogate baseline conversations.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    input_files : str (directory name) or list of str (file names)\n",
      "        Cleaned files to be analyzed. Behavior governed by `input_as_directory`\n",
      "        parameter as well.\n",
      "\n",
      "    surrogate_file_directory : str\n",
      "        Name of directory where raw surrogate data will be saved.\n",
      "\n",
      "    output_file_directory : str\n",
      "        Name of directory where output for individual surrogate\n",
      "        conversations will be saved.\n",
      "\n",
      "    semantic_model_input_file : str\n",
      "        Name of file to be used for creating the semantic model. A compatible\n",
      "        file will be saved as an output of `prepare_transcripts()`.\n",
      "\n",
      "    pretrained_input_file : str or None\n",
      "        If using a pretrained vector to create the semantic model, use\n",
      "        name of model here. If not, use None. Behavior governed by\n",
      "        `use_pretrained_vectors` parameter as well.\n",
      "\n",
      "    high_sd_cutoff : int, optional (default: 3)\n",
      "        High-frequency cutoff (in SD over the mean) for lexical items\n",
      "        when creating the semantic model.\n",
      "\n",
      "    low_n_cutoff : int, optional (default: 1)\n",
      "        Low-frequency cutoff (in raw frequency) for lexical items when\n",
      "        creating the semantic models. Items with frequency less than or\n",
      "        equal to the number provided here will be removed. To remove the\n",
      "        low-frequency cutoff, set to 0.\n",
      "\n",
      "    id_separator : str, optional (default: '\\-')\n",
      "        Character separator between the dyad and condition IDs in\n",
      "        original data file names.\n",
      "\n",
      "    condition_label : str, optional (default: 'cond')\n",
      "        String preceding ID for each unique condition. Anything after this\n",
      "        label will be identified as a unique condition ID.\n",
      "\n",
      "    dyad_label : str, optional (default: 'dyad')\n",
      "        String preceding ID for each unique dyad. Anything after this label\n",
      "        will be identified as a unique dyad ID.\n",
      "\n",
      "    all_surrogates : boolean, optional (default: True)\n",
      "        Specify whether to generate all possible surrogates across original\n",
      "        dataset (True) or to generate only a subset of surrogates equal to\n",
      "        the real sample size drawn randomly from all possible surrogates\n",
      "        (False).\n",
      "\n",
      "    keep_original_turn_order : boolean, optional (default: True)\n",
      "        Specify whether to retain original turn ordering when pairing surrogate\n",
      "        dyads (True) or to pair surrogate partners' turns in random order\n",
      "        (False).\n",
      "\n",
      "    delay : int, optional (default: 1)\n",
      "        Delay (or lag) at which to calculate similarity. A lag of 1 (default)\n",
      "        considers only adjacent turns.\n",
      "\n",
      "    maxngram : int, optional (default: 2)\n",
      "        Maximum n-gram size for calculations. Similarity scores for n-grams\n",
      "        from unigrams to the maximum size specified here will be calculated.\n",
      "\n",
      "    use_pretrained_vectors : boolean, optional (default: True)\n",
      "        Specify whether to use a pretrained gensim model for word2vec\n",
      "        analysis. If True, the file name of a valid model must be\n",
      "        provided to the `pretrained_input_file` parameter.\n",
      "\n",
      "    ignore_duplicates : boolean, optional (default: True)\n",
      "        Specify whether to remove exact duplicates when calculating\n",
      "        part-of-speech similarity scores. By default, ignore perfectly\n",
      "        mimicked lexical items for POS similarity calculation.\n",
      "\n",
      "    add_stanford_tags : boolean, optional (default: False)\n",
      "        Specify whether to return part-of-speech similarity scores\n",
      "        based on Stanford POS tagger (in addition to the Penn POS\n",
      "        tagger).\n",
      "\n",
      "    input_as_directory : boolean, optional (default: True)\n",
      "        Specify whether the value passed to `input_files` parameter should\n",
      "        be read as a directory or a list of files to be processed.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "\n",
      "    surrogate_final_turn_df : Pandas DataFrame\n",
      "        A dataframe of lexical, syntactic, and conceptual alignment scores\n",
      "        between turns at specified delay for surrogate partners. `NaN` values\n",
      "        will be returned for turns in which the speaker only produced words\n",
      "        that were removed from the corpus (e.g., too rare or too common words)\n",
      "        or words that were present in the corpus but not in the semantic model.\n",
      "\n",
      "    surrogate_final_convo_df : Pandas DataFrame\n",
      "        A dataframe of lexical, syntactic, and conceptual alignment scores\n",
      "        between surrogate partners across the entire conversation.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print align.calculate_baseline_alignment.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As mentioned above, when calculating the baseline, it is **vital** \n",
    "to include the *same* parameter values for any parameters that \n",
    "are included  in both `calculate_alignment()` and \n",
    "`calculate_baseline_alignment()`. As a result, we re-use those\n",
    "values here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We demonstrate other possible uses for labels by setting \n",
    "`dyad_label = time`, allowing us to compare alignment over \n",
    "time across the same speakers. We also demonstrate how to \n",
    "generate a subset of surrogate pairings rather than all \n",
    "possible pairings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In addition to the parameters that we're re-using from\n",
    "the `calculate_alignment()` values (see above), we'll \n",
    "keep most parameters at their defaults by:\n",
    "* preserving the turn order when creating surrogate\n",
    " pairs (`keep_original_turn_order=True`),\n",
    "* specifying condition with `cond` prefix \n",
    " (`condition_label='cond'`), and\n",
    "* using a hyphen to separate the condition and\n",
    " dyad identifiers (`id_separator='\\-'`).\n",
    " \n",
    "However, we will also change some of these defaults,\n",
    "including:\n",
    "* generating only a subset of surrogate data equal\n",
    " to the size of the real data (`all_surrogates=False`)\n",
    " and\n",
    "* specifying that we'll be shuffling the baseline data\n",
    " by time instead of by dyad (`dyad_label='time'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## For surrogate data: Run alignment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[turn_surrogate,convo_surrogate] = align.calculate_baseline_alignment(\n",
    "                                    input_files=INPUT_FILES, \n",
    "                                    maxngram=MAXNGRAM,\n",
    "                                    use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                                    pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                                    semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                                    output_file_directory=ANALYSIS_READY,\n",
    "                                    add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                                    ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                                    high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                                    low_n_cutoff=LOW_N_CUTOFF,\n",
    "                                    surrogate_file_directory=SURROGATE_TRANSCRIPTS,\n",
    "                                    all_surrogates=False,\n",
    "                                    keep_original_turn_order=True,\n",
    "                                    id_separator='\\-',\n",
    "                                    dyad_label='time',\n",
    "                                    condition_label='cond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ALIGN output overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Speed calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As promised, let's take a look at how long it takes to run each section. Time is given in seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Phase 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187.42389583587646"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_phase1 - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Phase 2, real data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636.3902680873871"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_phase2real - start_phase2real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Phase 2, surrogate data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.65395307540894"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_phase2surrogate - start_phase2surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**All phases:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1272.4963638782501"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_phase2surrogate - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Printouts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And that's it! Before we go, let's take a look at the output from the real data analyzed at the turn level for each conversation (`turn_real`) and at the conversation level for each dyad (`convo_real`). We'll then look at our surrogate data, analyzed both at the turn level (`turn_surrogate`) and at the conversation level (`convo_surrogate`). In our next step, we would then take these data and plug them into our statistical model of choice. As an example of how this was done for Duran, Paxton, and Fusaroli (under review) please visit: https://osf.io/3TGUF/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>syntax_penn_tok2</th>\n",
       "      <th>syntax_penn_lem2</th>\n",
       "      <th>lexical_tok2</th>\n",
       "      <th>lexical_lem2</th>\n",
       "      <th>cosine_semanticL</th>\n",
       "      <th>partner_direction</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.305861</td>\n",
       "      <td>1&gt;0.0</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.202721</td>\n",
       "      <td>0.196537</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.38781</td>\n",
       "      <td>0&gt;1.0</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.632858</td>\n",
       "      <td>0.68331</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.91538</td>\n",
       "      <td>1&gt;0.0</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.347767</td>\n",
       "      <td>0.387183</td>\n",
       "      <td>0.0492366</td>\n",
       "      <td>0.0492366</td>\n",
       "      <td>0.891923</td>\n",
       "      <td>0&gt;1.0</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.284398</td>\n",
       "      <td>0.292597</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.465361</td>\n",
       "      <td>1&gt;0.0</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.277007</td>\n",
       "      <td>0&gt;1.0</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427911</td>\n",
       "      <td>1&gt;0.0</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0907841</td>\n",
       "      <td>0.126365</td>\n",
       "      <td>0.0445435</td>\n",
       "      <td>0.0416667</td>\n",
       "      <td>0.653616</td>\n",
       "      <td>0&gt;1.0</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.364629</td>\n",
       "      <td>0.496455</td>\n",
       "      <td>0.0575055</td>\n",
       "      <td>0.0537914</td>\n",
       "      <td>0.810211</td>\n",
       "      <td>1&gt;0.0</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.425971</td>\n",
       "      <td>0.508368</td>\n",
       "      <td>0.050637</td>\n",
       "      <td>0.050157</td>\n",
       "      <td>0.824253</td>\n",
       "      <td>0&gt;1.0</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time syntax_penn_tok2 syntax_penn_lem2 lexical_tok2 lexical_lem2  \\\n",
       "0     0                0                0            0            0   \n",
       "1     1         0.202721         0.196537            0            0   \n",
       "2     2         0.632858          0.68331     0.109091     0.127273   \n",
       "3     3         0.347767         0.387183    0.0492366    0.0492366   \n",
       "4     4         0.284398         0.292597            0            0   \n",
       "5     5         0.242536         0.218218            0            0   \n",
       "6     6                0                0            0            0   \n",
       "7     7        0.0907841         0.126365    0.0445435    0.0416667   \n",
       "8     8         0.364629         0.496455    0.0575055    0.0537914   \n",
       "9     9         0.425971         0.508368     0.050637     0.050157   \n",
       "\n",
       "  cosine_semanticL partner_direction        condition_info  \n",
       "0         0.305861             1>0.0  dyad65_cond2-1-3.txt  \n",
       "1          0.38781             0>1.0  dyad65_cond2-1-3.txt  \n",
       "2          0.91538             1>0.0  dyad65_cond2-1-3.txt  \n",
       "3         0.891923             0>1.0  dyad65_cond2-1-3.txt  \n",
       "4         0.465361             1>0.0  dyad65_cond2-1-3.txt  \n",
       "5         0.277007             0>1.0  dyad65_cond2-1-3.txt  \n",
       "6         0.427911             1>0.0  dyad65_cond2-1-3.txt  \n",
       "7         0.653616             0>1.0  dyad65_cond2-1-3.txt  \n",
       "8         0.810211             1>0.0  dyad65_cond2-1-3.txt  \n",
       "9         0.824253             0>1.0  dyad65_cond2-1-3.txt  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>syntax_penn_tok2</th>\n",
       "      <th>syntax_penn_lem2</th>\n",
       "      <th>lexical_tok2</th>\n",
       "      <th>lexical_lem2</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.806825</td>\n",
       "      <td>0.806825</td>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.309366</td>\n",
       "      <td>dyad65_cond2-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.700579</td>\n",
       "      <td>0.700579</td>\n",
       "      <td>0.310319</td>\n",
       "      <td>0.382488</td>\n",
       "      <td>dyad25_cond1-1-2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.679482</td>\n",
       "      <td>0.679482</td>\n",
       "      <td>0.422805</td>\n",
       "      <td>0.469363</td>\n",
       "      <td>dyad61_cond2-1-2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.848431</td>\n",
       "      <td>0.848431</td>\n",
       "      <td>0.248461</td>\n",
       "      <td>0.305112</td>\n",
       "      <td>dyad18_cond1-1-2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.687558</td>\n",
       "      <td>0.687558</td>\n",
       "      <td>0.296837</td>\n",
       "      <td>0.352145</td>\n",
       "      <td>dyad36_cond1-1-2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.812085</td>\n",
       "      <td>0.812085</td>\n",
       "      <td>0.182649</td>\n",
       "      <td>0.212311</td>\n",
       "      <td>dyad21_cond1-1-2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.799601</td>\n",
       "      <td>0.799601</td>\n",
       "      <td>0.303276</td>\n",
       "      <td>0.378844</td>\n",
       "      <td>dyad42_cond2-0-2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.687675</td>\n",
       "      <td>0.687675</td>\n",
       "      <td>0.281711</td>\n",
       "      <td>0.283565</td>\n",
       "      <td>dyad11_cond1-0-2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.775469</td>\n",
       "      <td>0.775469</td>\n",
       "      <td>0.447013</td>\n",
       "      <td>0.529013</td>\n",
       "      <td>dyad7_cond1-1-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.634042</td>\n",
       "      <td>0.634042</td>\n",
       "      <td>0.181008</td>\n",
       "      <td>0.179188</td>\n",
       "      <td>dyad55_cond2-0-3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   syntax_penn_tok2  syntax_penn_lem2  lexical_tok2  lexical_lem2  \\\n",
       "0          0.806825          0.806825      0.294379      0.309366   \n",
       "1          0.700579          0.700579      0.310319      0.382488   \n",
       "2          0.679482          0.679482      0.422805      0.469363   \n",
       "3          0.848431          0.848431      0.248461      0.305112   \n",
       "4          0.687558          0.687558      0.296837      0.352145   \n",
       "5          0.812085          0.812085      0.182649      0.212311   \n",
       "6          0.799601          0.799601      0.303276      0.378844   \n",
       "7          0.687675          0.687675      0.281711      0.283565   \n",
       "8          0.775469          0.775469      0.447013      0.529013   \n",
       "9          0.634042          0.634042      0.181008      0.179188   \n",
       "\n",
       "         condition_info  \n",
       "0  dyad65_cond2-1-3.txt  \n",
       "1  dyad25_cond1-1-2.txt  \n",
       "2  dyad61_cond2-1-2.txt  \n",
       "3  dyad18_cond1-1-2.txt  \n",
       "4  dyad36_cond1-1-2.txt  \n",
       "5  dyad21_cond1-1-2.txt  \n",
       "6  dyad42_cond2-0-2.txt  \n",
       "7  dyad11_cond1-0-2.txt  \n",
       "8   dyad7_cond1-1-3.txt  \n",
       "9  dyad55_cond2-0-3.txt  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_surrogate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_surrogate.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
