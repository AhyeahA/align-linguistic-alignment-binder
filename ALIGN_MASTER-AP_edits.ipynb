{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# To-do list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Things for Alex to do:**\n",
    "* [ ] Handling requirements (after getting them)\n",
    "* [ ] Dockerizing\n",
    "* [ ] Jupyter app-ifying\n",
    "* [ ] Getting Stanford tagger included automatically\n",
    "* [ ] Clean up markdown text (when final notebooks are ready)\n",
    "* [ ] See if I can implement w2v function (https://github.com/a-paxton/Gensim-LSI-Word-Similarities)\n",
    "* [ ] Convert functions into library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Things for Nick to do:**\n",
    "* [x] Implement surrogate to match by conversation order AND conversation type\n",
    "* [x] Make file names more intuitive\n",
    "* [ ] Identify condition/dyad/number flexibly (using regex) - SKIPPED\n",
    "* [x] Allow surrogate baseline to be created using a smaller subset (permutations) â€” 2-3x?\n",
    "* [**???**] Do pip freeze or conda list -e > req.txt\n",
    "* [**???**] Redo analysis with new baseline + consider doing sample-wise shuffled baseline - Have questions on how to proceed\n",
    "* [ ] Go over manuscript again with new baseline + review comments/edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: After going through the notebook, it looks like a *lot* of this could be offloaded to a Python package with good documentation. The benefit there is that the \"guts\" of the notebook can be really streamlined and converted into more of an example file, allowing people to see *what* all of this does rather than *how* it does it. (There will be folks, for instance, who don't know code well enough to want to get into these underlying issues.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ALIGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook provides an introduction to **ALIGN**, a tool for quantifying multi-level linguistic similarity between speakers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Table of Contents**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* [Getting Started](#Getting-Started)\n",
    "    * [Prerequisites](#Prerequisites)\n",
    "    * [Preparing input data](#Preparing-input-data)\n",
    "    * [Filename conventions](#Filename-conventions)\n",
    "    * [User-specified parameters](#User-specified-parameters)\n",
    "    * [Main calls](#Main-calls)\n",
    "* [Setup](#Setup)\n",
    "    * [Import libraries](#Import-libraries)\n",
    "    * [User-specified settings](#User-specified-settings)\n",
    "* [Phase 1: Generate \"prepped\" transcripts](#Phase-1:-Generate-\"prepped\"-transcripts)\n",
    "    * [](#)\n",
    "* [Phase 2: Generate alignment scores](#Phase-2:-Generate-alignment-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Jupyter Notebook with Python 2.7.1.3 kernel\n",
    "* Packages in `requirements.txt`\n",
    "\n",
    "*See notes in \"DISTRIBUTION ISSUES\" Notebook for suggestions on how to package effectively and accomodate Python 3 users*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preparing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Each input text file needs to contain a single conversation organized in an `N x 2` matrix\n",
    "    * Text file must be tab-delimited.\n",
    "* Each row must correspond to a single conversational turn from a speaker.\n",
    "    * Rows must be temporally ordered based on their occurrence in the conversation.\n",
    "    * Rows must alternate between speakers.\n",
    "* Speaker identifier and content for each turn are divided across two columns.\n",
    "    * Column 1 must have the header `participant`.\n",
    "        * Each cell specifies the speaker.\n",
    "        * Each speaker must have a unique label (e.g., `P1` and `P2`, `0` and `1`).\n",
    "    * Column 2 must have the header `content`.\n",
    "        * Each cell corresponds to the transcribed utterance from the speaker.\n",
    "        * Each cell must end with a newline character: `\\n`\n",
    "* See `filename.txt` in Github repository for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Filename conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Each conversation text file needs to be named in the format: `A_B.txt`\n",
    "    * `A` corresponds to the dyad number for that conversation\n",
    "    * `B` corresponding to a condition code for that conversation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### User-specified parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Define input path\n",
    "* Define input folder where original transcripts are located\n",
    "* Define folder to save prepped transcripts \n",
    "* Define folder to save surrogate transcripts \n",
    "* Decide the minimum number of words for each turn\n",
    "    * Default: 1\n",
    "* Decide maximum size for n-gram chunking\n",
    "    * Default: 4\n",
    "* Decide on whether to run the Stanford tagger (time-consuming) along with Penn tagger\n",
    "    * Default: Penn tagger only\n",
    "* Decide on max delay between partner's turns to generate alignment score\n",
    "    * Currently only option is for contiguous turns\n",
    "    * Will be updated in a future version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: The default in surrogate set sounds like it's missing some parts of the sentence. Can you double-check for sense?\n",
    "\n",
    "**ND** I don't think we need this option after all. It does not take long to run the full surrogate set and if users want fewer for analysis, they can remove accordingly when they get to that stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Main calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`PHASE1RUN`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Converts each conversation into standardized format.\n",
    "* Each utterance is tokenized and lemmatized and has POS tags added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`PHASE2RUN_REAL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Generates turn-level and conversation-level alignment scores (lexical, conceptual, and syntactic) across a range of n-gram sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`PHASE2RUN_SURROGATE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Generates a surrogage corpus.\n",
    "* Runs identical analysis as PHASE2RUN_REAL on the surrogage corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, we'll get ready to run ALIGN over our target dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*NOTE: Can include these in the Anaconda \".yml\" file so automatically imported and can be removed from the main notebook here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[To top](#ALIGN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os,re,math,csv,string,random,logging,glob,itertools,operator\n",
    "from os import listdir \n",
    "from os.path import isfile, join \n",
    "from collections import Counter, defaultdict \n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Third-party libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For data analysis and data handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For natural language processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet as wn \n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Don't forget to grab the POS tagger we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /Users/alexandra/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_treebank_pos_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note**: The `StanfordPOSTagger` will be\n",
    "used in conjunction with local folder `stanford-postagger-2015-04-20` and `.jar` file. Both files will be called below, after user-specified folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For building semantic space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## User-specified settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Directories and folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set working directory, in which all notebook and supporting files are located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or \"Pathname for the unzipped project folder\" if going with `anaconda-project.yml` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "INPUT_PATH=os.getcwd()+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set variable for folder name (as string) for relative location of folder containing the original transcript files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRANSCRIPTS = 'toy_data-original/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set variable for folder name (as string) for relative location of folder into which prepared transcript files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PREPPED_TRANSCRIPTS = 'toy_data-prepped/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set variable for folder name (as string) for relative location of folder into which analysis-ready dataframe files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ANALYSIS_READY = 'toy_data-analysis/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set variable for folder name (as string) for relative location of folder into which all prepared surrogate transcript files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SURROGATE_TRANSCRIPTS = 'toy_data-surrogate/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analysis settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set minimum number of words for each turn. (Default: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MINWORDS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set maximum size for n-gram chunking. (Default: 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAXNGRAM = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Decide whether to run the Stanford POS tagger along with Penn POS tagger. Adding the Stanford POS tagger to the Penn tagger will lead to an increase in processing time. (Default: Penn only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: What do the values here (below) mean? I'm interpreting it as `1` means to run Stanford and `0` would mean not to run Stanford. If so, we should include this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "STANFORD = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Decide whether to extract a surrogate set that includes all possible dyad pairs within condition. (Default: extracts a smaller the is 2x the size as the original dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: As I mentioned in the earlier `ALLSURROGATE` description, this seems like there's some words missing above. Also, what does this value mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ALLSURROGATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set max delay between partner's turns when generating alignment score.\n",
    "\n",
    "Currently, the only acceptable value is 1 (i.e., contiguous turns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DELAY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# PHASE1RUN()\n",
    "# PHASE2RUN_REAL()\n",
    "# PHASE2RUN_SURROGATE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: What does the code chunk above do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phase 1: Generate \"prepped\" transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initial clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **[Clean up text](#Clean-up-text)** by removing:\n",
    "    * numbers, punctuation, and other non-ASCII alphabet characters\n",
    "    * common speech fillers (e.g., \"um\", \"huh\") and their derivations\n",
    "    * empty turns that may have inadvertently been included\n",
    "    * user-specified short turns\n",
    "        * Default: removes 1-word turns\n",
    "* **[Merge adjacent turns by the same participant](#Merge-adjacent-turns-by-the-same-participant)** into a single utterance row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[To top](#ALIGN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def InitialCleanup(dataframe,minwords):\n",
    "#     WHITELIST = string.letters + '\\'' + ' '\n",
    "#     clean = []\n",
    "#     utteranceLen = []\n",
    "#     for value in dataframe['content'].values:\n",
    "#         cleantext = ''.join(c for c in value if c in WHITELIST).lower() \n",
    "        \n",
    "#         ## OPTIONAL: remove typical speech fillers, examples: \"um, mm, oh, hm, uhm, uh\" ; additional regular expressions can be added as needed for specific needs\n",
    "#         removefiller = re.sub('^[uhmo]+[mh]+\\s', ' ', cleantext) ## at the start of a string\n",
    "#         removefiller = re.sub('\\s[uhmo]+[mh]+\\s', ' ', removefiller) ## within a string                    \n",
    "#         clean.append(removefiller)        \n",
    "        \n",
    "#         utteranceLen.append(len(re.findall(r'\\w+', removefiller)))\n",
    "    \n",
    "#     ## drop the old \"content\" column and add the clean \"content\" column\n",
    "#     dataframe = dataframe.iloc[:, [0,1]]\n",
    "#     dataframe['content'] = clean\n",
    "    \n",
    "#     ## remove rows that are blank or do not meet \"minwords\" requirement, then drop length column\n",
    "#     dataframe['utteranceLen'] = utteranceLen \n",
    "#     dataframe = dataframe.loc[(dataframe['utteranceLen'] != 0) & (dataframe['utteranceLen'] > int(minwords))]\n",
    "#     dataframe = dataframe.iloc[:, [0,1]]\n",
    "#     return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: The form above doesn't catch fillers that appear at the end of lines, since it requires there to be space before and after fillers -- but `\\n` aren't included in the `WHITELIST` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "... Alternatively, we could add a line before line 6 to swap all newlines for spaces. Otherwise, we might not catch fillers at the very end of the line (since the regex requires us to find a whitespace before and after the filler)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: It looks like we only catch 2-letter fillers. Something like \"uhm\" or \"huh\" wouldn't be caught, so we'll need to update it to catch 3-letter fillers without getting non-filler words. It might be worth our while to create a dictionary of filler words rather than a regex expression.\n",
    "\n",
    "That would probably lead to a performance boost for this, too, since we could do a list comprehension instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_InitialCleanup(dataframe,\n",
    "                        minwords=1,\n",
    "                        remove_regex_fillers=True,\n",
    "                        remove_other_list=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform basic text cleaning to prepare dataframe\n",
    "    for analysis. Remove non-letter/-space characters,\n",
    "    empty turns, turns below a minimum length (default:\n",
    "    1 word), and fillers.\n",
    "    \n",
    "    By default, remove 2-letter fillers through regex.\n",
    "    If desired, skip regex filtering of fillers with\n",
    "    `remove_regex_fillers=FALSE`.\n",
    "    \n",
    "    If desired, remove other words (e.g., fillers) \n",
    "    passed as a list to `remove_other_list` argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    # only allow strings, spaces, and newlines to pass\n",
    "    WHITELIST = string.letters + '\\'' + ' '\n",
    "    clean = []\n",
    "    utteranceLen = []\n",
    "    for value in dataframe['content'].values:\n",
    "        cleantext = ''.join(c for c in value if c in WHITELIST).lower() \n",
    "        \n",
    "        ## OPTIONAL: remove typical speech fillers, examples: \"um, mm, oh, hm, uh\"\n",
    "        if remove_regex_fillers==True:\n",
    "            cleantext = re.sub('(^|\\s)[uhmo]+[mh]+[\\s|$]', ' ', cleantext)\n",
    "        \n",
    "        # optional: remove other words specified in list\n",
    "        if remove_other_list!=None:\n",
    "            cleantext = [word for word in cleantext if word not in other_filler_list]\n",
    "        \n",
    "        # append cleaned lines\n",
    "        clean.append(cleantext)        \n",
    "        utteranceLen.append(len(re.findall(r'\\w+', cleantext)))\n",
    "    \n",
    "    ## drop the old \"content\" column and add the clean \"content\" column\n",
    "    dataframe = dataframe.iloc[:, [0,1]]\n",
    "    dataframe['content'] = clean\n",
    "    \n",
    "    ## remove rows that are blank or do not meet \"minwords\" requirement, then drop length column\n",
    "    dataframe['utteranceLen'] = utteranceLen \n",
    "    dataframe = dataframe.loc[(dataframe['utteranceLen'] != 0) & (dataframe['utteranceLen'] > int(minwords))]\n",
    "    dataframe = dataframe.iloc[:, [0,1]]\n",
    "    \n",
    "    # return the cleaned dataframe\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Merge adjacent turns by the same participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def AdjacentMerge(dataframe):\n",
    "    repeat=1\n",
    "    while repeat==1:\n",
    "        l1=len(dataframe) \n",
    "        DfMerge = []\n",
    "        k = 0\n",
    "        if len(dataframe) > 0:\n",
    "            while k < len(dataframe)-1: \n",
    "                if dataframe['participant'].iloc[k] != dataframe['participant'].iloc[k+1]:\n",
    "                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])         \n",
    "                    k = k + 1\n",
    "                elif dataframe['participant'].iloc[k] == dataframe['participant'].iloc[k+1]:                    \n",
    "                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k] + \" \" + dataframe['content'].iloc[k+1]])           \n",
    "                    k = k + 2   \n",
    "            if k == len(dataframe)-1:\n",
    "                DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])      \n",
    "        \n",
    "        dataframe=pd.DataFrame(DfMerge,columns=('participant','content'))\n",
    "        if l1==len(dataframe): \n",
    "            repeat=0    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare transcript text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **[Check spelling](#Check-spelling)** via a Bayesian spell-checking algorithm (http://norvig.com/spell-correct.html).\n",
    "* **[Tokenize and apply spell correction](#Tokenize-and-apply-spell-correction)** to the original transcript text.\n",
    "* **[Lemmatize](#Lemmatize)** using WordNet-derived categories.\n",
    "* [**Part-of-speech tagging**](#Part-of-speech-tagging) with user-defined tagger(s) on both lemmatized and non-lemmatized tokens.\n",
    "    * Users may choose to use the Penn Treebank POS tagger (default) and/or the Stanford POS tagger (optional). The Penn tagger is more time-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def words(text): return re.findall('[a-z]+', text.lower()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: This is only used in the master function, so I suggest incorporating it into there. See `alex_PHASE1RUN` for a possible implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def train(features): \n",
    "#     model = defaultdict(lambda: 1)\n",
    "#     for f in features:\n",
    "#         model[f] += 1\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: Same as previous comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def edits1(word): \n",
    "#     alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#     splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "#     deletes    = [a + b[1:] for a, b in splits if b]\n",
    "#     transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "#     replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "#     inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "#     return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I'd recommend (for Pythonic purposes) using `string.lowercase` instead of a new string. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def edits1(word): \n",
    "#     splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "#     deletes    = [a + b[1:] for a, b in splits if b]\n",
    "#     transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "#     replaces   = [a + c + b[1:] for a, b in splits for c in string.lowercase if b]\n",
    "#     inserts    = [a + c + b     for a, b in splits for c in string.lowercase]\n",
    "#     return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def known_edits2(word,nwords):\n",
    "#     return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in nwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def known(words,nwords): return set(w for w in words if w in nwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def correct(word,nwords):\n",
    "#     candidates = known([word],nwords) or known(edits1(word),nwords) or known_edits2(word,nwords) or [word]\n",
    "#     return max(candidates, key=nwords.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: For documentation purposes, I'd recommend wrapping all of the functions related to `Tokenize` into the `Tokenize` function. I've proposed a modified version below as `alex_Tokenize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize and apply spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def Tokenize(text,nwords):\n",
    "    \n",
    "#     cleantoken = []\n",
    "#     token = word_tokenize(text)\n",
    "    \n",
    "#     for word in token:\n",
    "#         cleantoken.append(correct(word,nwords))\n",
    "#     return cleantoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_Tokenize(text,nwords):\n",
    "    \"\"\"\n",
    "    Given list of text to be processed and a list \n",
    "    of known words, return a list of edited and \n",
    "    tokenized words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # internal function: identify possible spelling errors for a given word\n",
    "    def edits1(word): \n",
    "        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes    = [a + b[1:] for a, b in splits if b]\n",
    "        transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "        replaces   = [a + c + b[1:] for a, b in splits for c in string.lowercase if b]\n",
    "        inserts    = [a + c + b     for a, b in splits for c in string.lowercase]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    # internal function: identify known edits\n",
    "    def known_edits2(word,nwords):\n",
    "        return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in nwords)\n",
    "\n",
    "    # internal function: identify known words\n",
    "    def known(words,nwords): return set(w for w in words if w in nwords)\n",
    "\n",
    "    # internal function: correct spelling\n",
    "    def correct(word,nwords):\n",
    "        candidates = known([word],nwords) or known(edits1(word),nwords) or known_edits2(word,nwords) or [word]\n",
    "        return max(candidates, key=nwords.get)\n",
    "    \n",
    "    # process all words in the text\n",
    "    cleantoken = []\n",
    "    token = word_tokenize(text)    \n",
    "    for word in token:\n",
    "        cleantoken.append(correct(word,nwords))\n",
    "\n",
    "    # return list of tokenized words\n",
    "    return cleantoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I'm not sure that defining the functions here is helpful to readers trying to figure out what's happening. I think it might lead to more transparent (and possibly shorter) code if we simply move the functions into the loop as plain lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert Penn tagger output into a format that Wordnet\n",
    "    can use in order to properly lemmatize the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create some inner functions for simplicity\n",
    "    def is_noun(tag):\n",
    "        return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    def is_verb(tag):\n",
    "        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    def is_adverb(tag):\n",
    "        return tag in ['RB', 'RBR', 'RBS']\n",
    "    def is_adjective(tag):\n",
    "        return tag in ['JJ', 'JJR', 'JJS']\n",
    "    \n",
    "    # check each tag against all possible categories\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return wn.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: Is line 26 (above) needed, or is that an error? It seems as though it's functioning as an implicit `else`. Is that intentional?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Lemmatize(tokenlist):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    pennPos = nltk.pos_tag(tokenlist) # get the POS tags from Penn Treebank tagset that will be used\n",
    "    words_lemma = []\n",
    "    for item in pennPos: # need to convert these POS tags to a format that wordnet uses to properly lemmatize\n",
    "        words_lemma.append(lemmatizer.lemmatize(item[0],\n",
    "                                                penn_to_wn(item[1])))\n",
    "    return words_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_ApplyTokenLemmatize(df,nwords):\n",
    "    df['token'] = \"\"\n",
    "    df['lemma'] = \"\"\n",
    "    for i in range(0,len(df)):\n",
    "        df['token'].iloc[i]=alex_Tokenize(df['content'].iloc[i],nwords)\n",
    "        df['lemma'].iloc[i]=Lemmatize(df['token'].iloc[i])  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def ImportStanfordTagger():\n",
    "#     \"\"\"\n",
    "#     Import the Stanford POS Tagger included in the package.\n",
    "#     \"\"\"\n",
    "#     stantagger = StanfordPOSTagger(INPUT_PATH + 'stanford-postagger-2015-04-20/models/english-bidirectional-distsim.tagger',\n",
    "#                                    INPUT_PATH + 'stanford-postagger-2015-04-20/stanford-postagger.jar')\n",
    "#     return stantagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: `ImportStanfordTagger` is a very targeted function that's only used in (as far as I can see) one other function. I think we should remove it and instead incorporate it into the POS tagger as an optional argument (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_ApplyPOSTagging(df,filename,add_stanford_tags=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Apply part-of-speech tagging to a dataframe of conversation turns \n",
    "    (df). Pass filename as a string to create create a new df variable. \n",
    "    By default, return only tags from the Penn POS tagger. Optionally,\n",
    "    also return Stanford POS tagger results by setting  \n",
    "    `add_stanford_tags=1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create new columns in our dataframe\n",
    "    df['tagged_penn_token'] = \"\"\n",
    "    df['tagged_penn_lemma'] = \"\"\n",
    "    df['file'] = \"\"\n",
    "    if add_stanford_tags == 1:\n",
    "        df['tagged_stan_token'] = \"\"\n",
    "        df['tagged_stan_lemma'] = \"\"\n",
    "        \n",
    "    # if desired, import Stanford tagger\n",
    "    if add_stanford_tags == 1:\n",
    "        stanford_tagger = StanfordPOSTagger(INPUT_PATH + 'stanford-postagger-2015-04-20/models/english-bidirectional-distsim.tagger',\n",
    "                                            INPUT_PATH + 'stanford-postagger-2015-04-20/stanford-postagger.jar')\n",
    "    \n",
    "    # cycle through each line in the dataframe\n",
    "    for i in range(0,len(df)):\n",
    "        df['file'].iloc[i]=filename\n",
    "\n",
    "        # by default, tag with Penn POS tagger\n",
    "        pos_penn_token=nltk.pos_tag(df['token'].iloc[i])\n",
    "        df['tagged_penn_token'].iloc[i]=pos_penn_token \n",
    "        pos_penn_lemma=nltk.pos_tag(df['lemma'].iloc[i])\n",
    "        df['tagged_penn_lemma'].iloc[i]=pos_penn_lemma \n",
    "\n",
    "        # if desired, also tag with Stanford tagger\n",
    "        if add_stanford_tags == 1:\n",
    "            pos_stan_token=stanford_tagger.tag(df['token'].iloc[i])\n",
    "            df['tagged_stan_token'].iloc[i]=pos_stan_token    \n",
    "            pos_stan_lemma=stanford_tagger.tag(df['lemma'].iloc[i])\n",
    "            df['tagged_stan_lemma'].iloc[i]=pos_stan_lemma  \n",
    "\n",
    "    # return finished dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I updated the above with some new variable names, combining the guts of `ImportStanfordTagger` into the function, and more comments. It also no longer returns the Stanford columns if the user opts out of using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RUN Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* For each original transcript file, saves new file with columns for:\n",
    "    * \"Clean\" text\n",
    "    * Tokenized words\n",
    "    * Tokenized lemmatized-words\n",
    "    * Penn POS-tagging on tokenized words\n",
    "    * Penn POS-tagging on lemmatized-words\n",
    "    * Stanford POS-tagging on tokenized words\n",
    "    * Stanford POS-tagging on lemmatized-words\n",
    "* Also saves a single datasheet with all tokenized lemmatized utterances from all transcripts as individual rows\n",
    "    * called forSemantic.txt\n",
    "    * to be used in building semantic space for Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def PHASE1RUN():   \n",
    "\n",
    "#     # import the POS taggers and spell checking\n",
    "#     st = ImportStanfordTagger()\n",
    "#     nwords = train(words(file(INPUT_PATH + 'big.txt').read()))\n",
    "    \n",
    "#     # grab filenames\n",
    "#     filesList = [ f for f in listdir(INPUT_PATH + TRANSCRIPTS) if isfile(join(INPUT_PATH + TRANSCRIPTS,f)) ]\n",
    "# #     filesList=filesList[19:21] ##// run all files   \n",
    "#     filesList=filesList[1:]\n",
    "    \n",
    "#     # cycle through all files \n",
    "#     main = []\n",
    "#     for fileName in filesList:      \n",
    "#         print fileName\n",
    "#         dataframe = pd.read_csv(INPUT_PATH + TRANSCRIPTS + fileName, sep='\\t',encoding='utf-8')\n",
    "#         dataframe = InitialCleanup(dataframe,MINWORDS)\n",
    "#         dataframe = AdjacentMerge(dataframe)\n",
    "#         dataframe = ApplyTokenLemmatize(dataframe,nwords)\n",
    "#         dataframe = ApplyPOSTagging(dataframe, fileName, st)\n",
    "        \n",
    "#         # export the dataframe as a CSV\n",
    "#         dataframe.to_csv(INPUT_PATH + PREPPED_TRANSCRIPTS + fileName, encoding='utf-8',index=False,sep='\\t')\n",
    "#         main.append(dataframe)\n",
    "#         dataframe=None\n",
    "\n",
    "#     # return the finished dataframe\n",
    "#     main = pd.concat(main, axis=0)\n",
    "#     return main\n",
    "# #     main.to_csv(INPUT_PATH  + \"forSemantic.txt\",encoding='utf-8',index=False,sep='\\t') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I've got a few cells below where I tried to pull back the function to examine its guts. I made a few notes along the way in markdown cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#     # import the POS taggers and spell checking\n",
    "#     st = ImportStanfordTagger()\n",
    "#     nwords = train(words(file(INPUT_PATH + 'big.txt').read()))\n",
    "    \n",
    "#     # grab filenames\n",
    "#     filesList = [ f for f in listdir(INPUT_PATH + TRANSCRIPTS) if isfile(join(INPUT_PATH + TRANSCRIPTS,f)) ]\n",
    "# #     filesList=filesList[19:21] ##// run all files   \n",
    "#     filesList=filesList[1:]\n",
    "    \n",
    "#     # cycle through all files \n",
    "#     main = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fileName=filesList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#         print fileName        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#         dataframe = pd.read_csv(INPUT_PATH + TRANSCRIPTS + fileName, sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#         dataframe = InitialCleanup(dataframe,MINWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: Yeah, it looks like the current regex isn't capturing end-of-turn fillers. We need to update the code to address that (see suggestions above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#         dataframe = AdjacentMerge(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#         dataframe = ApplyTokenLemmatize(dataframe,nwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: In inspecting the output of the tagger, it looks like it's accidentally tagging the possessive `'s` as the verb contraction. We should think about what we're doing there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#         dataframe = ApplyPOSTagging(dataframe, fileName, st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PHASE1RUN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I may have just missed it, but I thought that the Stanford tagger was option (cf. line 4). Is there an option or something that I'm missing somewhere?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: Are lines 8 and 10 both necessary? Moreover, line 10 starts from the beginning of an R index (i.e., 1) rather than the Pythonic index start (i.e., 0). It seems like we should just remove line 10... or specify an option in the fuction to start from a certain file index number (again, keeping in mind the Pythonic numbering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: Is line 9 legacy code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: Since we rename `dataframe` by loading in a new file each time, I don't think we need line 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: From the text box above this, it looks like we should be uncommenting line 30? Alternatively, we could assign it to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_PHASE1RUN(input_file_directory, \n",
    "                   output_file_directory,\n",
    "                   training_dictionary=INPUT_PATH+'big.txt',\n",
    "                   add_stanford_tagger=0):   \n",
    "\n",
    "    \"\"\"\n",
    "    Given a directory of individual .txt files, \n",
    "    return a completely prepared dataframe of transcribed \n",
    "    conversations for later ALIGN analysis, including: text \n",
    "    cleaning, merging adjacent turns, spell-checking, \n",
    "    tokenization, lemmatization, and part-of-speech tagging. \n",
    "    By default, return only the Penn Treebank \n",
    "    POS tagger values; optionally, also return Stanford POS tagger\n",
    "    values with `add_standford_tagger=1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create an internal function to train the model\n",
    "    def train(features): \n",
    "        model = defaultdict(lambda: 1)\n",
    "        for f in features:\n",
    "            model[f] += 1\n",
    "        return model\n",
    "        \n",
    "    # train our spell-checking model\n",
    "    nwords = train(re.findall('[a-z]+',(file(training_dictionary).read().lower())))\n",
    "    \n",
    "    # cycle through all files \n",
    "    import glob\n",
    "    file_list = glob.glob(input_file_directory+\"*.txt\")\n",
    "    main = []\n",
    "    for fileName in file_list:      \n",
    "        \n",
    "        # let us know which file we're processing\n",
    "        dataframe = pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        print \"Processing: \"+fileName\n",
    "\n",
    "        # clean up, merge, spellcheck, tokenize, lemmatize, and POS-tag\n",
    "        dataframe = alex_InitialCleanup(dataframe,MINWORDS)\n",
    "        dataframe = AdjacentMerge(dataframe)\n",
    "        dataframe = alex_ApplyTokenLemmatize(dataframe,nwords)\n",
    "        dataframe = alex_ApplyPOSTagging(dataframe, \n",
    "                                    os.path.basename(fileName), \n",
    "                                    add_stanford_tagger)\n",
    "        \n",
    "        # export the conversation's dataframe as a CSV\n",
    "        dataframe.to_csv(output_file_directory + os.path.basename(fileName), \n",
    "                         encoding='utf-8',index=False,sep='\\t')\n",
    "        main.append(dataframe)\n",
    "\n",
    "    # save the concatenated dataframe\n",
    "    main = pd.concat(main, axis=0)\n",
    "    main.to_csv(output_file_directory +'../' + \"align_concatenated_dataframe.txt\",encoding='utf-8',\n",
    "                index=False, sep='\\t')\n",
    "    \n",
    "    # return the dataframe\n",
    "    return main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: Here's how it would run in our context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_10-condition_1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandra/anaconda3/envs/py2/lib/python2.7/site-packages/pandas/core/indexing.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_10-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_12-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_12-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_15-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_15-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_13-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_13-condition_2.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>content</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tagged_penn_token</th>\n",
       "      <th>tagged_penn_lemma</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing uh</td>\n",
       "      <td>[what, are, we, doing, up]</td>\n",
       "      <td>[what, be, we, do, up]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VB...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP), (...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, is, test, some, more]</td>\n",
       "      <td>[let, be, test, some, more]</td>\n",
       "      <td>[(let, NN), (is, VBZ), (test, NN), (some, DT),...</td>\n",
       "      <td>[(let, NN), (be, VB), (test, NN), (some, DT), ...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing this is just some sample text</td>\n",
       "      <td>[and, more, testing, this, is, just, some, sam...</td>\n",
       "      <td>[and, more, testing, this, be, just, some, sam...</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN), (this,...</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN), (this,...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, is, test, some, more]</td>\n",
       "      <td>[let, be, test, some, more]</td>\n",
       "      <td>[(let, NN), (is, VBZ), (test, NN), (some, DT),...</td>\n",
       "      <td>[(let, NN), (be, VB), (test, NN), (some, DT), ...</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing</td>\n",
       "      <td>[more, testing]</td>\n",
       "      <td>[more, testing]</td>\n",
       "      <td>[(more, JJR), (testing, NN)]</td>\n",
       "      <td>[(more, JJR), (testing, NN)]</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>this is just some sample text</td>\n",
       "      <td>[this, is, just, some, sample, text]</td>\n",
       "      <td>[this, be, just, some, sample, text]</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, NN), (just, RB), (some, DT),...</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing uh</td>\n",
       "      <td>[what, are, we, doing, up]</td>\n",
       "      <td>[what, be, we, do, up]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VB...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP), (...</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>this is just some sample text</td>\n",
       "      <td>[this, is, just, some, sample, text]</td>\n",
       "      <td>[this, be, just, some, sample, text]</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, NN), (just, RB), (some, DT),...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, is, test, some, more]</td>\n",
       "      <td>[let, be, test, some, more]</td>\n",
       "      <td>[(let, NN), (is, VBZ), (test, NN), (some, DT),...</td>\n",
       "      <td>[(let, NN), (be, VB), (test, NN), (some, DT), ...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing uh</td>\n",
       "      <td>[what, are, we, doing, up]</td>\n",
       "      <td>[what, be, we, do, up]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VB...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP), (...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing uh</td>\n",
       "      <td>[what, are, we, doing, up]</td>\n",
       "      <td>[what, be, we, do, up]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VB...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP), (...</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, is, test, some, more]</td>\n",
       "      <td>[let, be, test, some, more]</td>\n",
       "      <td>[(let, NN), (is, VBZ), (test, NN), (some, DT),...</td>\n",
       "      <td>[(let, NN), (be, VB), (test, NN), (some, DT), ...</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing uh</td>\n",
       "      <td>[what, are, we, doing, up]</td>\n",
       "      <td>[what, be, we, do, up]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VB...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP), (...</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, is, test, some, more]</td>\n",
       "      <td>[let, be, test, some, more]</td>\n",
       "      <td>[(let, NN), (is, VBZ), (test, NN), (some, DT),...</td>\n",
       "      <td>[(let, NN), (be, VB), (test, NN), (some, DT), ...</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>this is just some sample text</td>\n",
       "      <td>[this, is, just, some, sample, text]</td>\n",
       "      <td>[this, be, just, some, sample, text]</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, NN), (just, RB), (some, DT),...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, is, test, some, more]</td>\n",
       "      <td>[let, be, test, some, more]</td>\n",
       "      <td>[(let, NN), (is, VBZ), (test, NN), (some, DT),...</td>\n",
       "      <td>[(let, NN), (be, VB), (test, NN), (some, DT), ...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing uh</td>\n",
       "      <td>[what, are, we, doing, up]</td>\n",
       "      <td>[what, be, we, do, up]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VB...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP), (...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing uh</td>\n",
       "      <td>[what, are, we, doing, up]</td>\n",
       "      <td>[what, be, we, do, up]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VB...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP), (...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, is, test, some, more]</td>\n",
       "      <td>[let, be, test, some, more]</td>\n",
       "      <td>[(let, NN), (is, VBZ), (test, NN), (some, DT),...</td>\n",
       "      <td>[(let, NN), (be, VB), (test, NN), (some, DT), ...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing this is just some sample text</td>\n",
       "      <td>[and, more, testing, this, is, just, some, sam...</td>\n",
       "      <td>[and, more, testing, this, be, just, some, sam...</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN), (this,...</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN), (this,...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, is, test, some, more]</td>\n",
       "      <td>[let, be, test, some, more]</td>\n",
       "      <td>[(let, NN), (is, VBZ), (test, NN), (some, DT),...</td>\n",
       "      <td>[(let, NN), (be, VB), (test, NN), (some, DT), ...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing</td>\n",
       "      <td>[more, testing]</td>\n",
       "      <td>[more, testing]</td>\n",
       "      <td>[(more, JJR), (testing, NN)]</td>\n",
       "      <td>[(more, JJR), (testing, NN)]</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, JJR), (testing, NN)]</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, JJR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>this is just some sample text</td>\n",
       "      <td>[this, is, just, some, sample, text]</td>\n",
       "      <td>[this, be, just, some, sample, text]</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, NN), (just, RB), (some, DT),...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing uh</td>\n",
       "      <td>[what, are, we, doing, up]</td>\n",
       "      <td>[what, be, we, do, up]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VB...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP), (...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant                                            content  \\\n",
       "0            1                                what're we doing uh   \n",
       "1            2   we just want to make sure that we're capturin...   \n",
       "2            1  we don't really care about what we're saying r...   \n",
       "3            2                               let's test some more   \n",
       "4            1              more testing but we need more variety   \n",
       "5            2     and more testing this is just some sample text   \n",
       "0            2                               let's test some more   \n",
       "1            1                                       more testing   \n",
       "2            2                                   and more testing   \n",
       "3            1              more testing but we need more variety   \n",
       "4            2                      this is just some sample text   \n",
       "5            1                                what're we doing uh   \n",
       "6            2   we just want to make sure that we're capturin...   \n",
       "7            1  we don't really care about what we're saying r...   \n",
       "0            2                      this is just some sample text   \n",
       "1            1  we don't really care about what we're saying r...   \n",
       "2            2                               let's test some more   \n",
       "3            1              more testing but we need more variety   \n",
       "4            2                                   and more testing   \n",
       "5            1                                what're we doing uh   \n",
       "6            2   we just want to make sure that we're capturin...   \n",
       "0            1              more testing but we need more variety   \n",
       "1            2                                   and more testing   \n",
       "2            1                                what're we doing uh   \n",
       "3            2   we just want to make sure that we're capturin...   \n",
       "4            1  we don't really care about what we're saying r...   \n",
       "5            2                               let's test some more   \n",
       "0            1              more testing but we need more variety   \n",
       "1            2                                   and more testing   \n",
       "2            1                                what're we doing uh   \n",
       "3            2   we just want to make sure that we're capturin...   \n",
       "4            1  we don't really care about what we're saying r...   \n",
       "5            2                               let's test some more   \n",
       "0            2                      this is just some sample text   \n",
       "1            1  we don't really care about what we're saying r...   \n",
       "2            2                               let's test some more   \n",
       "3            1              more testing but we need more variety   \n",
       "4            2                                   and more testing   \n",
       "5            1                                what're we doing uh   \n",
       "6            2   we just want to make sure that we're capturin...   \n",
       "0            1                                what're we doing uh   \n",
       "1            2   we just want to make sure that we're capturin...   \n",
       "2            1  we don't really care about what we're saying r...   \n",
       "3            2                               let's test some more   \n",
       "4            1              more testing but we need more variety   \n",
       "5            2     and more testing this is just some sample text   \n",
       "0            2                               let's test some more   \n",
       "1            1                                       more testing   \n",
       "2            2                                   and more testing   \n",
       "3            1              more testing but we need more variety   \n",
       "4            2                      this is just some sample text   \n",
       "5            1                                what're we doing uh   \n",
       "6            2   we just want to make sure that we're capturin...   \n",
       "7            1  we don't really care about what we're saying r...   \n",
       "\n",
       "                                               token  \\\n",
       "0                         [what, are, we, doing, up]   \n",
       "1  [we, just, want, to, make, sure, that, we, are...   \n",
       "2  [we, do, not, really, care, about, what, we, a...   \n",
       "3                        [let, is, test, some, more]   \n",
       "4      [more, testing, but, we, need, more, variety]   \n",
       "5  [and, more, testing, this, is, just, some, sam...   \n",
       "0                        [let, is, test, some, more]   \n",
       "1                                    [more, testing]   \n",
       "2                               [and, more, testing]   \n",
       "3      [more, testing, but, we, need, more, variety]   \n",
       "4               [this, is, just, some, sample, text]   \n",
       "5                         [what, are, we, doing, up]   \n",
       "6  [we, just, want, to, make, sure, that, we, are...   \n",
       "7  [we, do, not, really, care, about, what, we, a...   \n",
       "0               [this, is, just, some, sample, text]   \n",
       "1  [we, do, not, really, care, about, what, we, a...   \n",
       "2                        [let, is, test, some, more]   \n",
       "3      [more, testing, but, we, need, more, variety]   \n",
       "4                               [and, more, testing]   \n",
       "5                         [what, are, we, doing, up]   \n",
       "6  [we, just, want, to, make, sure, that, we, are...   \n",
       "0      [more, testing, but, we, need, more, variety]   \n",
       "1                               [and, more, testing]   \n",
       "2                         [what, are, we, doing, up]   \n",
       "3  [we, just, want, to, make, sure, that, we, are...   \n",
       "4  [we, do, not, really, care, about, what, we, a...   \n",
       "5                        [let, is, test, some, more]   \n",
       "0      [more, testing, but, we, need, more, variety]   \n",
       "1                               [and, more, testing]   \n",
       "2                         [what, are, we, doing, up]   \n",
       "3  [we, just, want, to, make, sure, that, we, are...   \n",
       "4  [we, do, not, really, care, about, what, we, a...   \n",
       "5                        [let, is, test, some, more]   \n",
       "0               [this, is, just, some, sample, text]   \n",
       "1  [we, do, not, really, care, about, what, we, a...   \n",
       "2                        [let, is, test, some, more]   \n",
       "3      [more, testing, but, we, need, more, variety]   \n",
       "4                               [and, more, testing]   \n",
       "5                         [what, are, we, doing, up]   \n",
       "6  [we, just, want, to, make, sure, that, we, are...   \n",
       "0                         [what, are, we, doing, up]   \n",
       "1  [we, just, want, to, make, sure, that, we, are...   \n",
       "2  [we, do, not, really, care, about, what, we, a...   \n",
       "3                        [let, is, test, some, more]   \n",
       "4      [more, testing, but, we, need, more, variety]   \n",
       "5  [and, more, testing, this, is, just, some, sam...   \n",
       "0                        [let, is, test, some, more]   \n",
       "1                                    [more, testing]   \n",
       "2                               [and, more, testing]   \n",
       "3      [more, testing, but, we, need, more, variety]   \n",
       "4               [this, is, just, some, sample, text]   \n",
       "5                         [what, are, we, doing, up]   \n",
       "6  [we, just, want, to, make, sure, that, we, are...   \n",
       "7  [we, do, not, really, care, about, what, we, a...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0                             [what, be, we, do, up]   \n",
       "1  [we, just, want, to, make, sure, that, we, be,...   \n",
       "2  [we, do, not, really, care, about, what, we, b...   \n",
       "3                        [let, be, test, some, more]   \n",
       "4      [more, testing, but, we, need, more, variety]   \n",
       "5  [and, more, testing, this, be, just, some, sam...   \n",
       "0                        [let, be, test, some, more]   \n",
       "1                                    [more, testing]   \n",
       "2                               [and, more, testing]   \n",
       "3      [more, testing, but, we, need, more, variety]   \n",
       "4               [this, be, just, some, sample, text]   \n",
       "5                             [what, be, we, do, up]   \n",
       "6  [we, just, want, to, make, sure, that, we, be,...   \n",
       "7  [we, do, not, really, care, about, what, we, b...   \n",
       "0               [this, be, just, some, sample, text]   \n",
       "1  [we, do, not, really, care, about, what, we, b...   \n",
       "2                        [let, be, test, some, more]   \n",
       "3      [more, testing, but, we, need, more, variety]   \n",
       "4                               [and, more, testing]   \n",
       "5                             [what, be, we, do, up]   \n",
       "6  [we, just, want, to, make, sure, that, we, be,...   \n",
       "0      [more, testing, but, we, need, more, variety]   \n",
       "1                               [and, more, testing]   \n",
       "2                             [what, be, we, do, up]   \n",
       "3  [we, just, want, to, make, sure, that, we, be,...   \n",
       "4  [we, do, not, really, care, about, what, we, b...   \n",
       "5                        [let, be, test, some, more]   \n",
       "0      [more, testing, but, we, need, more, variety]   \n",
       "1                               [and, more, testing]   \n",
       "2                             [what, be, we, do, up]   \n",
       "3  [we, just, want, to, make, sure, that, we, be,...   \n",
       "4  [we, do, not, really, care, about, what, we, b...   \n",
       "5                        [let, be, test, some, more]   \n",
       "0               [this, be, just, some, sample, text]   \n",
       "1  [we, do, not, really, care, about, what, we, b...   \n",
       "2                        [let, be, test, some, more]   \n",
       "3      [more, testing, but, we, need, more, variety]   \n",
       "4                               [and, more, testing]   \n",
       "5                             [what, be, we, do, up]   \n",
       "6  [we, just, want, to, make, sure, that, we, be,...   \n",
       "0                             [what, be, we, do, up]   \n",
       "1  [we, just, want, to, make, sure, that, we, be,...   \n",
       "2  [we, do, not, really, care, about, what, we, b...   \n",
       "3                        [let, be, test, some, more]   \n",
       "4      [more, testing, but, we, need, more, variety]   \n",
       "5  [and, more, testing, this, be, just, some, sam...   \n",
       "0                        [let, be, test, some, more]   \n",
       "1                                    [more, testing]   \n",
       "2                               [and, more, testing]   \n",
       "3      [more, testing, but, we, need, more, variety]   \n",
       "4               [this, be, just, some, sample, text]   \n",
       "5                             [what, be, we, do, up]   \n",
       "6  [we, just, want, to, make, sure, that, we, be,...   \n",
       "7  [we, do, not, really, care, about, what, we, b...   \n",
       "\n",
       "                                   tagged_penn_token  \\\n",
       "0  [(what, WP), (are, VBP), (we, PRP), (doing, VB...   \n",
       "1  [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "2  [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "3  [(let, NN), (is, VBZ), (test, NN), (some, DT),...   \n",
       "4  [(more, JJR), (testing, NN), (but, CC), (we, P...   \n",
       "5  [(and, CC), (more, JJR), (testing, NN), (this,...   \n",
       "0  [(let, NN), (is, VBZ), (test, NN), (some, DT),...   \n",
       "1                       [(more, JJR), (testing, NN)]   \n",
       "2            [(and, CC), (more, JJR), (testing, NN)]   \n",
       "3  [(more, JJR), (testing, NN), (but, CC), (we, P...   \n",
       "4  [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "5  [(what, WP), (are, VBP), (we, PRP), (doing, VB...   \n",
       "6  [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "7  [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "0  [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "1  [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "2  [(let, NN), (is, VBZ), (test, NN), (some, DT),...   \n",
       "3  [(more, JJR), (testing, NN), (but, CC), (we, P...   \n",
       "4            [(and, CC), (more, JJR), (testing, NN)]   \n",
       "5  [(what, WP), (are, VBP), (we, PRP), (doing, VB...   \n",
       "6  [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "0  [(more, JJR), (testing, NN), (but, CC), (we, P...   \n",
       "1            [(and, CC), (more, JJR), (testing, NN)]   \n",
       "2  [(what, WP), (are, VBP), (we, PRP), (doing, VB...   \n",
       "3  [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4  [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "5  [(let, NN), (is, VBZ), (test, NN), (some, DT),...   \n",
       "0  [(more, JJR), (testing, NN), (but, CC), (we, P...   \n",
       "1            [(and, CC), (more, JJR), (testing, NN)]   \n",
       "2  [(what, WP), (are, VBP), (we, PRP), (doing, VB...   \n",
       "3  [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4  [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "5  [(let, NN), (is, VBZ), (test, NN), (some, DT),...   \n",
       "0  [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "1  [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "2  [(let, NN), (is, VBZ), (test, NN), (some, DT),...   \n",
       "3  [(more, JJR), (testing, NN), (but, CC), (we, P...   \n",
       "4            [(and, CC), (more, JJR), (testing, NN)]   \n",
       "5  [(what, WP), (are, VBP), (we, PRP), (doing, VB...   \n",
       "6  [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "0  [(what, WP), (are, VBP), (we, PRP), (doing, VB...   \n",
       "1  [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "2  [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "3  [(let, NN), (is, VBZ), (test, NN), (some, DT),...   \n",
       "4  [(more, JJR), (testing, NN), (but, CC), (we, P...   \n",
       "5  [(and, CC), (more, JJR), (testing, NN), (this,...   \n",
       "0  [(let, NN), (is, VBZ), (test, NN), (some, DT),...   \n",
       "1                       [(more, JJR), (testing, NN)]   \n",
       "2            [(and, CC), (more, JJR), (testing, NN)]   \n",
       "3  [(more, JJR), (testing, NN), (but, CC), (we, P...   \n",
       "4  [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "5  [(what, WP), (are, VBP), (we, PRP), (doing, VB...   \n",
       "6  [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "7  [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "\n",
       "                                   tagged_penn_lemma                     file  \n",
       "0  [(what, WP), (be, VB), (we, PRP), (do, VBP), (...  dyad_10-condition_1.txt  \n",
       "1  [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_10-condition_1.txt  \n",
       "2  [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_10-condition_1.txt  \n",
       "3  [(let, NN), (be, VB), (test, NN), (some, DT), ...  dyad_10-condition_1.txt  \n",
       "4  [(more, JJR), (testing, NN), (but, CC), (we, P...  dyad_10-condition_1.txt  \n",
       "5  [(and, CC), (more, JJR), (testing, NN), (this,...  dyad_10-condition_1.txt  \n",
       "0  [(let, NN), (be, VB), (test, NN), (some, DT), ...  dyad_10-condition_2.txt  \n",
       "1                       [(more, JJR), (testing, NN)]  dyad_10-condition_2.txt  \n",
       "2            [(and, CC), (more, JJR), (testing, NN)]  dyad_10-condition_2.txt  \n",
       "3  [(more, JJR), (testing, NN), (but, CC), (we, P...  dyad_10-condition_2.txt  \n",
       "4  [(this, DT), (be, NN), (just, RB), (some, DT),...  dyad_10-condition_2.txt  \n",
       "5  [(what, WP), (be, VB), (we, PRP), (do, VBP), (...  dyad_10-condition_2.txt  \n",
       "6  [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_10-condition_2.txt  \n",
       "7  [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_10-condition_2.txt  \n",
       "0  [(this, DT), (be, NN), (just, RB), (some, DT),...  dyad_12-condition_1.txt  \n",
       "1  [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_12-condition_1.txt  \n",
       "2  [(let, NN), (be, VB), (test, NN), (some, DT), ...  dyad_12-condition_1.txt  \n",
       "3  [(more, JJR), (testing, NN), (but, CC), (we, P...  dyad_12-condition_1.txt  \n",
       "4            [(and, CC), (more, JJR), (testing, NN)]  dyad_12-condition_1.txt  \n",
       "5  [(what, WP), (be, VB), (we, PRP), (do, VBP), (...  dyad_12-condition_1.txt  \n",
       "6  [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_12-condition_1.txt  \n",
       "0  [(more, JJR), (testing, NN), (but, CC), (we, P...  dyad_12-condition_2.txt  \n",
       "1            [(and, CC), (more, JJR), (testing, NN)]  dyad_12-condition_2.txt  \n",
       "2  [(what, WP), (be, VB), (we, PRP), (do, VBP), (...  dyad_12-condition_2.txt  \n",
       "3  [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_12-condition_2.txt  \n",
       "4  [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_12-condition_2.txt  \n",
       "5  [(let, NN), (be, VB), (test, NN), (some, DT), ...  dyad_12-condition_2.txt  \n",
       "0  [(more, JJR), (testing, NN), (but, CC), (we, P...  dyad_15-condition_2.txt  \n",
       "1            [(and, CC), (more, JJR), (testing, NN)]  dyad_15-condition_2.txt  \n",
       "2  [(what, WP), (be, VB), (we, PRP), (do, VBP), (...  dyad_15-condition_2.txt  \n",
       "3  [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_15-condition_2.txt  \n",
       "4  [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_15-condition_2.txt  \n",
       "5  [(let, NN), (be, VB), (test, NN), (some, DT), ...  dyad_15-condition_2.txt  \n",
       "0  [(this, DT), (be, NN), (just, RB), (some, DT),...  dyad_15-condition_1.txt  \n",
       "1  [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_15-condition_1.txt  \n",
       "2  [(let, NN), (be, VB), (test, NN), (some, DT), ...  dyad_15-condition_1.txt  \n",
       "3  [(more, JJR), (testing, NN), (but, CC), (we, P...  dyad_15-condition_1.txt  \n",
       "4            [(and, CC), (more, JJR), (testing, NN)]  dyad_15-condition_1.txt  \n",
       "5  [(what, WP), (be, VB), (we, PRP), (do, VBP), (...  dyad_15-condition_1.txt  \n",
       "6  [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_15-condition_1.txt  \n",
       "0  [(what, WP), (be, VB), (we, PRP), (do, VBP), (...  dyad_13-condition_1.txt  \n",
       "1  [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_13-condition_1.txt  \n",
       "2  [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_13-condition_1.txt  \n",
       "3  [(let, NN), (be, VB), (test, NN), (some, DT), ...  dyad_13-condition_1.txt  \n",
       "4  [(more, JJR), (testing, NN), (but, CC), (we, P...  dyad_13-condition_1.txt  \n",
       "5  [(and, CC), (more, JJR), (testing, NN), (this,...  dyad_13-condition_1.txt  \n",
       "0  [(let, NN), (be, VB), (test, NN), (some, DT), ...  dyad_13-condition_2.txt  \n",
       "1                       [(more, JJR), (testing, NN)]  dyad_13-condition_2.txt  \n",
       "2            [(and, CC), (more, JJR), (testing, NN)]  dyad_13-condition_2.txt  \n",
       "3  [(more, JJR), (testing, NN), (but, CC), (we, P...  dyad_13-condition_2.txt  \n",
       "4  [(this, DT), (be, NN), (just, RB), (some, DT),...  dyad_13-condition_2.txt  \n",
       "5  [(what, WP), (be, VB), (we, PRP), (do, VBP), (...  dyad_13-condition_2.txt  \n",
       "6  [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_13-condition_2.txt  \n",
       "7  [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_13-condition_2.txt  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_PHASE1RUN(input_file_directory=INPUT_PATH+TRANSCRIPTS,\n",
    "                      output_file_directory=INPUT_PATH+PREPPED_TRANSCRIPTS,\n",
    "                      training_dictionary=INPUT_PATH+'big.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phase 2: Generate alignment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* [**Create helper functions**](#Create-helper-functions) for processing turn- and conversation-level data.\n",
    "* **[Build semantic space](#Build-semantic-space)** from the `forSemantic.txt` generated in Phase 1 and return a `word2vec` semantic space and vocabulary list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[To top.](#ALIGN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: This whole section could be easily collapsed into just the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def convert_list_to_string(in_str):\n",
    "#     \"\"\"\n",
    "#     Add something here?\n",
    "#     \"\"\"     \n",
    "#     result = []\n",
    "#     tokens=in_str.split(\",\")\n",
    "#     for t in tokens:\n",
    "#         s = unicode(t.replace(\"[\",\"\").replace(\"]\", \"\").replace(\" \",\"\"))\n",
    "#         result.append(s)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: `convert_list_to_string` was acting a bit strangely for me (and I think was related to the broken loop issue I had, noted below), so I implemented similar functionality in `BuildSemantic` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def convert_tup(in_str):\n",
    "#     \"\"\"\n",
    "#     Add something here?\n",
    "#     \"\"\"      \n",
    "#     result = []\n",
    "#     current_tuple = []\n",
    "#     tokens=in_str.split(\",\")\n",
    "#     for t in tokens:\n",
    "#         s = unicode(t.replace(\"(\",\"\").replace(\")\", \"\").replace(\" \",\"\"))\n",
    "#         s = unicode(s.replace(\"[\",\"\").replace(\"]\", \"\").replace(\" \",\"\"))\n",
    "#         current_tuple.append(s)\n",
    "#         if \")\" in t:\n",
    "#             result.append(tuple(current_tuple))\n",
    "#             current_tuple = []\n",
    "#     return (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: The `convert_tup` function can also be mimicked using existing functions -- I've gone ahead and swapped that out through the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def gather_items(seq):\n",
    "#     \"\"\"\n",
    "#     Add something here?\n",
    "#     \"\"\"         \n",
    "#     new_seq = []\n",
    "#     for tup in seq:\n",
    "#         getPos = [item[1] for item in tup]\n",
    "#         new_seq.append(' '.join(item for item in getPos))\n",
    "#     return new_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def gather_items_lexical(seq):\n",
    "#     \"\"\"\n",
    "#     Add something here?\n",
    "#     \"\"\"     \n",
    "#     new_seq = []\n",
    "#     for tup in seq:\n",
    "#         getPos = [item for item in tup]\n",
    "#         new_seq.append(' '.join(item for item in getPos))\n",
    "#     return new_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: `gather_items` functions can be implemented in a list comprehension inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def alex_ngram_remove_dups_pos(sequence1, sequence2, ngramsize=2):\n",
    "#     \"\"\"\n",
    "#     Remove mimicked lexical sequences from two interlocutors'\n",
    "#     sequences and return a dictionary of counts of ngrams\n",
    "#     of the desired size for each sequence.\n",
    "    \n",
    "#     By default, consider bigrams. If desired, this may be \n",
    "#     changed by setting `ngramsize` to the appropriate \n",
    "#     value.\n",
    "    \n",
    "#     Note that the returned counter dictionaries do not retain \n",
    "#     original ordering of the n-grams.\n",
    "#     \"\"\"     \n",
    "    \n",
    "#     # remove duplicates and recreate sequences\n",
    "#     sequence1 = set(ngrams(sequence1,ngramsize))\n",
    "#     sequence2 = set(ngrams(sequence2,ngramsize))\n",
    "#     noDup1 = list(sequence1 - sequence2)\n",
    "#     noDup2 = list(sequence2 - sequence1)\n",
    "#     new_sequence1 = [tuple([' '.join(pair) for pair in tup]) for tup in noDup1]\n",
    "#     new_sequence2 = [tuple([' '.join(pair) for pair in tup]) for tup in noDup2]\n",
    "    \n",
    "#     # return counters\n",
    "#     return Counter(new_sequence1), Counter(new_sequence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I modified the de-duplication to be passed as an argument to the `ngram_pos` function -- see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_ngram_pos(sequence1,sequence2,\n",
    "                   ignore_duplicates=True,\n",
    "                   ngramsize=2):\n",
    "    \"\"\"\n",
    "    Remove mimicked lexical sequences from two interlocutors'\n",
    "    sequences and return a dictionary of counts of ngrams\n",
    "    of the desired size for each sequence.\n",
    "    \n",
    "    By default, consider bigrams. If desired, this may be \n",
    "    changed by setting `ngramsize` to the appropriate \n",
    "    value.\n",
    "    \n",
    "    By default, ignore duplicate lexical n-grams when\n",
    "    processing these sequences. If desired, this may\n",
    "    be changed with `ignore_duplicates=False`.\n",
    "    \"\"\"     \n",
    "\n",
    "    # remove duplicates and recreate sequences\n",
    "    sequence1 = set(ngrams(sequence1,ngramsize))\n",
    "    sequence2 = set(ngrams(sequence2,ngramsize))\n",
    " \n",
    "    # if desired, remove duplicates from sequences\n",
    "    if ignore_duplicates==True:\n",
    "        new_sequence1 = [tuple([' '.join(pair) for pair in tup]) for tup in list(sequence1 - sequence2)]\n",
    "        new_sequence2 = [tuple([' '.join(pair) for pair in tup]) for tup in list(sequence2 - sequence1)]\n",
    "    else:\n",
    "        new_sequence1 = [tuple([' '.join(pair) for pair in tup]) for tup in sequence1]\n",
    "        new_sequence2 = [tuple([' '.join(pair) for pair in tup]) for tup in sequence2]\n",
    "        \n",
    "    # return counters\n",
    "    return Counter(new_sequence1), Counter(new_sequence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_ngram_lexical(sequence1,sequence2,ngramsize=2):\n",
    "    \"\"\"\n",
    "    Create ngrams of the desired size for each of two\n",
    "    interlocutors' sequences and return a dictionary \n",
    "    of counts of ngrams for each sequence.\n",
    "    \n",
    "    By default, consider bigrams. If desired, this may be \n",
    "    changed by setting `ngramsize` to the appropriate \n",
    "    value.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # generate ngrams\n",
    "    sequence1 = list(ngrams(sequence1,ngramsize))\n",
    "    sequence2 = list(ngrams(sequence2,ngramsize)) \n",
    "\n",
    "    # join for counters\n",
    "    new_sequence1 = [' '.join(pair) for pair in sequence1]\n",
    "    new_sequence2 = [' '.join(pair) for pair in sequence2]\n",
    "    \n",
    "    # return counters\n",
    "    return Counter(new_sequence1), Counter(new_sequence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I updated the above with the list comprehension. According to processing of the code, the `ngram_lexical` doesn't deal with `(word,POS)` tuples but a list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: This throws some errors when one sequence is shorter than the maximum n-gram size. We need to figure out how we'll deal with this. For example, will we just produce NAs from it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_get_cosine(vec1, vec2): \n",
    "    \"\"\"\n",
    "    Derive cosine similarity metric, standard measure.\n",
    "    Adapted from <https://stackoverflow.com/a/33129724>.\n",
    "    \"\"\"     \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I tried implementing some of the standard functions, but it looks like the `get_cosine` is best for text-based cosines. I found this in a StackOverflow answer and linked to it in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def create_columns_list(dictionaries_list):\n",
    "#     \"\"\"\n",
    "#     Takes in list of the following dictionaries:\n",
    "#         cosine_syntaxPN, cosine_syntaxPY, cosine_syntaxPYL, \n",
    "#         cosine_syntaxPNL, cosine_syntaxSN, cosine_syntaxSY, cosine_syntaxSYL, \n",
    "#         cosine_syntaxSNL, cosine_lexical, cosine_lexicalL,\n",
    "#         cosine_semanticL, partner_direction, condition_info\n",
    "        \n",
    "#     Gathers all the keys from these dictionaries in a list\n",
    "#     \"\"\"                 \n",
    "#     columns_list = [col for i, dic in enumerate(d for d in dictionaries_list) for col in sorted(dic)] \n",
    "#     return columns_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: `create_columns_list` (above) is a function that only includes a single, clean line of code -- I think we can just stick it into the relevant functions as-is. (I've gone ahead and implemented that in all of the functions below.) Same for `merge_dict_list` (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def merge_dict_list(dict_list):\n",
    "#     \"\"\"\n",
    "#     Add something here?\n",
    "#     \"\"\"     \n",
    "#     return dict(j for i in dict_list for j in i.items())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def add_index(df,index):\n",
    "#     \"\"\"\n",
    "#     Add something here?\n",
    "#     \"\"\"    \n",
    "#     i1=np.arange(len(df))\n",
    "#     d1 = pd.DataFrame(index, index = i1)\n",
    "#     df=df.join(d1)\n",
    "#     df=df.rename(columns = {0:'time'})\n",
    "#     return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: It seems like we could swap `add_index` (above) for a native pandas call, like `df.reset_index(inplace=True)` or `df['index1'] = df.index`. I updated these calls with native implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_composite_semantic_vector(lemma_seq,vocablist,highDimModel):\n",
    "    \"\"\"\n",
    "    Function for producing vocablist and model is called in the main loop\n",
    "    \"\"\"\n",
    "    getComposite = [0] * len(highDimModel[vocablist[1]])    \n",
    "    for w1 in lemma_seq:\n",
    "        if w1 in vocablist:\n",
    "            semvector = highDimModel[w1]\n",
    "            getComposite = getComposite + semvector    \n",
    "    return getComposite\n",
    "\n",
    "# what we want to do here is find the union of the vocablist within the HDM and then sum over all of the columns.\n",
    "# should be faster/easier than "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build semantic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def BuildSemanticModel():\n",
    "#     \"\"\"\n",
    "#     Build a semantic model from all transcripts from all conversations in target corpus.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     ##// The output of the lemmatizer is saved in text files and when they are loaded again \n",
    "#     # lists are read as strings and need to be converted to lists again\n",
    "#     data1=pd.read_csv(INPUT_PATH + \"forSemantic.txt\", sep='\\t',encoding='utf-8')\n",
    "\n",
    "#     ## get individual words in each utterance\n",
    "#     data1 = data1.lemma.values\n",
    "#     getSentences = [[convert_list_to_string(word) for word in data.split()] for data in data1]\n",
    "\n",
    "#     ## Generate a dictionary and frequency count of all included words\n",
    "#     frequency = defaultdict(int)\n",
    "#     for text in getSentences:\n",
    "#         for token in text:\n",
    "#             frequency[token] += 1\n",
    "#     ## Remove words that only occur once\n",
    "#     frequency = {k: v for k, v in frequency.iteritems() if v > 1}\n",
    "\n",
    "#     ## get list of ALL unique words and corresponding frequency counts\n",
    "#     uniqueWords = frequency.keys() \n",
    "#     uniqueValues = frequency.values() \n",
    "\n",
    "#     ## find very frequent words based on 3SDs above mean, and generate a final list of ALL unique words and of unique content words (not the highest frequency). \n",
    "#     ## the cutoff to be considered high-frequency\n",
    "#     getOut = np.mean(uniqueValues)+(np.std(uniqueValues)*3) \n",
    "#     ## final list of CONTENT words to be used when building composite vectors\n",
    "#     contentWords = [words2 for words2 in frequency.keys() if frequency[words2] < getOut] \n",
    "\n",
    "#     ## Finally, build actual semantic space\n",
    "#     logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "#     semmodel = word2vec.Word2Vec(getSentences, min_count=1) ##// one line, simply builds model and word vectors can be easily accessed and combined\n",
    "\n",
    "#     return contentWords, semmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick:** I'm not sure why, but the loop in lines 16-18 broke for me. I've updated below based on that, although I'm really not sure why it broke for me. I also noticed that the high-frequency words aren't stripped from the word2vec model building, since the `getSentences` values aren't stripped of high-frequency words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_BuildSemanticModel(semantic_model_input_file,\n",
    "                            high_sd_cutoff=3,\n",
    "                            low_n_cutoff=1):\n",
    "    \"\"\"\n",
    "    Given an input file produced by the ALIGN Phase 1 functions, \n",
    "    build a semantic model from all transcripts in all conversations\n",
    "    in target corpus after removing high- and low-frequency words.\n",
    "    High-frequency words are determined by a user-defined number of\n",
    "    SDs over the mean (by default, `high_sd_cutoff=3`). Low-frequency\n",
    "    words must appear over a specified number of raw occurrences \n",
    "    (by default, `low_n_cutoff=1`).\n",
    "    \n",
    "    Frequency cutoffs can be removed by `high_sd_cutoff=None` and/or\n",
    "    `low_n_cutoff=0`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read in the file\n",
    "    data1 = pd.read_csv(semantic_model_input_file, sep='\\t',encoding='utf-8')\n",
    "    \n",
    "    # get frequency count of all included words\n",
    "    all_words = filter(str.isalpha,[word.strip() for word in str(data1['lemma']).split(',')])\n",
    "    frequency = defaultdict(int)\n",
    "    for word in all_words:\n",
    "        frequency[word] += 1\n",
    "        \n",
    "    # remove words that only occur more frequently than our cutoff (defined in occurrences)\n",
    "    frequency = {word: freq for word, freq in frequency.iteritems() if freq > low_n_cutoff}\n",
    "\n",
    "    # if desired, remove high-frequency words (over user-defined SDs above mean) \n",
    "    if high_sd_cutoff == None:\n",
    "        contentWords = [word for word in frequency.keys()] \n",
    "    else:\n",
    "        getOut = np.mean(frequency.values())+(np.std(frequency.values())*(high_sd_cutoff))\n",
    "        contentWords = {word: freq for word, freq in frequency.iteritems() if freq < getOut}.keys()\n",
    "    \n",
    "    # identify the sentences in the file, stripping out words we won't keep\n",
    "    getSentences = [re.sub('[^\\w\\s]+','',str(row)).split(' ') for row in list(data1['lemma'])]\n",
    "    keepSentences = [[word for word in row if word in contentWords] for row in getSentences]\n",
    "    \n",
    "    # build actual semantic space\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    semantic_model = word2vec.Word2Vec(keepSentences, min_count=low_n_cutoff)\n",
    "\n",
    "    # return all the content words and the word2vec model space\n",
    "    return contentWords, semantic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: Here's an example of how the function above would run, given our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-27 17:51:57,414 : INFO : collecting all words and their counts\n",
      "2017-11-27 17:51:57,415 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-27 17:51:57,416 : INFO : collected 22 word types from a corpus of 316 raw words and 54 sentences\n",
      "2017-11-27 17:51:57,417 : INFO : Loading a fresh vocabulary\n",
      "2017-11-27 17:51:57,419 : INFO : min_count=1 retains 22 unique words (100% of original 22, drops 0)\n",
      "2017-11-27 17:51:57,420 : INFO : min_count=1 leaves 316 word corpus (100% of original 316, drops 0)\n",
      "2017-11-27 17:51:57,421 : INFO : deleting the raw counts dictionary of 22 items\n",
      "2017-11-27 17:51:57,424 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2017-11-27 17:51:57,426 : INFO : downsampling leaves estimated 51 word corpus (16.2% of prior 316)\n",
      "2017-11-27 17:51:57,428 : INFO : estimated required memory for 22 words and 100 dimensions: 28600 bytes\n",
      "2017-11-27 17:51:57,430 : INFO : resetting layer weights\n",
      "2017-11-27 17:51:57,431 : INFO : training model with 3 workers on 22 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-27 17:51:57,435 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-27 17:51:57,437 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-27 17:51:57,438 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-27 17:51:57,440 : INFO : training on 1580 raw words (251 effective words) took 0.0s, 46200 effective words/s\n",
      "2017-11-27 17:51:57,441 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['just',\n",
       "  'do',\n",
       "  'testing',\n",
       "  'some',\n",
       "  'sample',\n",
       "  'want',\n",
       "  'need',\n",
       "  'really',\n",
       "  'what',\n",
       "  'make',\n",
       "  'to',\n",
       "  'test',\n",
       "  'more',\n",
       "  'be',\n",
       "  'we',\n",
       "  'sure',\n",
       "  'that',\n",
       "  'but',\n",
       "  'not',\n",
       "  'care',\n",
       "  'about',\n",
       "  'this'],\n",
       " <gensim.models.word2vec.Word2Vec at 0x116490d50>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_BuildSemanticModel(semantic_model_input_file=INPUT_PATH + \"align_concatenated_dataframe.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculate lexical and POS alignment scores for each n-gram length across two comparison vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_LexicalPOSAlignment(tok1,lem1,penn_tok1,penn_lem1,\n",
    "                             tok2,lem2,penn_tok2,penn_lem2,\n",
    "                             stan_tok1=None,stan_lem1=None,\n",
    "                             stan_tok2=None,stan_lem2=None,\n",
    "                             ngramsLength=2,\n",
    "                             ignore_duplicates=True,\n",
    "                             add_stanford_tagger = 0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Derive lexical and part-of-speech alignment scores\n",
    "    between interlocutors (suffix `1` and `2` in arguments\n",
    "    passed to function). \n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tagger=1` and by providing appropriate \n",
    "    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and \n",
    "    `stan_lem2`.\n",
    "    \n",
    "    By default, consider only bigrams when calculating\n",
    "    similarity. If desired, this window may be expanded \n",
    "    by changing the `ngramsLength` argument value.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty dictionaries for syntactic similarity\n",
    "    cosine_syntax_penn_tok = {}\n",
    "    cosine_syntax_penn_lex = {}\n",
    "    \n",
    "    # if desired, generate Stanford-based scores\n",
    "    if add_stanford_tagger == 1:\n",
    "        cosine_syntax_stan_tok = {}\n",
    "        cosine_syntax_stan_lem = {}\n",
    "    \n",
    "    # create empty dictionaries for lexical similarity\n",
    "    cosine_lexical_tok = {}\n",
    "    cosine_lexical_lem = {}\n",
    "    \n",
    "    # cycle through all desired ngram lengths\n",
    "    for ngram in range(2,ngramsLength+1):\n",
    "         \n",
    "        # calculate similarity for lexical ngrams (tokens and lemmas)\n",
    "        [vectorT1, vectorT2] = alex_ngram_lexical(tok1,tok2,ngramsize=ngram)\n",
    "        [vectorL1, vectorL2] = alex_ngram_lexical(lem1,lem2,ngramsize=ngram)\n",
    "        cosine_lexical_tok['cosine_lexical_tok{0}'.format(ngram)] = alex_get_cosine(vectorT1,vectorT2)\n",
    "        cosine_lexical_lem['cosine_lexical_lem{0}'.format(ngram)] = alex_get_cosine(vectorL1, vectorL2)\n",
    "\n",
    "        # calculate similarity for Penn POS ngrams (tokens)\n",
    "        [vector_penn_tok1, vector_penn_tok2] = alex_ngram_pos(penn_tok1,penn_tok2,\n",
    "                                                ngramsize=ngram,\n",
    "                                                ignore_duplicates=ignore_duplicates) \n",
    "        cosine_syntax_penn_tok['cosine_syntax_penn_tok{0}'.format(ngram)] = alex_get_cosine(vector_penn_tok1, \n",
    "                                                                                            vector_penn_tok2)\n",
    "        \n",
    "        # calculate similarity for Penn POS ngrams (lemmas)\n",
    "        [vector_penn_lem1, vector_penn_lem2] = alex_ngram_pos(penn_lem1,penn_lem2,\n",
    "                                                              ngramsize=ngram,\n",
    "                                                              ignore_duplicates=ignore_duplicates) \n",
    "        cosine_syntax_penn_lex['cosine_syntax_penn_lex{0}'.format(ngram)] = alex_get_cosine(vector_penn_lem1, \n",
    "                                                                                            vector_penn_lem2) \n",
    "\n",
    "        # if desired, also calculate using Stanford POS\n",
    "        if add_stanford_tagger == 1:         \n",
    "          \n",
    "            # calculate similarity for Stanford POS ngrams (tokens)\n",
    "            [vector_stan_tok1, vector_stan_tok2] = alex_ngram_pos(stan_tok1,stan_tok2,\n",
    "                                                                  ngramsize=ngram,\n",
    "                                                                  ignore_duplicates=ignore_duplicates) \n",
    "            cosine_syntax_stan_tok['cosine_syntax_stan_tok{0}'.format(ngram)] = alex_get_cosine(vector_stan_tok1,\n",
    "                                                                                                vector_stan_tok2)\n",
    "\n",
    "            # calculate similarity for Stanford POS ngrams (lemmas)\n",
    "            [vector_stan_lem1, vector_stan_lem2] = alex_ngram_pos(stan_lem1,stan_lem2,\n",
    "                                                                  ngramsize=ngram,\n",
    "                                                                  ignore_duplicates=ignore_duplicates) \n",
    "            cosine_syntax_stan_lem['cosine_syntax_stan_lem{0}'.format(ngram)] = alex_get_cosine(vector_stan_lem1,\n",
    "                                                                                                vector_stan_lem2)\n",
    "        \n",
    "    # return requested information\n",
    "    if add_stanford_tagger==1:\n",
    "        dictionaries_list = [cosine_syntax_penn_tok, cosine_syntax_penn_lex,\n",
    "                             cosine_syntax_stan_tok, cosine_syntax_stan_lem, \n",
    "                             cosine_lexical_tok, cosine_lexical_lem]      \n",
    "    else:\n",
    "        dictionaries_list = [cosine_syntax_penn_tok, cosine_syntax_penn_lex,\n",
    "                             cosine_lexical_tok, cosine_lexical_lem]      \n",
    "    return dictionaries_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I updated the code above to implement some of those \"if desired\" options we talked about in the paper. I also made it so that the code doesn't pass along Stanford taggers if that's not desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: It looks like the code was generating some output that included duplicates and some that didn't. I unified this here to only generate output according to the `ignore_duplicates` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate turn-level analysis of alignment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* `CombineSemanticLexPos`\n",
    "    * Gets conceptual alignment scores across two comparison vectors\n",
    "    * Compines with output from `LexicalPOSAlignment`\n",
    "    * Adds condition info and direction of alignment between conversational partners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I think we should slightly tweak the `CombineSemanticLexPos` function name. I'm suggesting `returnMultilevelAlignment` below. (I've added `alex_` as a prefix just to indicate that I've changed it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I think we should spin out the two parts of the `CombineSemanticLexPos` function, since folks might want to use the useful conceptual alignment function outside of the containing function. To that end, I've added it below as `alex_calculateConceptualAlignment`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* `TurnByTurnAnalysis`\n",
    "    * Builds final dataframe with all combined turn-by-turn alignment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_conceptualAlignment(lem1, lem2, vocablist, highDimModel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate conceptual alignment scores from list of lemmas\n",
    "    from between two interocutors (suffix `1` and `2` in arguments\n",
    "    passed to function) using `word2vec`.\n",
    "    \"\"\"\n",
    "\n",
    "    # aggregate composite high-dimensional vectors of all words in utterance\n",
    "    W2Vec1 = build_composite_semantic_vector(lem1,vocablist,highDimModel)\n",
    "    W2Vec2 = build_composite_semantic_vector(lem2,vocablist,highDimModel)\n",
    "\n",
    "    # return cosine distance alignment score\n",
    "    return 1 - spatial.distance.cosine(W2Vec1, W2Vec2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_returnMultilevelAlignment(cond_info,\n",
    "                                   partnerA, text1,tok1,lem1,penn_tok1,penn_lem1,\n",
    "                                   partnerB,text2,tok2,lem2,penn_tok2,penn_lem2,\n",
    "                                   vocablist, highDimModel, \n",
    "                                   stan_tok1=None,stan_lem1=None,\n",
    "                                   stan_tok2=None,stan_lem2=None,\n",
    "                                   add_stanford_tagger=0,\n",
    "                                   ngramsLength=2, \n",
    "                                   ignore_duplicates=True):\n",
    "   \n",
    "    \"\"\"\n",
    "    Calculate lexical, syntactic, and conceptual alignment\n",
    "    between a pair of turns by individual interlocutors \n",
    "    (suffix `1` and `2` in arguments passed to function), \n",
    "    including leading/following comparison directionality.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tagger=1` and by providing appropriate \n",
    "    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and \n",
    "    `stan_lem2`.\n",
    "    \n",
    "    By default, consider only bigrams when calculating\n",
    "    similarity. If desired, this window may be expanded \n",
    "    by changing the `ngramsLength` argument value.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create empty dictionaries \n",
    "    partner_direction = {}\n",
    "    condition_info = {}\n",
    "    cosine_semanticL = {}\n",
    "    \n",
    "    # calculate lexical and syntactic alignment\n",
    "    dictionaries_list = alex_LexicalPOSAlignment(tok1=tok1,lem1=lem1,\n",
    "                                                 penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                 tok2=tok2,lem2=lem2,\n",
    "                                                 penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                 stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                 stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                 ngramsLength=ngramsLength,\n",
    "                                                 ignore_duplicates=ignore_duplicates,\n",
    "                                                 add_stanford_tagger=add_stanford_tagger)\n",
    "    \n",
    "    # calculate conceptual alignment\n",
    "    cosine_semanticL['cosine_semanticL'] = alex_conceptualAlignment(lem1,lem2,vocablist,highDimModel)\n",
    "    dictionaries_list.append(cosine_semanticL.copy())\n",
    "    \n",
    "    # determine directionality of leading/following comparison\n",
    "    partner_direction['partner_direction'] = str(partnerA) + \">\" + str(partnerB)\n",
    "    dictionaries_list.append(partner_direction.copy())\n",
    "\n",
    "    # add condition information\n",
    "    condition_info['condition_info'] = cond_info    \n",
    "    dictionaries_list.append(condition_info.copy())\n",
    "\n",
    "    # return alignment scores\n",
    "    return dictionaries_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: In the `TurnByTurnAnalysis` function, you set `maxngram`'s default to 4, but the other `ngramsLength` defaults are set to 2. Is there a reason why the defaults are different? For the sake of the package, I'd recommend we use a unified default value, even if we decide to increase that ngram size for our application of it.\n",
    "\n",
    "FYI, I haven't changed the default value below, since I don't know whether you'd prefer to set the default to 2 or 4 throughout the function. I don't have strong feelings either way. \n",
    "\n",
    "However, we should probably put a flag in somewhere that the `maxngram` value only plays nicely with the rest of the code when it's equal to the minimum turn length. That is, our code doesn't work well when a turn is unable to provide at least 1 complete n-gram of the specified length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_TurnByTurnAnalysis(dataframe,\n",
    "                            vocablist,\n",
    "                            highDimModel, \n",
    "                            delay=1,\n",
    "                            maxngram = 4,\n",
    "                            add_stanford_tagger=0,\n",
    "                            ignore_duplicates=True):    \n",
    "\n",
    "    \"\"\"\n",
    "    Calculate lexical, syntactic, and conceptual alignment\n",
    "    between interlocutors over an entire conversation.\n",
    "    Automatically detect individual speakers by unique\n",
    "    speaker codes.\n",
    "    \n",
    "    By default, compare only adjacent turns. If desired,\n",
    "    the comparison distance may be changed by increasing\n",
    "    the `delay` argument.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 4. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tagger=1`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if we don't want the Stanford tagger data, set defaults\n",
    "    if add_stanford_tagger == 0:\n",
    "        stan_tok1=None\n",
    "        stan_lem1=None\n",
    "        stan_tok2=None\n",
    "        stan_lem2=None\n",
    "    \n",
    "    # prepare the data to the appropriate type\n",
    "    dataframe['token'] = dataframe['token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['lemma'] = dataframe['lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_penn_token'] = dataframe['tagged_penn_token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_penn_token'] = dataframe['tagged_penn_token'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "    dataframe['tagged_penn_lemma'] = dataframe['tagged_penn_lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_penn_lemma'] = dataframe['tagged_penn_lemma'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "    \n",
    "    # if desired, prepare the Stanford tagger data\n",
    "    if add_stanford_tagger == 1:           \n",
    "        dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "        dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "        dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "        dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "\n",
    "    # create lagged version of the dataframe\n",
    "    df_original = dataframe.drop(dataframe.tail(delay).index,inplace=False)\n",
    "    df_lagged = dataframe.shift(-delay).drop(dataframe.tail(delay).index,inplace=False)\n",
    "    \n",
    "    # cycle through each pair of turns\n",
    "    aggregated_df = pd.DataFrame()\n",
    "    for i in range(0,df_original.shape[0]):\n",
    "\n",
    "        # identify the condition for this dataframe\n",
    "        cond_info = dataframe['file'].unique()\n",
    "        if len(cond_info)==1: \n",
    "            cond_info = str(cond_info[0])\n",
    "        \n",
    "        # break and flag error if we have more than 1 condition per dataframe\n",
    "        else: \n",
    "            raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)\n",
    "\n",
    "        # grab all of first participant's data\n",
    "        first_row = df_original.iloc[i]\n",
    "        first_partner = first_row['participant']\n",
    "        text1=first_row['content']\n",
    "        tok1=first_row['token']\n",
    "        lem1=first_row['lemma']\n",
    "        penn_tok1=first_row['tagged_penn_token']\n",
    "        penn_lem1=first_row['tagged_penn_lemma']\n",
    "\n",
    "        # grab all of lagged participant's data\n",
    "        lagged_row = df_lagged.iloc[i]\n",
    "        lagged_partner = lagged_row['participant']\n",
    "        text2=lagged_row['content']\n",
    "        tok2=lagged_row['token']\n",
    "        lem2=lagged_row['lemma']\n",
    "        penn_tok2=lagged_row['tagged_penn_token']\n",
    "        penn_lem2=lagged_row['tagged_penn_lemma']\n",
    "        \n",
    "        # if desired, grab the Stanford tagger data for both participants\n",
    "        if add_stanford_tagger == 1:           \n",
    "            stan_tok1=first_row['tagged_stan_token']\n",
    "            stan_lem1=first_row['tagged_stan_lemma']\n",
    "            stan_tok2=lagged_row['tagged_stan_token']\n",
    "            stan_lem2=lagged_row['tagged_stan_lemma']\n",
    "                \n",
    "        # process multilevel alignment\n",
    "        dictionaries_list=alex_returnMultilevelAlignment(cond_info=cond_info,\n",
    "                                                         partnerA=first_partner,\n",
    "                                                         text1=text1,\n",
    "                                                         tok1=tok1,lem1=lem1,\n",
    "                                                         penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                         partnerB=lagged_partner,\n",
    "                                                         text2=text2,\n",
    "                                                         tok2=tok2,lem2=lem2,\n",
    "                                                         penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                         vocablist=vocablist,\n",
    "                                                         highDimModel=highDimModel,\n",
    "                                                         stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                         stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                         ngramsLength = maxngram,\n",
    "                                                         ignore_duplicates = ignore_duplicates,\n",
    "                                                         add_stanford_tagger = add_stanford_tagger) \n",
    "        \n",
    "        # append data to existing structures\n",
    "        next_df_line = pd.DataFrame.from_dict(dict(j for i in dictionaries_list for j in i.items()),\n",
    "                               orient='index').transpose()\n",
    "        aggregated_df = aggregated_df.append(next_df_line)\n",
    "            \n",
    "    # reformat turn information and add index\n",
    "    aggregated_df = aggregated_df.reset_index(drop=True).reset_index().rename(columns={\"index\":\"time\"})\n",
    "\n",
    "    # give us our finished dataframe\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I removed the column aggregation and merge dict list functions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: It doesn't look like we're doing anything with the `text` variables (above). Are you wanting to keep it just so that you have a record of the text later?  If that's all, I think it might be useful for us to do something more efficient than slicing and converting the text at each line every time -- for example, by keeping a separate file with the raw text as a dataframe and then joining it at the end with the dataframe or by cutting spinning out the raw text from the dataframe and then re-combining it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generate conversation-level analysis of alignment scores\n",
    "-----------------------------------------------------\n",
    "* Main Functions\n",
    "    * ConvoByConvoAnalysis\n",
    "        * Combines each lexical and syntactic utternace turn for each participant into a single vector \n",
    "    * CombineConvoDict\n",
    "        * Runs conversation-level vector through \"LexicalPOSAlignment\" function to get alignment scores\n",
    "        * Adds condition info\n",
    "        * Builds final dataframe with all combined turn-by-turn alignment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def getEachParticipant(df):\n",
    "#     \"\"\"\n",
    "#     Gets the unique participant identification codes\n",
    "#     \"\"\"\n",
    "#     possPcodes = df['participant'].values\n",
    "#     possPcodes2 = np.unique(possPcodes)\n",
    "#     dA=df[df['participant']==possPcodes2[0]]\n",
    "#     dB=df[df['participant']==possPcodes2[1]]\n",
    "#     return dA, dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I don't really think `getEachParticipant` is neeeded, since we can use native functions for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def alex_ConvoByConvoAnalysis(dataframe,\n",
    "#                               add_stanford_tagger=0):\n",
    "\n",
    "\n",
    "#     \"\"\"\n",
    "#     Prepare data for conversation-level analysis of \n",
    "#     similarity, given a dataframe prepared by Phase 1\n",
    "#     of ALIGN.\n",
    "    \n",
    "#     By default, only consider Penn POS tagger\n",
    "#     information. If desired, also consider Stanford\n",
    "#     POS tagger with `add_stanford_tagger=1`.\n",
    "#     \"\"\"\n",
    "    \n",
    "# #     # prepare the data to the appropriate type\n",
    "# #     dataframe['token'] = dataframe['token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "# #     dataframe['lemma'] = dataframe['lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "# #     dataframe['tagged_penn_token'] = dataframe['tagged_penn_token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "# #     dataframe['tagged_penn_token'] = dataframe['tagged_penn_token'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "# #     dataframe['tagged_penn_lemma'] = dataframe['tagged_penn_lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "# #     dataframe['tagged_penn_lemma'] = dataframe['tagged_penn_lemma'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "    \n",
    "# #     # if desired, prepare the Stanford tagger data\n",
    "# #     if add_stanford_tagger == 1:           \n",
    "# #         dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "# #         dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "# #         dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "# #         dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "\n",
    "#     # identify the condition for this dataframe\n",
    "#     cond_info = dataframe['file'].unique()\n",
    "#     if len(cond_info)==1: \n",
    "#         cond_info = str(cond_info[0])\n",
    "\n",
    "#     # break and flag error if we have more than 1 condition per dataframe\n",
    "#     else: \n",
    "#         raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)\n",
    "    \n",
    "#     # concatenate the token, lemma, and POS information for this conversation\n",
    "#     tok_convo = [word for turn in dataframe['token'] for word in turn]\n",
    "#     lem_convo = [word for turn in dataframe['lemma'] for word in turn]\n",
    "#     penn_tok_convo = [POS for turn in dataframe['tagged_penn_token'] for POS in turn]    \n",
    "#     penn_lem_convo = [POS for turn in dataframe['tagged_penn_token'] for POS in turn] \n",
    "\n",
    "#     # if desired, also add Stanford tagger info\n",
    "#     if add_stanford_tagger == 1:\n",
    "#         stan_tok_convo = [POS for turn in dataframe['tagged_stan_token'] for POS in turn]    \n",
    "#         stan_lem_convo = [POS for turn in dataframe['tagged_stan_lemma'] for POS in turn] \n",
    "                \n",
    "#     # return desired data\n",
    "#     if add_stanford_tagger==1:\n",
    "#         return cond_info, tok_convo, lem_convo, penn_tok_convo, penn_lem_convo, stan_tok_convo, stan_lem_convo\n",
    "#     else: \n",
    "#         return cond_info, tok_convo, lem_convo, penn_tok_convo, penn_lem_convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_ConvoByConvoAnalysis(dataframe,\n",
    "                          ngramsLength = 2,\n",
    "                          ignore_duplicates=True,\n",
    "                          add_stanford_tagger = 0):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate analysis of multilevel similarity over\n",
    "    a conversation between two interlocutors from a \n",
    "    transcript dataframe prepared by Phase 1\n",
    "    of ALIGN. Automatically detect speakers by unique\n",
    "    speaker codes.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 2. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tagger=1`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # identify the condition for this dataframe\n",
    "    cond_info = dataframe['file'].unique()\n",
    "    if len(cond_info)==1: \n",
    "        cond_info = str(cond_info[0])\n",
    "    \n",
    "    # break and flag error if we have more than 1 condition per dataframe\n",
    "    else: \n",
    "        raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)\n",
    "   \n",
    "    # if we don't want the Stanford info, set defaults \n",
    "    if add_stanford_tagger==0:\n",
    "        stan_tok1 = None\n",
    "        stan_lem1 = None\n",
    "        stan_tok2 = None\n",
    "        stan_lem2 = None\n",
    "\n",
    "    # identify individual interlocutors\n",
    "    df_A = dataframe.loc[dataframe['participant'] == dataframe['participant'].unique()[0]]\n",
    "    df_B = dataframe.loc[dataframe['participant'] == dataframe['participant'].unique()[1]]\n",
    "   \n",
    "    # concatenate the token, lemma, and POS information for participant A\n",
    "    tok1 = [word for turn in df_A['token'] for word in turn]\n",
    "    lem1 = [word for turn in df_A['lemma'] for word in turn]\n",
    "    penn_tok1 = [POS for turn in df_A['tagged_penn_token'] for POS in turn]    \n",
    "    penn_lem1 = [POS for turn in df_A['tagged_penn_token'] for POS in turn] \n",
    "    if add_stanford_tagger == 1:\n",
    "        stan_tok1 = [POS for turn in df_A['tagged_stan_token'] for POS in turn]    \n",
    "        stan_lem21 = [POS for turn in df_A['tagged_stan_lemma'] for POS in turn] \n",
    "\n",
    "    # concatenate the token, lemma, and POS information for participant B\n",
    "    tok2 = [word for turn in df_B['token'] for word in turn]\n",
    "    lem2 = [word for turn in df_B['lemma'] for word in turn]\n",
    "    penn_tok2 = [POS for turn in df_B['tagged_penn_token'] for POS in turn]    \n",
    "    penn_lem2 = [POS for turn in df_B['tagged_penn_token'] for POS in turn] \n",
    "    if add_stanford_tagger == 1:\n",
    "        stan_tok2 = [POS for turn in df_B['tagged_stan_token'] for POS in turn]    \n",
    "        stan_lem2 = [POS for turn in df_B['tagged_stan_lemma'] for POS in turn] \n",
    "        \n",
    "    # process multilevel alignment\n",
    "    dictionaries_list = alex_LexicalPOSAlignment(tok1=tok1,lem1=lem1,\n",
    "                                                 penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                 tok2=tok2,lem2=lem2,\n",
    "                                                 penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                 stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                 stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                 ngramsLength=ngramsLength,\n",
    "                                                 ignore_duplicates=ignore_duplicates,\n",
    "                                                 add_stanford_tagger=add_stanford_tagger)\n",
    "    \n",
    "    # append data to existing structures\n",
    "    dictionary_df = pd.DataFrame.from_dict(dict(j for i in dictionaries_list for j in i.items()),\n",
    "                           orient='index').transpose()\n",
    "    dictionary_df['condition_info'] = cond_info\n",
    "            \n",
    "    # return the dataframe\n",
    "    return dictionary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I think it makes more sense to collapse the `ConvoByConvoAnalysis` and `CombineConvoDict` functions. See above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to self**: We could/should probably make `convobyconvo` an optional add-on from `turnbyturn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RUN Phase 2: Actual Partners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* For each prepped transcript file, runs turn-level and conversational-level alignment scores\n",
    "* Saves output into single datasheet to be used in statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_PHASE2RUN_REAL(input_file_directory, \n",
    "                        output_file_directory,\n",
    "                        semantic_model_input_file,\n",
    "                        high_sd_cutoff=3,\n",
    "                        low_n_cutoff=1,\n",
    "                        delay=1,\n",
    "                        maxngram=4,\n",
    "                        ignore_duplicates=True,\n",
    "                        add_stanford_tagger=0):   \n",
    "\n",
    "    \"\"\"\n",
    "    Given a directory of individual .txt files and the\n",
    "    vocab list that have been generated by the `PHASE1RUN` \n",
    "    preparation stage, return multi-level alignment \n",
    "    scores with turn-by-turn and conversation-level metrics.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    high-frequency cutoff of 3 SD over the mean. If \n",
    "    desired, this can be changed with the \n",
    "    `high_sd_cutoff` argument and can be removed with\n",
    "    `high_sd_cutoff=None`.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    low-frequency cutoff in which a word will be \n",
    "    removed if they occur 1 or fewer times. if\n",
    "    desired, this can be changed with the \n",
    "    `low_n_cutoff` argument and can be removed with\n",
    "    `low_n_cutoff=0`.\n",
    "    \n",
    "    By default, compare only adjacent turns. If desired,\n",
    "    the comparison distance may be changed by increasing\n",
    "    the `delay` argument.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 4. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tagger=1`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # grab the files in the list\n",
    "    file_list = glob.glob(input_file_directory+\"*.txt\")\n",
    "    \n",
    "    # build the semantic model to be used for all conversations\n",
    "    [vocablist, highDimModel] = alex_BuildSemanticModel(semantic_model_input_file=semantic_model_input_file,\n",
    "                                                        high_sd_cutoff=high_sd_cutoff,\n",
    "                                                        low_n_cutoff=low_n_cutoff)\n",
    "\n",
    "    # create containers for alignment values\n",
    "    AlignmentT2T = pd.DataFrame()\n",
    "    AlignmentC2C = pd.DataFrame()\n",
    "    \n",
    "    # cycle through each prepared file\n",
    "    for fileName in file_list:\n",
    "        \n",
    "        # process the file if it's got a valid conversation\n",
    "        dataframe=pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        if len(dataframe) > 0:\n",
    "            \n",
    "            # let us know which filename we're processing\n",
    "            print \"Processing: \"+fileName   \n",
    "\n",
    "            # calculate turn-by-turn alignment scores\n",
    "            xT2T=alex_TurnByTurnAnalysis(dataframe=dataframe,\n",
    "                                         delay=delay,\n",
    "                                         maxngram=maxngram,\n",
    "                                         vocablist=vocablist,\n",
    "                                         highDimModel=highDimModel)\n",
    "            AlignmentT2T=AlignmentT2T.append(xT2T)\n",
    "            \n",
    "            # calculate conversation-level alignment scores\n",
    "            xC2C = alex_ConvoByConvoAnalysis(dataframe=dataframe,\n",
    "                                             ngramsLength = maxngram,\n",
    "                                             ignore_duplicates=ignore_duplicates,\n",
    "                                             add_stanford_tagger = add_stanford_tagger)\n",
    "            AlignmentC2C=AlignmentC2C.append(xC2C)\n",
    "        \n",
    "        # if it's invalid, let us know\n",
    "        else:\n",
    "            print \"Invalid file: \"+fileName   \n",
    "            \n",
    "    # update final dataframes\n",
    "    FINAL_TURN = AlignmentT2T.reset_index(drop=True)\n",
    "    FINAL_CONVO = AlignmentC2C.reset_index(drop=True)\n",
    "    \n",
    "    # export the final files\n",
    "    FINAL_TURN.to_csv(output_file_directory+\"AlignmentT2T.txt\",\n",
    "                      encoding='utf-8',index=False,sep='\\t')   \n",
    "    FINAL_CONVO.to_csv(output_file_directory+\"AlignmentC2C.txt\",\n",
    "                       encoding='utf-8',index=False,sep='\\t') \n",
    "\n",
    "    # display the info, too\n",
    "    return FINAL_TURN, FINAL_CONVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-27 17:51:57,799 : INFO : collecting all words and their counts\n",
      "2017-11-27 17:51:57,800 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-27 17:51:57,802 : INFO : collected 22 word types from a corpus of 316 raw words and 54 sentences\n",
      "2017-11-27 17:51:57,803 : INFO : Loading a fresh vocabulary\n",
      "2017-11-27 17:51:57,805 : INFO : min_count=1 retains 22 unique words (100% of original 22, drops 0)\n",
      "2017-11-27 17:51:57,807 : INFO : min_count=1 leaves 316 word corpus (100% of original 316, drops 0)\n",
      "2017-11-27 17:51:57,808 : INFO : deleting the raw counts dictionary of 22 items\n",
      "2017-11-27 17:51:57,809 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2017-11-27 17:51:57,810 : INFO : downsampling leaves estimated 51 word corpus (16.2% of prior 316)\n",
      "2017-11-27 17:51:57,812 : INFO : estimated required memory for 22 words and 100 dimensions: 28600 bytes\n",
      "2017-11-27 17:51:57,813 : INFO : resetting layer weights\n",
      "2017-11-27 17:51:57,815 : INFO : training model with 3 workers on 22 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-27 17:51:57,818 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-27 17:51:57,820 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-27 17:51:57,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-27 17:51:57,823 : INFO : training on 1580 raw words (251 effective words) took 0.0s, 53020 effective words/s\n",
      "2017-11-27 17:51:57,824 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_10-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_10-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_12-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_12-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_15-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_15-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_13-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_13-condition_2.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    time cosine_lexical_tok2 cosine_lexical_tok3           condition_info  \\\n",
       " 0      0                   0                   0  dyad_10-condition_1.txt   \n",
       " 1      1          0.08703883                   0  dyad_10-condition_1.txt   \n",
       " 2      2                   0                   0  dyad_10-condition_1.txt   \n",
       " 3      3                   0                   0  dyad_10-condition_1.txt   \n",
       " 4      4           0.1443376                   0  dyad_10-condition_1.txt   \n",
       " 5      0                   0                   0  dyad_10-condition_2.txt   \n",
       " 6      1           0.7071068                   0  dyad_10-condition_2.txt   \n",
       " 7      2           0.2886751                   0  dyad_10-condition_2.txt   \n",
       " 8      3                   0                   0  dyad_10-condition_2.txt   \n",
       " 9      4                   0                   0  dyad_10-condition_2.txt   \n",
       " 10     5                   0                   0  dyad_10-condition_2.txt   \n",
       " 11     6          0.08703883                   0  dyad_10-condition_2.txt   \n",
       " 12     0                   0                   0  dyad_12-condition_1.txt   \n",
       " 13     1                   0                   0  dyad_12-condition_1.txt   \n",
       " 14     2                   0                   0  dyad_12-condition_1.txt   \n",
       " 15     3           0.2886751                   0  dyad_12-condition_1.txt   \n",
       " 16     4                   0                   0  dyad_12-condition_1.txt   \n",
       " 17     5                   0                   0  dyad_12-condition_1.txt   \n",
       " 18     0           0.2886751                   0  dyad_12-condition_2.txt   \n",
       " 19     1                   0                   0  dyad_12-condition_2.txt   \n",
       " 20     2                   0                   0  dyad_12-condition_2.txt   \n",
       " 21     3          0.07106691                   0  dyad_12-condition_2.txt   \n",
       " 22     4                   0                   0  dyad_12-condition_2.txt   \n",
       " 23     0           0.2886751                   0  dyad_15-condition_2.txt   \n",
       " 24     1                   0                   0  dyad_15-condition_2.txt   \n",
       " 25     2                   0                   0  dyad_15-condition_2.txt   \n",
       " 26     3          0.07106691                   0  dyad_15-condition_2.txt   \n",
       " 27     4                   0                   0  dyad_15-condition_2.txt   \n",
       " 28     0                   0                   0  dyad_15-condition_1.txt   \n",
       " 29     1                   0                   0  dyad_15-condition_1.txt   \n",
       " 30     2                   0                   0  dyad_15-condition_1.txt   \n",
       " 31     3           0.2886751                   0  dyad_15-condition_1.txt   \n",
       " 32     4                   0                   0  dyad_15-condition_1.txt   \n",
       " 33     5                   0                   0  dyad_15-condition_1.txt   \n",
       " 34     0                   0                   0  dyad_13-condition_1.txt   \n",
       " 35     1          0.08703883                   0  dyad_13-condition_1.txt   \n",
       " 36     2                   0                   0  dyad_13-condition_1.txt   \n",
       " 37     3                   0                   0  dyad_13-condition_1.txt   \n",
       " 38     4           0.1443376                   0  dyad_13-condition_1.txt   \n",
       " 39     0                   0                   0  dyad_13-condition_2.txt   \n",
       " 40     1           0.7071068                   0  dyad_13-condition_2.txt   \n",
       " 41     2           0.2886751                   0  dyad_13-condition_2.txt   \n",
       " 42     3                   0                   0  dyad_13-condition_2.txt   \n",
       " 43     4                   0                   0  dyad_13-condition_2.txt   \n",
       " 44     5                   0                   0  dyad_13-condition_2.txt   \n",
       " 45     6          0.08703883                   0  dyad_13-condition_2.txt   \n",
       " \n",
       "    cosine_lexical_lem4 cosine_lexical_tok4 cosine_lexical_lem3  \\\n",
       " 0                    0                   0                   0   \n",
       " 1                    0                   0                   0   \n",
       " 2                    0                   0                   0   \n",
       " 3                    0                   0                   0   \n",
       " 4                    0                   0                   0   \n",
       " 5                    0                   0                   0   \n",
       " 6                    0                   0                   0   \n",
       " 7                    0                   0                   0   \n",
       " 8                    0                   0                   0   \n",
       " 9                    0                   0                   0   \n",
       " 10                   0                   0                   0   \n",
       " 11                   0                   0                   0   \n",
       " 12                   0                   0                   0   \n",
       " 13                   0                   0                   0   \n",
       " 14                   0                   0                   0   \n",
       " 15                   0                   0                   0   \n",
       " 16                   0                   0                   0   \n",
       " 17                   0                   0                   0   \n",
       " 18                   0                   0                   0   \n",
       " 19                   0                   0                   0   \n",
       " 20                   0                   0                   0   \n",
       " 21                   0                   0                   0   \n",
       " 22                   0                   0                   0   \n",
       " 23                   0                   0                   0   \n",
       " 24                   0                   0                   0   \n",
       " 25                   0                   0                   0   \n",
       " 26                   0                   0                   0   \n",
       " 27                   0                   0                   0   \n",
       " 28                   0                   0                   0   \n",
       " 29                   0                   0                   0   \n",
       " 30                   0                   0                   0   \n",
       " 31                   0                   0                   0   \n",
       " 32                   0                   0                   0   \n",
       " 33                   0                   0                   0   \n",
       " 34                   0                   0                   0   \n",
       " 35                   0                   0                   0   \n",
       " 36                   0                   0                   0   \n",
       " 37                   0                   0                   0   \n",
       " 38                   0                   0                   0   \n",
       " 39                   0                   0                   0   \n",
       " 40                   0                   0                   0   \n",
       " 41                   0                   0                   0   \n",
       " 42                   0                   0                   0   \n",
       " 43                   0                   0                   0   \n",
       " 44                   0                   0                   0   \n",
       " 45                   0                   0                   0   \n",
       " \n",
       "    cosine_syntax_penn_tok4 cosine_syntax_penn_tok3 cosine_syntax_penn_tok2  \\\n",
       " 0                        0                       0                       0   \n",
       " 1                        0                       0                       0   \n",
       " 2                        0                       0                       0   \n",
       " 3                        0                       0                       0   \n",
       " 4                        0                       0                       0   \n",
       " 5                        0                       0                       0   \n",
       " 6                        0                       0                       0   \n",
       " 7                        0                       0                       0   \n",
       " 8                        0                       0                       0   \n",
       " 9                        0                       0                       0   \n",
       " 10                       0                       0                       0   \n",
       " 11                       0                       0                       0   \n",
       " 12                       0                       0                       0   \n",
       " 13                       0                       0                       0   \n",
       " 14                       0                       0                       0   \n",
       " 15                       0                       0                       0   \n",
       " 16                       0                       0                       0   \n",
       " 17                       0                       0                       0   \n",
       " 18                       0                       0                       0   \n",
       " 19                       0                       0                       0   \n",
       " 20                       0                       0                       0   \n",
       " 21                       0                       0                       0   \n",
       " 22                       0                       0                       0   \n",
       " 23                       0                       0                       0   \n",
       " 24                       0                       0                       0   \n",
       " 25                       0                       0                       0   \n",
       " 26                       0                       0                       0   \n",
       " 27                       0                       0                       0   \n",
       " 28                       0                       0                       0   \n",
       " 29                       0                       0                       0   \n",
       " 30                       0                       0                       0   \n",
       " 31                       0                       0                       0   \n",
       " 32                       0                       0                       0   \n",
       " 33                       0                       0                       0   \n",
       " 34                       0                       0                       0   \n",
       " 35                       0                       0                       0   \n",
       " 36                       0                       0                       0   \n",
       " 37                       0                       0                       0   \n",
       " 38                       0                       0                       0   \n",
       " 39                       0                       0                       0   \n",
       " 40                       0                       0                       0   \n",
       " 41                       0                       0                       0   \n",
       " 42                       0                       0                       0   \n",
       " 43                       0                       0                       0   \n",
       " 44                       0                       0                       0   \n",
       " 45                       0                       0                       0   \n",
       " \n",
       "    cosine_syntax_penn_lex2 cosine_syntax_penn_lex3 cosine_syntax_penn_lex4  \\\n",
       " 0                        0                       0                       0   \n",
       " 1                        0                       0                       0   \n",
       " 2                        0                       0                       0   \n",
       " 3                        0                       0                       0   \n",
       " 4                        0                       0                       0   \n",
       " 5                        0                       0                       0   \n",
       " 6                        0                       0                       0   \n",
       " 7                        0                       0                       0   \n",
       " 8                        0                       0                       0   \n",
       " 9                        0                       0                       0   \n",
       " 10                       0                       0                       0   \n",
       " 11                       0                       0                       0   \n",
       " 12                       0                       0                       0   \n",
       " 13                       0                       0                       0   \n",
       " 14                       0                       0                       0   \n",
       " 15                       0                       0                       0   \n",
       " 16                       0                       0                       0   \n",
       " 17                       0                       0                       0   \n",
       " 18                       0                       0                       0   \n",
       " 19                       0                       0                       0   \n",
       " 20                       0                       0                       0   \n",
       " 21                       0                       0                       0   \n",
       " 22                       0                       0                       0   \n",
       " 23                       0                       0                       0   \n",
       " 24                       0                       0                       0   \n",
       " 25                       0                       0                       0   \n",
       " 26                       0                       0                       0   \n",
       " 27                       0                       0                       0   \n",
       " 28                       0                       0                       0   \n",
       " 29                       0                       0                       0   \n",
       " 30                       0                       0                       0   \n",
       " 31                       0                       0                       0   \n",
       " 32                       0                       0                       0   \n",
       " 33                       0                       0                       0   \n",
       " 34                       0                       0                       0   \n",
       " 35                       0                       0                       0   \n",
       " 36                       0                       0                       0   \n",
       " 37                       0                       0                       0   \n",
       " 38                       0                       0                       0   \n",
       " 39                       0                       0                       0   \n",
       " 40                       0                       0                       0   \n",
       " 41                       0                       0                       0   \n",
       " 42                       0                       0                       0   \n",
       " 43                       0                       0                       0   \n",
       " 44                       0                       0                       0   \n",
       " 45                       0                       0                       0   \n",
       " \n",
       "    cosine_semanticL partner_direction cosine_lexical_lem2  \n",
       " 0          0.571742             1>2.0                   0  \n",
       " 1         0.6656333             2>1.0          0.08703883  \n",
       " 2         0.3349915             1>2.0                   0  \n",
       " 3           0.40693             2>1.0                   0  \n",
       " 4         0.4127169             1>2.0           0.1443376  \n",
       " 5         0.3651285             2>1.0                   0  \n",
       " 6                 1             1>2.0           0.7071068  \n",
       " 7          0.658746             2>1.0           0.2886751  \n",
       " 8         0.1233059             1>2.0                   0  \n",
       " 9         0.3596222             2>1.0                   0  \n",
       " 10         0.571742             1>2.0                   0  \n",
       " 11        0.6656333             2>1.0          0.08703883  \n",
       " 12         0.380415             2>1.0                   0  \n",
       " 13        0.3349915             1>2.0                   0  \n",
       " 14          0.40693             2>1.0                   0  \n",
       " 15         0.658746             1>2.0           0.2886751  \n",
       " 16      -0.03504958             2>1.0                   0  \n",
       " 17         0.571742             1>2.0                   0  \n",
       " 18         0.658746             1>2.0           0.2886751  \n",
       " 19      -0.03504958             2>1.0                   0  \n",
       " 20        0.5925365             1>2.0                   0  \n",
       " 21        0.6720995             2>1.0          0.07106691  \n",
       " 22        0.3349915             1>2.0                   0  \n",
       " 23         0.658746             1>2.0           0.2886751  \n",
       " 24      -0.03504958             2>1.0                   0  \n",
       " 25        0.5925365             1>2.0                   0  \n",
       " 26        0.6720995             2>1.0          0.07106691  \n",
       " 27        0.3349915             1>2.0                   0  \n",
       " 28         0.380415             2>1.0                   0  \n",
       " 29        0.3349915             1>2.0                   0  \n",
       " 30          0.40693             2>1.0                   0  \n",
       " 31         0.658746             1>2.0           0.2886751  \n",
       " 32      -0.03504958             2>1.0                   0  \n",
       " 33         0.571742             1>2.0                   0  \n",
       " 34         0.571742             1>2.0                   0  \n",
       " 35        0.6656333             2>1.0          0.08703883  \n",
       " 36        0.3349915             1>2.0                   0  \n",
       " 37          0.40693             2>1.0                   0  \n",
       " 38        0.4127169             1>2.0           0.1443376  \n",
       " 39        0.3651285             2>1.0                   0  \n",
       " 40                1             1>2.0           0.7071068  \n",
       " 41         0.658746             2>1.0           0.2886751  \n",
       " 42        0.1233059             1>2.0                   0  \n",
       " 43        0.3596222             2>1.0                   0  \n",
       " 44         0.571742             1>2.0                   0  \n",
       " 45        0.6656333             2>1.0          0.08703883  ,\n",
       "    cosine_lexical_tok2  cosine_lexical_tok3  cosine_lexical_lem4  \\\n",
       " 0             0.081786                    0                    0   \n",
       " 1             0.113228                    0                    0   \n",
       " 2             0.081786                    0                    0   \n",
       " 3             0.081786                    0                    0   \n",
       " 4             0.081786                    0                    0   \n",
       " 5             0.081786                    0                    0   \n",
       " 6             0.081786                    0                    0   \n",
       " 7             0.113228                    0                    0   \n",
       " \n",
       "    cosine_lexical_tok4  cosine_lexical_lem3  cosine_syntax_penn_tok4  \\\n",
       " 0                    0                    0                        0   \n",
       " 1                    0                    0                        0   \n",
       " 2                    0                    0                        0   \n",
       " 3                    0                    0                        0   \n",
       " 4                    0                    0                        0   \n",
       " 5                    0                    0                        0   \n",
       " 6                    0                    0                        0   \n",
       " 7                    0                    0                        0   \n",
       " \n",
       "    cosine_syntax_penn_tok3  cosine_syntax_penn_tok2  cosine_syntax_penn_lex2  \\\n",
       " 0                        0                        0                        0   \n",
       " 1                        0                        0                        0   \n",
       " 2                        0                        0                        0   \n",
       " 3                        0                        0                        0   \n",
       " 4                        0                        0                        0   \n",
       " 5                        0                        0                        0   \n",
       " 6                        0                        0                        0   \n",
       " 7                        0                        0                        0   \n",
       " \n",
       "    cosine_syntax_penn_lex3  cosine_syntax_penn_lex4  cosine_lexical_lem2  \\\n",
       " 0                        0                        0             0.078446   \n",
       " 1                        0                        0             0.109254   \n",
       " 2                        0                        0             0.078446   \n",
       " 3                        0                        0             0.078446   \n",
       " 4                        0                        0             0.078446   \n",
       " 5                        0                        0             0.078446   \n",
       " 6                        0                        0             0.078446   \n",
       " 7                        0                        0             0.109254   \n",
       " \n",
       "             condition_info  \n",
       " 0  dyad_10-condition_1.txt  \n",
       " 1  dyad_10-condition_2.txt  \n",
       " 2  dyad_12-condition_1.txt  \n",
       " 3  dyad_12-condition_2.txt  \n",
       " 4  dyad_15-condition_2.txt  \n",
       " 5  dyad_15-condition_1.txt  \n",
       " 6  dyad_13-condition_1.txt  \n",
       " 7  dyad_13-condition_2.txt  )"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_PHASE2RUN_REAL(input_file_directory = INPUT_PATH+PREPPED_TRANSCRIPTS, \n",
    "                        output_file_directory = INPUT_PATH+ANALYSIS_READY,\n",
    "                        semantic_model_input_file = INPUT_PATH+'align_concatenated_dataframe.txt',\n",
    "                        high_sd_cutoff=3,\n",
    "                        low_n_cutoff=1,\n",
    "                        delay=1,\n",
    "                        maxngram=4,\n",
    "                        ignore_duplicates=True,\n",
    "                        add_stanford_tagger=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generate surrogate pairings\n",
    "-------------------------\n",
    "* Collects all possible pairs of participants across the dyads in each condition and creates surrogate pairings by combining their conversational turns, preserving turn order. Output saved as new separate conversational transcripts. \n",
    "* Main Function:\n",
    "    * GenerateSurrogate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def alex_DyadsEachCondition(filesList,\n",
    "#                             id_separator = '\\-',\n",
    "#                             condition_label='cond'):\n",
    "#     \"\"\"\n",
    "#     Gets the files in each unique condition category \n",
    "#     as specified by user in file name. Information must \n",
    "#     be embedded in file names as: `dyad_X-condition_Y.txt`.\n",
    "    \n",
    "#     By default, the separator between dyad ID and\n",
    "#     condition ID is a hyphen (\\-). If desired,\n",
    "#     this may be changed in the `id_separator` \n",
    "#     argument.\n",
    "    \n",
    "#     By default, condition IDs will be identified as \n",
    "#     any characters following `cond`. If desired,\n",
    "#     this may be changed with the `condition_label`\n",
    "#     argument.\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     # grab the file name from the base path\n",
    "#     file_info = [re.sub('\\.txt','',os.path.basename(file_name)) for file_name in filesList]\n",
    "\n",
    "#     # separate conditions from dyads\n",
    "#     condition_ids = list(set([re.findall('[^'+id_separator+']*'+condition_label+'.*',metadata)[0] for metadata in file_info]))\n",
    "        \n",
    "#     # find all of the files in each condition\n",
    "#     files_conditions = {}\n",
    "#     for unique_condition in condition_ids:\n",
    "#         next_condition_files = [add_file for add_file in filesList if unique_condition in add_file]\n",
    "#         files_conditions[unique_condition] = next_condition_files\n",
    "        \n",
    "#     # return our dictionary of files and their conditions\n",
    "#     return files_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I don't think we need this as a separate function -- I've incorporated it into the surrogate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def SmallerSet(file_cond,index):\n",
    "#     \"\"\"\n",
    "#     Option that can be run to extract smaller random set of all possible pairs within conditions\n",
    "#     \"\"\"\n",
    "#     results = list(combinations(file_cond[index],2))\n",
    "#     inEachCondition = len(file_cond[index])\n",
    "#     return random.sample(results, inEachCondition) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def UniqueCodes(df):\n",
    "#     \"\"\"\n",
    "#     Gets the unique participant identification codes\n",
    "#     \"\"\"\n",
    "#     possPcodes = df['participant'].values\n",
    "#     possPcodes2 = np.unique(possPcodes)\n",
    "#     return possPcodes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def GenerateSurrogate(flist):\n",
    "    \n",
    "#     if ALLSURROGATE == 0:\n",
    "    \n",
    "    \n",
    "#         ##// should clear out anything in there to start fresh- call some sort of os call through python to remove. can this be done?\n",
    "# #         https://stackoverflow.com/questions/6996603/how-to-delete-a-file-or-folder\n",
    "    \n",
    "    \n",
    "#     file_cond = DyadsEachCondition(flist)\n",
    "#     for i in range(len(file_cond)):\n",
    "\n",
    "#         ##// DEFAULT, get a smaller random set of possible pairs within conditions (same number as original dataset)\n",
    "#         PairConvs=SmallerSet(file_cond, i)\n",
    "        \n",
    "#         ##// OPTIONAL: all possible dyad pairs within condition\n",
    "#         if ALLSURROGATE == 1:\n",
    "#             PairConvs=combinations(file_cond[i],2)\n",
    "            \n",
    "#         for d in PairConvs:\n",
    "#             df1=pd.read_csv(INPUT_PATH + PREPPED_TRANSCRIPTS + d[0], sep='\\t',encoding='utf-8')\n",
    "#             df2=pd.read_csv(INPUT_PATH + PREPPED_TRANSCRIPTS + d[1], sep='\\t',encoding='utf-8')\n",
    "\n",
    "#             columns = list(df1)\n",
    "#             columns = columns[0:]\n",
    "\n",
    "#             partnerA = UniqueCodes(df1)[0]\n",
    "#             partnerB = UniqueCodes(df1)[1]\n",
    "\n",
    "#             ### identify how many turn A and B have\n",
    "#             dA1=df1[df1['participant']==partnerA].reset_index()\n",
    "#             TurnA1=len(dA1)\n",
    "#             dA2=df2[df2['participant']==partnerA].reset_index()\n",
    "#             TurnA2=len(dA2)\n",
    "#             dB1=df1[df1['participant']==partnerB].reset_index()\n",
    "#             TurnB1=len(dB1)\n",
    "#             dB2=df2[df2['participant']==partnerB].reset_index()\n",
    "#             TurnB2=len(dB2)\n",
    "#             Turn1=min([TurnA1,TurnB2])\n",
    "#             Turn2=min([TurnA2,TurnB1])        \n",
    "\n",
    "#             index1 = np.arange(Turn1)\n",
    "#             index2 = np.arange(Turn2)       \n",
    "\n",
    "#             SurrInt1 = pd.DataFrame(columns=columns, index = range(Turn1*2))\n",
    "#             SurrInt2 = pd.DataFrame(columns=columns, index = range(Turn2*2))\n",
    "            \n",
    "#             n=0\n",
    "#             for t in range(0,Turn1):\n",
    "#                 SurrInt1.iloc[n]=dA1.iloc[t]\n",
    "#                 n+=1\n",
    "#                 SurrInt1.iloc[n]=dB2.iloc[t]\n",
    "#                 n+=1\n",
    "#             SurrInt1 = SurrInt1.dropna(thresh=2)\n",
    "\n",
    "#             n=0   \n",
    "#             for t in range(0,Turn2):\n",
    "#                 SurrInt2.iloc[n]=dA2.iloc[t]\n",
    "#                 n+=1\n",
    "#                 SurrInt2.iloc[n]=dB1.iloc[t]\n",
    "#                 n+=1\n",
    "#             SurrInt2 = SurrInt2.dropna(thresh=2)        \n",
    "\n",
    "#             SurrInt1['file'] = d[0] + '-' + d[1]\n",
    "#             SurrInt2['file'] = d[0] + '-' + d[1]\n",
    "            \n",
    "#             name1=u'SurrogatePair_'+unicode(d[0])+u'A'+'_'+unicode(d[1])+u'B.txt'\n",
    "#             name2=u'SurrogatePair_'+unicode(d[1])+u'A'+'_'+unicode(d[0])+u'B.txt'   \n",
    "            \n",
    "#             SurrInt1.to_csv(INPUT_PATH + SURROGATE_TRANSCRIPTS + name1, encoding='utf-8',index=False,sep='\\t')\n",
    "#             SurrInt2.to_csv(INPUT_PATH + SURROGATE_TRANSCRIPTS + name2, encoding='utf-8',index=False,sep='\\t')\n",
    "#             SurrInt1=None\n",
    "#             SurrInt2=None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_GenerateSurrogate(original_conversation_list,\n",
    "                           surrogate_file_directory,\n",
    "                           all_surrogates = False,\n",
    "                           id_separator = '\\-',\n",
    "                           dyad_label='dyad',\n",
    "                           condition_label='cond',\n",
    "                           keep_original_turn_order = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create transcripts for surrogate pairs of \n",
    "    participants (i.e., participants who did not \n",
    "    genuinely interact in the experiment), which\n",
    "    will later be used to generate baseline levels \n",
    "    of alignment. Store surrogate files in a new\n",
    "    folder each time the surrogate generation is run.\n",
    "    \n",
    "    Returns a list of all surrogate files created.\n",
    "\n",
    "    By default, the separator between dyad ID and\n",
    "    condition ID is a hyphen ('\\-'). If desired,\n",
    "    this may be changed in the `id_separator` \n",
    "    argument.\n",
    "\n",
    "    By default, condition IDs will be identified as \n",
    "    any characters following `cond`. If desired,\n",
    "    this may be changed with the `condition_label`\n",
    "    argument.\n",
    "    \n",
    "    By default, dyad IDs will be identified as \n",
    "    any characters following `dyad`. If desired,\n",
    "    this may be changed with the `dyad_label`\n",
    "    argument.\n",
    "    \n",
    "    By default, generate surrogates only from a subset\n",
    "    of all possible pairings. If desired, instead \n",
    "    generate surrogates from all possible pairings\n",
    "    with `all_surrogates=True`.\n",
    "    \n",
    "    By default, create surrogates by shuffling all\n",
    "    turns within each surrogate partner's data. If \n",
    "    desired, retain the original ordering of each\n",
    "    surrogate partner's data with \n",
    "    `keep_original_turn_order = True`.\n",
    "    \"\"\"\n",
    "        \n",
    "    # create a subfolder for the new set of surrogates\n",
    "    import time\n",
    "    new_surrogate_path = surrogate_file_directory + 'surrogate_run-' + str(time.time()) +'/'\n",
    "    if not os.path.exists(new_surrogate_path):\n",
    "        os.makedirs(new_surrogate_path)\n",
    "        \n",
    "    # grab condition types from each file name\n",
    "    file_info = [re.sub('\\.txt','',os.path.basename(file_name)) for file_name in original_conversation_list]\n",
    "    condition_ids = list(set([re.findall('[^'+id_separator+']*'+condition_label+'.*',metadata)[0] for metadata in file_info]))\n",
    "    files_conditions = {}\n",
    "    for unique_condition in condition_ids:\n",
    "        next_condition_files = [add_file for add_file in original_conversation_list if unique_condition in add_file]\n",
    "        files_conditions[unique_condition] = next_condition_files\n",
    "    \n",
    "    # cycle through conditions\n",
    "    for condition in files_conditions.keys():\n",
    "        \n",
    "        # grab all possible pairs of conversations of this condition\n",
    "        paired_surrogates = [pair for pair in combinations(files_conditions[condition],2)]\n",
    "        \n",
    "        # default: randomly pull from all pairs to get target surrogate sample\n",
    "        if all_surrogates == False:\n",
    "            import math\n",
    "            paired_surrogates = random.sample(paired_surrogates, \n",
    "                                              int(math.ceil(len(files_conditions[condition])/2)))\n",
    "            \n",
    "        # cycle through surrogate pairings\n",
    "        for next_surrogate in paired_surrogates:\n",
    "            \n",
    "            # read in the files\n",
    "            original_file1 = os.path.basename(next_surrogate[0])\n",
    "            original_file2 = os.path.basename(next_surrogate[1])\n",
    "            original_df1=pd.read_csv(next_surrogate[0], sep='\\t',encoding='utf-8')\n",
    "            original_df2=pd.read_csv(next_surrogate[1], sep='\\t',encoding='utf-8')\n",
    "            \n",
    "            # get participants A and B from df1\n",
    "            participantA_1_code = min(original_df1['participant'].unique())\n",
    "            participantB_1_code = max(original_df1['participant'].unique())\n",
    "            participantA_1 = original_df1[original_df1['participant'] == participantA_1_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            participantB_1 = original_df1[original_df1['participant'] == participantB_1_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            \n",
    "            # get participants A and B from df2\n",
    "            participantA_2_code = min(original_df2['participant'].unique())\n",
    "            participantB_2_code = max(original_df2['participant'].unique())\n",
    "            participantA_2 = original_df2[original_df2['participant'] == participantA_2_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            participantB_2 = original_df2[original_df2['participant'] == participantB_2_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            \n",
    "            # identify truncation point for both surrogates (to have even number of turns)\n",
    "            surrogateX_turns=min([participantA_1.shape[0],\n",
    "                                  participantB_2.shape[0]])\n",
    "            surrogateY_turns=min([participantA_2.shape[0],\n",
    "                                  participantB_1.shape[0]])\n",
    "            \n",
    "            # if desired, preserve original turn order for surrogate pairs\n",
    "            if keep_original_turn_order == True:\n",
    "                surrogateX = participantA_1.truncate(after=surrogateX_turns-1,copy=False).append(\n",
    "                                participantB_2.truncate(after=surrogateX_turns-1,copy=False)).sort(\n",
    "                                ['index']).reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "                surrogateY = participantA_2.truncate(after=surrogateX_turns-1,copy=False).append(\n",
    "                                participantB_1.truncate(after=surrogateX_turns-1,copy=False)).sort(\n",
    "                                ['index']).reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "            \n",
    "            # otherwise, just shuffle all turns within participants\n",
    "            else:\n",
    "                \n",
    "                # shuffle for first surrogate pairing\n",
    "                surrogateX_A1 = participantA_1.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateX_B2 = participantB_2.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateX = surrogateX_A1.append(surrogateX_B2).sort_index().reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "                \n",
    "                # and for second surrogate pairing\n",
    "                surrogateY_A2 = participantA_2.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateY_B1 = participantB_1.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateY = surrogateY_A2.append(surrogateY_B1).sort_index().reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "\n",
    "            # create filename for our surrogate file\n",
    "            original_dyad1 = re.findall(dyad_label+'[^'+id_separator+']*',original_file1)[0]\n",
    "            original_dyad2 = re.findall(dyad_label+'[^'+id_separator+']*',original_file2)[0]\n",
    "            surrogateX['file'] = condition + '-' + original_dyad1 + '-' + original_dyad2\n",
    "            surrogateY['file'] = condition + '-' + original_dyad1 + '-' + original_dyad2\n",
    "            nameX='SurrogatePair-'+original_dyad1+'A'+'-'+original_dyad2+'B'+'-'+condition+'.txt'\n",
    "            nameY='SurrogatePair-'+original_dyad2+'A'+'-'+original_dyad1+'B'+'-'+condition+'.txt'\n",
    "            \n",
    "            # save to file\n",
    "            surrogateX.to_csv(new_surrogate_path + nameX, encoding='utf-8',index=False,sep='\\t')\n",
    "            surrogateY.to_csv(new_surrogate_path + nameY, encoding='utf-8',index=False,sep='\\t')\n",
    "            \n",
    "    # return list of all surrogate files\n",
    "    return glob.glob(new_surrogate_path+\"*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: I'm not sure what the `if` statement in line 3 is trying to do, since it seems like clearing out the surrogates could be handled whether or not people wanted to use all possible surrogates or just a subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: With the surrogates, we're shuffling all of the turns within each participant, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Nick**: This was an interesting choice to me. I'd thought we were independently sampling participants across all dyads for surrogates, but what we're actually doing is randomly pairing dyads and then virtually swapping the partners within it. Perhaps I missed it when reading the paper earlier, but it might be good for us to beef up this explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "RUN Phase 2: Surrogate Partners\n",
    "-------------------------------\n",
    "* Runs function to generate new surrogate transcript conversations (separate files)\n",
    "* For each surrogate transcript file, runs turn-level and conversational-level alignment scores\n",
    "* Saves output into single datasheet to be used in statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alex_PHASE2RUN_SURROGATE(input_file_directory, \n",
    "                             surrogate_file_directory,\n",
    "                             output_file_directory,\n",
    "                             semantic_model_input_file,\n",
    "                             high_sd_cutoff=3,\n",
    "                             low_n_cutoff=1,\n",
    "                             id_separator = '\\-',\n",
    "                             condition_label='cond',\n",
    "                             dyad_label='dyad',\n",
    "                             all_surrogates=False,\n",
    "                             keep_original_turn_order = False,\n",
    "                             delay=1,\n",
    "                             maxngram=4,\n",
    "                             ignore_duplicates=True,\n",
    "                             add_stanford_tagger=0):   \n",
    "    \"\"\"\n",
    "    Given a directory of individual .txt files and the\n",
    "    vocab list that have been generated by the `PHASE1RUN` \n",
    "    preparation stage, return multi-level alignment \n",
    "    scores with turn-by-turn and conversation-level metrics\n",
    "    for surrogate baseline conversations.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    high-frequency cutoff of 3 SD over the mean. If \n",
    "    desired, this can be changed with the \n",
    "    `high_sd_cutoff` argument and can be removed with\n",
    "    `high_sd_cutoff=None`.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    low-frequency cutoff in which a word will be \n",
    "    removed if they occur 1 or fewer times. if\n",
    "    desired, this can be changed with the \n",
    "    `low_n_cutoff` argument and can be removed with\n",
    "    `low_n_cutoff=0`.\n",
    "    \n",
    "    By default, compare only adjacent turns. If desired,\n",
    "    the comparison distance may be changed by increasing\n",
    "    the `delay` argument.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 4. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tagger=1`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \n",
    "    By default, the separator between dyad ID and\n",
    "    condition ID in each file name is a hyphen ('\\-'). \n",
    "    If desired, this may be changed with the \n",
    "    `id_separator` argument.\n",
    "\n",
    "    By default, condition IDs in each file name\n",
    "    will be identified as any characters following \n",
    "    `cond`. If desired, this may be changed with the \n",
    "    `condition_label` argument.\n",
    "    \n",
    "    By default, dyad IDs in each file name\n",
    "    will be identified as any characters following \n",
    "    `dyad`. If desired, this may be changed with the \n",
    "    `dyad_label` argument.\n",
    "    \n",
    "    By default, generate surrogates only from a subset\n",
    "    of all possible pairings. If desired, instead \n",
    "    generate surrogates from all possible pairings\n",
    "    with `all_surrogates=True`\n",
    "    \"\"\"\n",
    "    \n",
    "    # grab the files in the input list\n",
    "    file_list = glob.glob(input_file_directory+\"*.txt\")\n",
    "    surrogate_file_list = alex_GenerateSurrogate(original_conversation_list = file_list,\n",
    "                                                   surrogate_file_directory = surrogate_file_directory,\n",
    "                                                   all_surrogates = all_surrogates,\n",
    "                                                   id_separator = id_separator,\n",
    "                                                   condition_label = condition_label,\n",
    "                                                   dyad_label = dyad_label,\n",
    "                                                   keep_original_turn_order = keep_original_turn_order) \n",
    "    \n",
    "    # build the semantic model to be used for all conversations\n",
    "    [vocablist, highDimModel] = alex_BuildSemanticModel(semantic_model_input_file=semantic_model_input_file,\n",
    "                                                        high_sd_cutoff=high_sd_cutoff,\n",
    "                                                        low_n_cutoff=low_n_cutoff)\n",
    "    \n",
    "    # create containers for alignment values\n",
    "    AlignmentT2T = pd.DataFrame()\n",
    "    AlignmentC2C = pd.DataFrame()\n",
    "    \n",
    "    # cycle through the files\n",
    "    for fileName in surrogate_file_list:\n",
    "        \n",
    "        # process the file if it's got a valid conversation\n",
    "        dataframe=pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        if len(dataframe) > 0:\n",
    "            \n",
    "            # let us know which filename we're processing\n",
    "            print \"Processing: \"+fileName   \n",
    "\n",
    "            # calculate turn-by-turn alignment scores\n",
    "            xT2T=alex_TurnByTurnAnalysis(dataframe=dataframe,\n",
    "                                         delay=delay,\n",
    "                                         maxngram=maxngram,\n",
    "                                         vocablist=vocablist,\n",
    "                                         highDimModel=highDimModel)\n",
    "            AlignmentT2T=AlignmentT2T.append(xT2T)\n",
    "            \n",
    "            # calculate conversation-level alignment scores\n",
    "            xC2C = alex_ConvoByConvoAnalysis(dataframe=dataframe,\n",
    "                                             ngramsLength = maxngram,\n",
    "                                             ignore_duplicates=ignore_duplicates,\n",
    "                                             add_stanford_tagger = add_stanford_tagger)\n",
    "            AlignmentC2C=AlignmentC2C.append(xC2C)\n",
    "        \n",
    "        # if it's invalid, let us know\n",
    "        else:\n",
    "            print \"Invalid file: \"+fileName   \n",
    "            \n",
    "    # update final dataframes\n",
    "    FINAL_TURN_SURROGATE = AlignmentT2T.reset_index(drop=True)\n",
    "    FINAL_CONVO_SURROGATE = AlignmentC2C.reset_index(drop=True)\n",
    "    \n",
    "    # export the final files\n",
    "    FINAL_TURN_SURROGATE.to_csv(output_file_directory+\"AlignmentT2T_Surrogate.txt\",\n",
    "                      encoding='utf-8',index=False,sep='\\t')   \n",
    "    FINAL_CONVO_SURROGATE.to_csv(output_file_directory+\"AlignmentC2C_Surrogate.txt\",\n",
    "                       encoding='utf-8',index=False,sep='\\t') \n",
    "\n",
    "    # display the info, too\n",
    "    return FINAL_TURN_SURROGATE, FINAL_CONVO_SURROGATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-27 17:51:58,645 : INFO : collecting all words and their counts\n",
      "2017-11-27 17:51:58,647 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-27 17:51:58,648 : INFO : collected 22 word types from a corpus of 316 raw words and 54 sentences\n",
      "2017-11-27 17:51:58,649 : INFO : Loading a fresh vocabulary\n",
      "2017-11-27 17:51:58,650 : INFO : min_count=1 retains 22 unique words (100% of original 22, drops 0)\n",
      "2017-11-27 17:51:58,652 : INFO : min_count=1 leaves 316 word corpus (100% of original 316, drops 0)\n",
      "2017-11-27 17:51:58,654 : INFO : deleting the raw counts dictionary of 22 items\n",
      "2017-11-27 17:51:58,655 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2017-11-27 17:51:58,657 : INFO : downsampling leaves estimated 51 word corpus (16.2% of prior 316)\n",
      "2017-11-27 17:51:58,658 : INFO : estimated required memory for 22 words and 100 dimensions: 28600 bytes\n",
      "2017-11-27 17:51:58,660 : INFO : resetting layer weights\n",
      "2017-11-27 17:51:58,661 : INFO : training model with 3 workers on 22 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-27 17:51:58,665 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-27 17:51:58,666 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-27 17:51:58,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-27 17:51:58,668 : INFO : training on 1580 raw words (251 effective words) took 0.0s, 61566 effective words/s\n",
      "2017-11-27 17:51:58,669 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511833918.44/SurrogatePair-dyad_13A-dyad_10B-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511833918.44/SurrogatePair-dyad_10A-dyad_12B-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511833918.44/SurrogatePair-dyad_15A-dyad_12B-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511833918.44/SurrogatePair-dyad_10A-dyad_13B-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511833918.44/SurrogatePair-dyad_12A-dyad_15B-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511833918.44/SurrogatePair-dyad_10A-dyad_15B-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511833918.44/SurrogatePair-dyad_15A-dyad_10B-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511833918.44/SurrogatePair-dyad_12A-dyad_10B-condition_1.txt\n"
     ]
    }
   ],
   "source": [
    "[turn_surrogate,convo_surrogate] = alex_PHASE2RUN_SURROGATE(input_file_directory = INPUT_PATH+PREPPED_TRANSCRIPTS, \n",
    "                             surrogate_file_directory= INPUT_PATH+SURROGATE_TRANSCRIPTS,\n",
    "                             output_file_directory= INPUT_PATH+ANALYSIS_READY,\n",
    "                             semantic_model_input_file=INPUT_PATH+'align_concatenated_dataframe.txt',\n",
    "                             high_sd_cutoff=3,\n",
    "                             low_n_cutoff=1,\n",
    "                             id_separator = '\\-',\n",
    "                             condition_label='cond',\n",
    "                             dyad_label='dyad',\n",
    "                             all_surrogates=False,\n",
    "                             keep_original_turn_order = False,\n",
    "                             delay=1,\n",
    "                             maxngram=4,\n",
    "                             ignore_duplicates=True,\n",
    "                             add_stanford_tagger=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Run everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase 1: Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_10-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_10-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_12-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_12-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_15-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_15-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_13-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-original/dyad_13-condition_2.txt\n"
     ]
    }
   ],
   "source": [
    "model_store = alex_PHASE1RUN(input_file_directory=INPUT_PATH+TRANSCRIPTS,\n",
    "                      output_file_directory=INPUT_PATH+PREPPED_TRANSCRIPTS,\n",
    "                      training_dictionary=INPUT_PATH+'big.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase 2: Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-27 17:57:37,564 : INFO : collecting all words and their counts\n",
      "2017-11-27 17:57:37,566 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-27 17:57:37,567 : INFO : collected 22 word types from a corpus of 316 raw words and 54 sentences\n",
      "2017-11-27 17:57:37,568 : INFO : Loading a fresh vocabulary\n",
      "2017-11-27 17:57:37,570 : INFO : min_count=1 retains 22 unique words (100% of original 22, drops 0)\n",
      "2017-11-27 17:57:37,572 : INFO : min_count=1 leaves 316 word corpus (100% of original 316, drops 0)\n",
      "2017-11-27 17:57:37,573 : INFO : deleting the raw counts dictionary of 22 items\n",
      "2017-11-27 17:57:37,575 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2017-11-27 17:57:37,577 : INFO : downsampling leaves estimated 51 word corpus (16.2% of prior 316)\n",
      "2017-11-27 17:57:37,579 : INFO : estimated required memory for 22 words and 100 dimensions: 28600 bytes\n",
      "2017-11-27 17:57:37,581 : INFO : resetting layer weights\n",
      "2017-11-27 17:57:37,583 : INFO : training model with 3 workers on 22 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-27 17:57:37,592 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-27 17:57:37,594 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-27 17:57:37,596 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-27 17:57:37,598 : INFO : training on 1580 raw words (251 effective words) took 0.0s, 34080 effective words/s\n",
      "2017-11-27 17:57:37,600 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_10-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_10-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_12-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_12-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_15-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_15-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_13-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-prepped/dyad_13-condition_2.txt\n"
     ]
    }
   ],
   "source": [
    "[turn_real,convo_real]=alex_PHASE2RUN_REAL(input_file_directory = INPUT_PATH+PREPPED_TRANSCRIPTS, \n",
    "                        output_file_directory = INPUT_PATH+ANALYSIS_READY,\n",
    "                        semantic_model_input_file = INPUT_PATH+'align_concatenated_dataframe.txt',\n",
    "                        high_sd_cutoff=3,\n",
    "                        low_n_cutoff=1,\n",
    "                        delay=1,\n",
    "                        maxngram=4,\n",
    "                        ignore_duplicates=True,\n",
    "                        add_stanford_tagger=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase 2: Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-27 17:57:38,102 : INFO : collecting all words and their counts\n",
      "2017-11-27 17:57:38,103 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-27 17:57:38,105 : INFO : collected 22 word types from a corpus of 316 raw words and 54 sentences\n",
      "2017-11-27 17:57:38,106 : INFO : Loading a fresh vocabulary\n",
      "2017-11-27 17:57:38,107 : INFO : min_count=1 retains 22 unique words (100% of original 22, drops 0)\n",
      "2017-11-27 17:57:38,108 : INFO : min_count=1 leaves 316 word corpus (100% of original 316, drops 0)\n",
      "2017-11-27 17:57:38,110 : INFO : deleting the raw counts dictionary of 22 items\n",
      "2017-11-27 17:57:38,111 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2017-11-27 17:57:38,112 : INFO : downsampling leaves estimated 51 word corpus (16.2% of prior 316)\n",
      "2017-11-27 17:57:38,114 : INFO : estimated required memory for 22 words and 100 dimensions: 28600 bytes\n",
      "2017-11-27 17:57:38,115 : INFO : resetting layer weights\n",
      "2017-11-27 17:57:38,116 : INFO : training model with 3 workers on 22 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-27 17:57:38,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-27 17:57:38,121 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-27 17:57:38,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-27 17:57:38,123 : INFO : training on 1580 raw words (251 effective words) took 0.0s, 55310 effective words/s\n",
      "2017-11-27 17:57:38,124 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511834257.9/SurrogatePair-dyad_10A-dyad_12B-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511834257.9/SurrogatePair-dyad_15A-dyad_12B-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511834257.9/SurrogatePair-dyad_15A-dyad_12B-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511834257.9/SurrogatePair-dyad_12A-dyad_15B-condition_2.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511834257.9/SurrogatePair-dyad_12A-dyad_15B-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511834257.9/SurrogatePair-dyad_10A-dyad_15B-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511834257.9/SurrogatePair-dyad_15A-dyad_10B-condition_1.txt\n",
      "Processing: /Users/alexandra/Dropbox/DA/DA_linguisticAlignment/working/ALIGN_NOTEBOOK_CLEAN/toy_data-surrogate/surrogate_run-1511834257.9/SurrogatePair-dyad_12A-dyad_10B-condition_2.txt\n"
     ]
    }
   ],
   "source": [
    "[turn_surrogate,convo_surrogate] = alex_PHASE2RUN_SURROGATE(input_file_directory = INPUT_PATH+PREPPED_TRANSCRIPTS, \n",
    "                             surrogate_file_directory= INPUT_PATH+SURROGATE_TRANSCRIPTS,\n",
    "                             output_file_directory= INPUT_PATH+ANALYSIS_READY,\n",
    "                             semantic_model_input_file=INPUT_PATH+'align_concatenated_dataframe.txt',\n",
    "                             high_sd_cutoff=3,\n",
    "                             low_n_cutoff=1,\n",
    "                             id_separator = '\\-',\n",
    "                             condition_label='cond',\n",
    "                             dyad_label='dyad',\n",
    "                             all_surrogates=False,\n",
    "                             keep_original_turn_order = False,\n",
    "                             delay=1,\n",
    "                             maxngram=4,\n",
    "                             ignore_duplicates=True,\n",
    "                             add_stanford_tagger=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Speed calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Phase 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.144092082977295"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_phase2real - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Phase 2 real time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33298707008361816"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_phase2surrogate - start_phase2real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Phase 2 surrogate time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48375582695007324"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end - start_phase2surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All 3 phases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.960834980010986"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Printouts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>cosine_lexical_tok2</th>\n",
       "      <th>cosine_lexical_tok3</th>\n",
       "      <th>condition_info</th>\n",
       "      <th>cosine_lexical_lem4</th>\n",
       "      <th>cosine_lexical_tok4</th>\n",
       "      <th>cosine_lexical_lem3</th>\n",
       "      <th>cosine_syntax_penn_tok4</th>\n",
       "      <th>cosine_syntax_penn_tok3</th>\n",
       "      <th>cosine_syntax_penn_tok2</th>\n",
       "      <th>cosine_syntax_penn_lex2</th>\n",
       "      <th>cosine_syntax_penn_lex3</th>\n",
       "      <th>cosine_syntax_penn_lex4</th>\n",
       "      <th>cosine_semanticL</th>\n",
       "      <th>partner_direction</th>\n",
       "      <th>cosine_lexical_lem2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571742</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.08703883</td>\n",
       "      <td>0</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6656333</td>\n",
       "      <td>2&gt;1.0</td>\n",
       "      <td>0.08703883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3349915</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.40693</td>\n",
       "      <td>2&gt;1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.1443376</td>\n",
       "      <td>0</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4127169</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0.1443376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3651285</td>\n",
       "      <td>2&gt;1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.7071068</td>\n",
       "      <td>0</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0.7071068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0.2886751</td>\n",
       "      <td>0</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658746</td>\n",
       "      <td>2&gt;1.0</td>\n",
       "      <td>0.2886751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1233059</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3596222</td>\n",
       "      <td>2&gt;1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time cosine_lexical_tok2 cosine_lexical_tok3           condition_info  \\\n",
       "0     0                   0                   0  dyad_10-condition_1.txt   \n",
       "1     1          0.08703883                   0  dyad_10-condition_1.txt   \n",
       "2     2                   0                   0  dyad_10-condition_1.txt   \n",
       "3     3                   0                   0  dyad_10-condition_1.txt   \n",
       "4     4           0.1443376                   0  dyad_10-condition_1.txt   \n",
       "5     0                   0                   0  dyad_10-condition_2.txt   \n",
       "6     1           0.7071068                   0  dyad_10-condition_2.txt   \n",
       "7     2           0.2886751                   0  dyad_10-condition_2.txt   \n",
       "8     3                   0                   0  dyad_10-condition_2.txt   \n",
       "9     4                   0                   0  dyad_10-condition_2.txt   \n",
       "\n",
       "  cosine_lexical_lem4 cosine_lexical_tok4 cosine_lexical_lem3  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "5                   0                   0                   0   \n",
       "6                   0                   0                   0   \n",
       "7                   0                   0                   0   \n",
       "8                   0                   0                   0   \n",
       "9                   0                   0                   0   \n",
       "\n",
       "  cosine_syntax_penn_tok4 cosine_syntax_penn_tok3 cosine_syntax_penn_tok2  \\\n",
       "0                       0                       0                       0   \n",
       "1                       0                       0                       0   \n",
       "2                       0                       0                       0   \n",
       "3                       0                       0                       0   \n",
       "4                       0                       0                       0   \n",
       "5                       0                       0                       0   \n",
       "6                       0                       0                       0   \n",
       "7                       0                       0                       0   \n",
       "8                       0                       0                       0   \n",
       "9                       0                       0                       0   \n",
       "\n",
       "  cosine_syntax_penn_lex2 cosine_syntax_penn_lex3 cosine_syntax_penn_lex4  \\\n",
       "0                       0                       0                       0   \n",
       "1                       0                       0                       0   \n",
       "2                       0                       0                       0   \n",
       "3                       0                       0                       0   \n",
       "4                       0                       0                       0   \n",
       "5                       0                       0                       0   \n",
       "6                       0                       0                       0   \n",
       "7                       0                       0                       0   \n",
       "8                       0                       0                       0   \n",
       "9                       0                       0                       0   \n",
       "\n",
       "  cosine_semanticL partner_direction cosine_lexical_lem2  \n",
       "0         0.571742             1>2.0                   0  \n",
       "1        0.6656333             2>1.0          0.08703883  \n",
       "2        0.3349915             1>2.0                   0  \n",
       "3          0.40693             2>1.0                   0  \n",
       "4        0.4127169             1>2.0           0.1443376  \n",
       "5        0.3651285             2>1.0                   0  \n",
       "6                1             1>2.0           0.7071068  \n",
       "7         0.658746             2>1.0           0.2886751  \n",
       "8        0.1233059             1>2.0                   0  \n",
       "9        0.3596222             2>1.0                   0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosine_lexical_tok2</th>\n",
       "      <th>cosine_lexical_tok3</th>\n",
       "      <th>cosine_lexical_lem4</th>\n",
       "      <th>cosine_lexical_tok4</th>\n",
       "      <th>cosine_lexical_lem3</th>\n",
       "      <th>cosine_syntax_penn_tok4</th>\n",
       "      <th>cosine_syntax_penn_tok3</th>\n",
       "      <th>cosine_syntax_penn_tok2</th>\n",
       "      <th>cosine_syntax_penn_lex2</th>\n",
       "      <th>cosine_syntax_penn_lex3</th>\n",
       "      <th>cosine_syntax_penn_lex4</th>\n",
       "      <th>cosine_lexical_lem2</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.109254</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.081786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.081786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.081786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.081786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.113228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.109254</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cosine_lexical_tok2  cosine_lexical_tok3  cosine_lexical_lem4  \\\n",
       "0             0.081786                    0                    0   \n",
       "1             0.113228                    0                    0   \n",
       "2             0.081786                    0                    0   \n",
       "3             0.081786                    0                    0   \n",
       "4             0.081786                    0                    0   \n",
       "5             0.081786                    0                    0   \n",
       "6             0.081786                    0                    0   \n",
       "7             0.113228                    0                    0   \n",
       "\n",
       "   cosine_lexical_tok4  cosine_lexical_lem3  cosine_syntax_penn_tok4  \\\n",
       "0                    0                    0                        0   \n",
       "1                    0                    0                        0   \n",
       "2                    0                    0                        0   \n",
       "3                    0                    0                        0   \n",
       "4                    0                    0                        0   \n",
       "5                    0                    0                        0   \n",
       "6                    0                    0                        0   \n",
       "7                    0                    0                        0   \n",
       "\n",
       "   cosine_syntax_penn_tok3  cosine_syntax_penn_tok2  cosine_syntax_penn_lex2  \\\n",
       "0                        0                        0                        0   \n",
       "1                        0                        0                        0   \n",
       "2                        0                        0                        0   \n",
       "3                        0                        0                        0   \n",
       "4                        0                        0                        0   \n",
       "5                        0                        0                        0   \n",
       "6                        0                        0                        0   \n",
       "7                        0                        0                        0   \n",
       "\n",
       "   cosine_syntax_penn_lex3  cosine_syntax_penn_lex4  cosine_lexical_lem2  \\\n",
       "0                        0                        0             0.078446   \n",
       "1                        0                        0             0.109254   \n",
       "2                        0                        0             0.078446   \n",
       "3                        0                        0             0.078446   \n",
       "4                        0                        0             0.078446   \n",
       "5                        0                        0             0.078446   \n",
       "6                        0                        0             0.078446   \n",
       "7                        0                        0             0.109254   \n",
       "\n",
       "            condition_info  \n",
       "0  dyad_10-condition_1.txt  \n",
       "1  dyad_10-condition_2.txt  \n",
       "2  dyad_12-condition_1.txt  \n",
       "3  dyad_12-condition_2.txt  \n",
       "4  dyad_15-condition_2.txt  \n",
       "5  dyad_15-condition_1.txt  \n",
       "6  dyad_13-condition_1.txt  \n",
       "7  dyad_13-condition_2.txt  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>cosine_lexical_tok2</th>\n",
       "      <th>cosine_lexical_tok3</th>\n",
       "      <th>condition_info</th>\n",
       "      <th>cosine_lexical_lem4</th>\n",
       "      <th>cosine_lexical_tok4</th>\n",
       "      <th>cosine_lexical_lem3</th>\n",
       "      <th>cosine_syntax_penn_tok4</th>\n",
       "      <th>cosine_syntax_penn_tok3</th>\n",
       "      <th>cosine_syntax_penn_tok2</th>\n",
       "      <th>cosine_syntax_penn_lex2</th>\n",
       "      <th>cosine_syntax_penn_lex3</th>\n",
       "      <th>cosine_syntax_penn_lex4</th>\n",
       "      <th>cosine_semanticL</th>\n",
       "      <th>partner_direction</th>\n",
       "      <th>cosine_lexical_lem2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_2-dyad_15-dyad_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1233059</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_2-dyad_15-dyad_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3596222</td>\n",
       "      <td>2&gt;1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_2-dyad_15-dyad_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.03504958</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_2-dyad_15-dyad_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.005377397</td>\n",
       "      <td>2&gt;1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_2-dyad_15-dyad_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3349915</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1-dyad_12-dyad_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3349915</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1-dyad_12-dyad_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3692705</td>\n",
       "      <td>2&gt;1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1-dyad_12-dyad_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2776606</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1443376</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1-dyad_12-dyad_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4127169</td>\n",
       "      <td>2&gt;1.0</td>\n",
       "      <td>0.1443376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1-dyad_12-dyad_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.353934</td>\n",
       "      <td>1&gt;2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time cosine_lexical_tok2 cosine_lexical_tok3               condition_info  \\\n",
       "0     0                   0                   0  condition_2-dyad_15-dyad_13   \n",
       "1     1                   0                   0  condition_2-dyad_15-dyad_13   \n",
       "2     2                   0                   0  condition_2-dyad_15-dyad_13   \n",
       "3     3                   0                   0  condition_2-dyad_15-dyad_13   \n",
       "4     4                   0                   0  condition_2-dyad_15-dyad_13   \n",
       "5     0                   0                   0  condition_1-dyad_12-dyad_13   \n",
       "6     1                   0                   0  condition_1-dyad_12-dyad_13   \n",
       "7     2                   0                   0  condition_1-dyad_12-dyad_13   \n",
       "8     3           0.1443376                   0  condition_1-dyad_12-dyad_13   \n",
       "9     4                   0                   0  condition_1-dyad_12-dyad_13   \n",
       "\n",
       "  cosine_lexical_lem4 cosine_lexical_tok4 cosine_lexical_lem3  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "5                   0                   0                   0   \n",
       "6                   0                   0                   0   \n",
       "7                   0                   0                   0   \n",
       "8                   0                   0                   0   \n",
       "9                   0                   0                   0   \n",
       "\n",
       "  cosine_syntax_penn_tok4 cosine_syntax_penn_tok3 cosine_syntax_penn_tok2  \\\n",
       "0                       0                       0                       0   \n",
       "1                       0                       0                       0   \n",
       "2                       0                       0                       0   \n",
       "3                       0                       0                       0   \n",
       "4                       0                       0                       0   \n",
       "5                       0                       0                       0   \n",
       "6                       0                       0                       0   \n",
       "7                       0                       0                       0   \n",
       "8                       0                       0                       0   \n",
       "9                       0                       0                       0   \n",
       "\n",
       "  cosine_syntax_penn_lex2 cosine_syntax_penn_lex3 cosine_syntax_penn_lex4  \\\n",
       "0                       0                       0                       0   \n",
       "1                       0                       0                       0   \n",
       "2                       0                       0                       0   \n",
       "3                       0                       0                       0   \n",
       "4                       0                       0                       0   \n",
       "5                       0                       0                       0   \n",
       "6                       0                       0                       0   \n",
       "7                       0                       0                       0   \n",
       "8                       0                       0                       0   \n",
       "9                       0                       0                       0   \n",
       "\n",
       "  cosine_semanticL partner_direction cosine_lexical_lem2  \n",
       "0        0.1233059             1>2.0                   0  \n",
       "1        0.3596222             2>1.0                   0  \n",
       "2      -0.03504958             1>2.0                   0  \n",
       "3     -0.005377397             2>1.0                   0  \n",
       "4        0.3349915             1>2.0                   0  \n",
       "5        0.3349915             1>2.0                   0  \n",
       "6        0.3692705             2>1.0                   0  \n",
       "7        0.2776606             1>2.0                   0  \n",
       "8        0.4127169             2>1.0           0.1443376  \n",
       "9         0.353934             1>2.0                   0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_surrogate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosine_lexical_tok2</th>\n",
       "      <th>cosine_lexical_tok3</th>\n",
       "      <th>cosine_lexical_lem4</th>\n",
       "      <th>cosine_lexical_tok4</th>\n",
       "      <th>cosine_lexical_lem3</th>\n",
       "      <th>cosine_syntax_penn_tok4</th>\n",
       "      <th>cosine_syntax_penn_tok3</th>\n",
       "      <th>cosine_syntax_penn_tok2</th>\n",
       "      <th>cosine_syntax_penn_lex2</th>\n",
       "      <th>cosine_syntax_penn_lex3</th>\n",
       "      <th>cosine_syntax_penn_lex4</th>\n",
       "      <th>cosine_lexical_lem2</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.057831</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055470</td>\n",
       "      <td>condition_2-dyad_15-dyad_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>condition_1-dyad_12-dyad_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.081786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>condition_2-dyad_12-dyad_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.101274</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101274</td>\n",
       "      <td>condition_2-dyad_15-dyad_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.057831</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055470</td>\n",
       "      <td>condition_1-dyad_12-dyad_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.081786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>condition_2-dyad_12-dyad_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.057831</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055470</td>\n",
       "      <td>condition_1-dyad_10-dyad_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.081786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>condition_1-dyad_10-dyad_15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cosine_lexical_tok2  cosine_lexical_tok3  cosine_lexical_lem4  \\\n",
       "0             0.057831                    0                    0   \n",
       "1             0.081786                    0                    0   \n",
       "2             0.081786                    0                    0   \n",
       "3             0.101274                    0                    0   \n",
       "4             0.057831                    0                    0   \n",
       "5             0.081786                    0                    0   \n",
       "6             0.057831                    0                    0   \n",
       "7             0.081786                    0                    0   \n",
       "\n",
       "   cosine_lexical_tok4  cosine_lexical_lem3  cosine_syntax_penn_tok4  \\\n",
       "0                    0                    0                        0   \n",
       "1                    0                    0                        0   \n",
       "2                    0                    0                        0   \n",
       "3                    0                    0                        0   \n",
       "4                    0                    0                        0   \n",
       "5                    0                    0                        0   \n",
       "6                    0                    0                        0   \n",
       "7                    0                    0                        0   \n",
       "\n",
       "   cosine_syntax_penn_tok3  cosine_syntax_penn_tok2  cosine_syntax_penn_lex2  \\\n",
       "0                        0                        0                        0   \n",
       "1                        0                        0                        0   \n",
       "2                        0                        0                        0   \n",
       "3                        0                        0                        0   \n",
       "4                        0                        0                        0   \n",
       "5                        0                        0                        0   \n",
       "6                        0                        0                        0   \n",
       "7                        0                        0                        0   \n",
       "\n",
       "   cosine_syntax_penn_lex3  cosine_syntax_penn_lex4  cosine_lexical_lem2  \\\n",
       "0                        0                        0             0.055470   \n",
       "1                        0                        0             0.078446   \n",
       "2                        0                        0             0.078446   \n",
       "3                        0                        0             0.101274   \n",
       "4                        0                        0             0.055470   \n",
       "5                        0                        0             0.078446   \n",
       "6                        0                        0             0.055470   \n",
       "7                        0                        0             0.078446   \n",
       "\n",
       "                condition_info  \n",
       "0  condition_2-dyad_15-dyad_13  \n",
       "1  condition_1-dyad_12-dyad_13  \n",
       "2  condition_2-dyad_12-dyad_15  \n",
       "3  condition_2-dyad_15-dyad_13  \n",
       "4  condition_1-dyad_12-dyad_13  \n",
       "5  condition_2-dyad_12-dyad_15  \n",
       "6  condition_1-dyad_10-dyad_15  \n",
       "7  condition_1-dyad_10-dyad_15  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_surrogate.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
