{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# To-do list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Things for Alex to do:**\n",
    "* [ ] Handling requirements (after getting them)\n",
    "* [ ] Dockerizing\n",
    "* [ ] Jupyter app-ifying\n",
    "* [ ] Getting Stanford tagger included automatically\n",
    "* [ ] Clean up markdown text (when final notebooks are ready)\n",
    "* [ ] See if I can implement w2v function (https://github.com/a-paxton/Gensim-LSI-Word-Similarities)\n",
    "* [ ] Convert functions into library\n",
    "* [ ] When run analysis, run syntax over token, lemma is weird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Things for Nick to do:**\n",
    "* [x] Implement surrogate to match by conversation order AND conversation type\n",
    "* [x] Make file names more intuitive\n",
    "* [ ] Identify condition/dyad/number flexibly (using regex) - SKIPPED\n",
    "* [x] Allow surrogate baseline to be created using a smaller subset (permutations) â€” 2-3x?\n",
    "* [**???**] Do pip freeze or conda list -e > req.txt\n",
    "* [**???**] Redo analysis with new baseline + consider doing sample-wise shuffled baseline - Have questions on how to proceed\n",
    "* [ ] Go over manuscript again with new baseline + review comments/edits\n",
    "* [ ] Need to create a simple other_filler_list as a text file that can be modified by a user and imported to be used here - make note that we only catch 2-letter fillers at this point with the regular expression default \n",
    "* [ ] Note that align_concatenated_dataframe.txt takes the place of forSemantic.txt. Make updates accordingly. \n",
    "* [ ] We could/should probably make `convobyconvo` an optional add-on from `turnbyturn`.\n",
    "* [ ] Consider other POS taggers: https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag\n",
    "* [ ] Create semantic space using the TASA corpus to make available to users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ALIGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook provides an introduction to **ALIGN**, a tool for quantifying multi-level linguistic similarity between speakers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Table of Contents**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* [Getting Started](#Getting-Started)\n",
    "    * [Prerequisites](#Prerequisites)\n",
    "    * [Preparing input data](#Preparing-input-data)\n",
    "    * [Filename conventions](#Filename-conventions)\n",
    "    * [User-specified parameters](#User-specified-parameters)\n",
    "    * [Main calls](#Main-calls)\n",
    "* [Setup](#Setup)\n",
    "    * [Import libraries](#Import-libraries)\n",
    "    * [User-specified settings](#User-specified-settings)\n",
    "* [Phase 1: Generate \"prepped\" transcripts](#Phase-1:-Generate-\"prepped\"-transcripts)\n",
    "    * [](#)\n",
    "* [Phase 2: Generate alignment scores](#Phase-2:-Generate-alignment-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Jupyter Notebook with Python 2.7.1.3 kernel\n",
    "* Packages in `requirements.txt`\n",
    "\n",
    "*See notes in \"DISTRIBUTION ISSUES\" Notebook for suggestions on how to package effectively and accomodate Python 3 users*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Nick**: Is the above reference still accurate? I don't see such a notebook now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preparing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Each input text file needs to contain a single conversation organized in an `N x 2` matrix\n",
    "    * Text file must be tab-delimited.\n",
    "* Each row must correspond to a single conversational turn from a speaker.\n",
    "    * Rows must be temporally ordered based on their occurrence in the conversation.\n",
    "    * Rows must alternate between speakers.\n",
    "* Speaker identifier and content for each turn are divided across two columns.\n",
    "    * Column 1 must have the header `participant`.\n",
    "        * Each cell specifies the speaker.\n",
    "        * Each speaker must have a unique label (e.g., `P1` and `P2`, `0` and `1`).\n",
    "    * Column 2 must have the header `content`.\n",
    "        * Each cell corresponds to the transcribed utterance from the speaker.\n",
    "        * Each cell must end with a newline character: `\\n`\n",
    "* See folder `examples > toy_data-original` in Github repository for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Filename conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Each conversation text file needs to be named in the format: `A_B.txt`\n",
    "    * `A` corresponds to the dyad number for that conversation\n",
    "    * `B` corresponding to a condition code for that conversation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### User-specified parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Define input path\n",
    "* Define input folder where original transcripts are located\n",
    "* Define folder to save prepped transcripts \n",
    "* Define folder to save surrogate transcripts \n",
    "* Decide maximum size for n-gram chunking\n",
    "    * Default: 3\n",
    "* Decide the minimum number of words for each turn\n",
    "    * Default: 3\n",
    "    * CRITICAL: The minimum number of words has to be at least as long as n-gram maximum size otherwise error will be generated\n",
    "* Decide on whether to run the Stanford tagger along with NLTK default tagger (slow) or NLTK tagger alone (fast)\n",
    "    * Default: 0 (NTLK tagger alone)\n",
    "    \n",
    "* remove_regex_fillers\n",
    "* remove_other_list\n",
    "    * need to combine the above into an either use regular expression or user-generated list\n",
    "    * now USE_FILLER_LIST\n",
    "    \n",
    "* Decide on max delay between partner's turns to generate alignment score \n",
    "    * Currently only option is for contiguous turns\n",
    "    * Will be updated in a future version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Main calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`PHASE1RUN`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Converts each conversation into standardized format.\n",
    "* Each utterance is tokenized and lemmatized and has POS tags added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`PHASE2RUN_REAL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Generates turn-level and conversation-level alignment scores (lexical, conceptual, and syntactic) across a range of n-gram sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`PHASE2RUN_SURROGATE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Generates a surrogate corpus.\n",
    "* Runs identical analysis as PHASE2RUN_REAL on the surrogate corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Checking latest version of Python and 3rd-party packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Version Info:\n",
      "0.21.1\n",
      "Numpy Version Info:\n",
      "1.11.3\n",
      "Scipy Version Info:\n",
      "0.19.0\n",
      "NLTK Version Info:\n",
      "3.2.5\n",
      "Gensim Version Info:\n",
      "3.1.0\n",
      "Python and Conda Environment Info:\n",
      "2.7.13 |Anaconda 2.3.0 (x86_64)| (default, Dec 20 2016, 23:05:08) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import scipy\n",
    "import nltk\n",
    "import gensim \n",
    "\n",
    "print(\"Pandas Version Info:\\n{}\".format(pandas.__version__))\n",
    "print(\"Numpy Version Info:\\n{}\".format(numpy.__version__))\n",
    "print(\"Scipy Version Info:\\n{}\".format(scipy.__version__))\n",
    "print(\"NLTK Version Info:\\n{}\".format(nltk.__version__))\n",
    "print(\"Gensim Version Info:\\n{}\".format(gensim.__version__))\n",
    "\n",
    "import sys\n",
    "print(\"Python and Conda Environment Info:\\n{}\".format(sys.version))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, we'll get ready to run ALIGN over our target dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[To top](#ALIGN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os,re,math,csv,string,random,logging,glob,itertools,operator\n",
    "from os import listdir \n",
    "from os.path import isfile, join \n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Third-party libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For data analysis and data handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For natural language processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet as wn \n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Download the NLTK default POS tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing collection member with no package: hmm_treebank_pos_tagger\n",
      "removing collection member with no package: hmm_treebank_pos_tagger\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nduran/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note:** With older version of NLTK (pre 3.1), the `maxent_treebank_pos_tagger` is also available. If desired, uncomment and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('maxent_treebank_pos_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note**: The `StanfordPOSTagger` will be\n",
    "used in conjunction with local folder `stanford-postagger-2017-06-09/` and `.jar` file. The `StanfordPOSTagger` also uses the trained model: `english-left3words-distsim.tagger`. These files will be called below if analysis is being run with the Stanford tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For building semantic space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## User-specified settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Directories and folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`INPUT_PATH`: Set working directory, in which all notebook and supporting files are located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or \"Pathname for the unzipped project folder\" if going with `anaconda-project.yml` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "INPUT_PATH=os.getcwd()+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`TRANSCRIPTS`: Set variable for folder name (as string) for relative location of folder containing the original transcript files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRANSCRIPTS = 'toy_data-original/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`STANFORD_PATH`: Path to Stanford POS tagger files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "STANFORD_POS_PATH = INPUT_PATH + 'package_files/stanford-postagger-2017-06-09/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`PREPPED_TRANSCRIPTS`: Set variable for folder name (as string) for relative location of folder into which prepared transcript files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PREPPED_TRANSCRIPTS = 'toy_data-prepped/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`ANALYSIS_READY`: Set variable for folder name (as string) for relative location of folder into which analysis-ready dataframe files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ANALYSIS_READY = 'toy_data-analysis/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`SURROGATE_TRANSCRIPTS`: Set variable for folder name (as string) for relative location of folder into which all prepared surrogate transcript files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SURROGATE_TRANSCRIPTS = 'toy_data-surrogate/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analysis settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`MAXNGRAM`: Set maximum size for n-gram chunking.\n",
    "\n",
    "* Default: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAXNGRAM = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`MINWORDS`: Set minimum number of words for each turn.\n",
    "\n",
    "* Default: 2\n",
    "\n",
    "**Note**: The minimum number of words must be at least as long as maximum *n*-gram size (`MAXNGRAM` above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MINWORDS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`ADD_STANFORD_TAGS`: Choose POS tagger. \n",
    "\n",
    "* Default: `0`\n",
    "    * Run NLTK default POS tagger (NLTK 3.1+): `averaged_perceptron_tagger`\n",
    "* Option: `1`\n",
    "    * Run both NLTK default POS tagger and Stanford POS tagger. Note: Adding the Stanford POS tagger will lead to an increase in processing time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ADD_STANFORD_TAGS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DELAY`: Set max delay between partner's turns when generating alignment score.\n",
    "\n",
    "* Currently, the only acceptable value is 1 (i.e., contiguous turns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DELAY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`USE_FILLER_LIST`: Choose method for removing speech fillers. \n",
    "\n",
    "* Default: `None`\n",
    "    * Does not provide additional speech fillers to be removed.\n",
    "* Option: list of strings\n",
    "    * Provide a list of literal strings to be removed from the transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "USE_FILLER_LIST = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`IGNORE_DUPLICATES`: Choose whether to remove duplicate lexial bigrams when computing syntactic alignment\n",
    "\n",
    "* Default: `True`\n",
    "    * Removes duplicate lexical bigrams.\n",
    "* Option `False`\n",
    "    * Keeps duplicate lexical bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IGNORE_DUPLICATES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`USE_PRETRAINED_VECTORS`: Choose whether to use high-dimensional semantic model pretrained vectors from GoogleNews or to build vectors based on transcripts (each utterance/row is equivalent to a single context). Note: if there are a small number of utterance/rows then the pretrained vectors should be used. \n",
    "\n",
    "* Default: `False`\n",
    "    * Builds high-dimensional based on input transcript\n",
    "* Option `True`\n",
    "    * Uses pre-trained vectors from GoogleNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "USE_PRETRAINED_VECTORS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phase 1: Generate \"prepped\" transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initial clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **[Clean up text](#Clean-up-text)** by removing:\n",
    "    * numbers, punctuation, and other non-ASCII alphabet characters\n",
    "    * common speech fillers (e.g., \"um\", \"huh\") and their derivations\n",
    "    * empty turns that may have inadvertently been included\n",
    "    * user-specified short turns\n",
    "        * removes short turns that are at least as long as maximum n-gram\n",
    "* **[Merge adjacent turns by the same participant](#Merge-adjacent-turns-by-the-same-participant)** into a single utterance row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[To top](#ALIGN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Clean up text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Nick**: The list of filler words would be passed to the function when it's called, not during the function specification. Essentially, this would look like the following example code cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```\n",
    "# from specified list\n",
    "specified_filler_list = ['ignore','these','words']\n",
    "InitialCleanup(raw_dataframe, minwords=2, use_filler_list = specified_filler_list)\n",
    "\n",
    "# from file containing list\n",
    "loaded_filler_list = list(pd.read_csv('filler_list_file.txt',squeeze=True))\n",
    "InitialCleanup(raw_dataframe, minwords=2, use_filler_list = loaded_filler_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Alex** Sounds good. I made the changes to the code. Could you please double check? If looks good, please delete these notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Alex** Also checkout below the new regular expressions. I think I finally figured out how to remove all the fillers that are of various sizes without affecting any other words. I've tested with the larger transcripts and all looks good. Now super clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def InitialCleanup(dataframe,\n",
    "                   minwords=2,\n",
    "                   use_filler_list=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform basic text cleaning to prepare dataframe\n",
    "    for analysis. Remove non-letter/-space characters,\n",
    "    empty turns, turns below a minimum length, and \n",
    "    fillers.\n",
    "    \n",
    "    By default, remove turns shorter than 3 words long.\n",
    "    If desired, this may be changed by updating the\n",
    "    `minwords` argument.\n",
    "    \n",
    "    By default, remove 2-letter fillers through regex.\n",
    "    If desired, remove other words by passing a list\n",
    "    of literal strings to `use_filler_list` argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    # only allow strings, spaces, and newlines to pass\n",
    "    WHITELIST = string.letters + '\\'' + ' '\n",
    "    clean = []\n",
    "    utteranceLen = []\n",
    "     \n",
    "    # remove inadvertent empty turns \n",
    "    dataframe = dataframe[pd.notnull(dataframe['content'])]\n",
    "    \n",
    "    for value in dataframe['content'].values:            \n",
    "        cleantext = ''.join(c for c in value if c in WHITELIST).lower() \n",
    "        \n",
    "        # DEFAULT: remove typical speech fillers examples: \"um, mm, oh, hm, uh, ha\")\n",
    "        if use_filler_list == None:                                \n",
    "            cleantext = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\\s', ' ', cleantext) # at the start of a string\n",
    "            cleantext = re.sub('\\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\\s', ' ', cleantext) # within a string \n",
    "            cleantext = re.sub('\\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', cleantext) # end of a string \n",
    "        # OPTIONAL: remove speech fillers or other words specified by user in a list\n",
    "        if use_filler_list != None:\n",
    "            cleantext = [word for word in cleantext.split(\" \") if word not in use_filler_list]\n",
    "            cleantext = \" \".join(cleantext)\n",
    "                \n",
    "        # append cleaned lines\n",
    "        clean.append(cleantext)        \n",
    "                \n",
    "    # drop the old \"content\" column and add the clean \"content\" column\n",
    "    dataframe = dataframe.iloc[:, [0,1]]\n",
    "    dataframe['content'] = clean\n",
    "        \n",
    "    # remove rows that are now blank or do not meet `minwords` requirement, then drop length column    \n",
    "    dataframe['utteranceLen'] = dataframe['content'].apply(lambda x: word_tokenize(x)).str.len()\n",
    "    dataframe = dataframe.drop(dataframe[dataframe.utteranceLen < int(minwords)].index)\n",
    "    dataframe = dataframe.iloc[:, [0,1]]\n",
    "        \n",
    "    # return the cleaned dataframe    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Merge adjacent turns by the same participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def AdjacentMerge(dataframe):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a dataframe of conversation turns,\n",
    "    merge adjacent turns by the same speaker.\n",
    "    \"\"\"    \n",
    "    \n",
    "    repeat=1\n",
    "    while repeat==1:\n",
    "        l1=len(dataframe) \n",
    "        DfMerge = []\n",
    "        k = 0\n",
    "        if len(dataframe) > 0:\n",
    "            while k < len(dataframe)-1: \n",
    "                if dataframe['participant'].iloc[k] != dataframe['participant'].iloc[k+1]:\n",
    "                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])         \n",
    "                    k = k + 1\n",
    "                elif dataframe['participant'].iloc[k] == dataframe['participant'].iloc[k+1]:                    \n",
    "                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k] + \" \" + dataframe['content'].iloc[k+1]])           \n",
    "                    k = k + 2   \n",
    "            if k == len(dataframe)-1:\n",
    "                DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])      \n",
    "        \n",
    "        dataframe=pd.DataFrame(DfMerge,columns=('participant','content'))\n",
    "        if l1==len(dataframe): \n",
    "            repeat=0 \n",
    "                \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare transcript text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **[Check spelling](#Check-spelling)** via a Bayesian spell-checking algorithm (http://norvig.com/spell-correct.html).\n",
    "* **[Tokenize and apply spell correction](#Tokenize-and-apply-spell-correction)** to the original transcript text.\n",
    "* **[Lemmatize](#Lemmatize)** using WordNet-derived categories.\n",
    "* [**Part-of-speech tagging**](#Part-of-speech-tagging) with user-defined tagger(s) on both lemmatized and non-lemmatized tokens.\n",
    "    * Users may choose to use the NLTK default POS tagger (default) and/or the Stanford POS tagger (optional). The NLTK default tagger is more time-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize and apply spell correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Nick**: If you're okay with specifying the contractions here, we should also delete the `.txt` file from the enclosing folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Alex**: Got it. Now deleted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Tokenize(text,nwords):\n",
    "    \"\"\"\n",
    "    Given list of text to be processed and a list \n",
    "    of known words, return a list of edited and \n",
    "    tokenized words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # internal function: identify possible spelling errors for a given word\n",
    "    def edits1(word): \n",
    "        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes    = [a + b[1:] for a, b in splits if b]\n",
    "        transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "        replaces   = [a + c + b[1:] for a, b in splits for c in string.lowercase if b]\n",
    "        inserts    = [a + c + b     for a, b in splits for c in string.lowercase]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    # internal function: identify known edits\n",
    "    def known_edits2(word,nwords):\n",
    "        return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in nwords)\n",
    "\n",
    "    # internal function: identify known words\n",
    "    def known(words,nwords): return set(w for w in words if w in nwords)\n",
    "\n",
    "    # internal function: correct spelling\n",
    "    def correct(word,nwords):\n",
    "        candidates = known([word],nwords) or known(edits1(word),nwords) or known_edits2(word,nwords) or [word]\n",
    "        return max(candidates, key=nwords.get)\n",
    "\n",
    "    # expand out based on a fixed list of common contractions \n",
    "    contract_dict = { \"ain't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he had\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he'll've\": \"he will have\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'd've\": \"i would have\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'll've\": \"i will have\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it'll've\": \"it will have\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she'll've\": \"she will have\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so as\",\n",
    "        \"that'd\": \"that had\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they'll've\": \"they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what'll've\": \"what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who'll've\": \"who will have\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\" }\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(contract_dict.keys()))      \n",
    "\n",
    "    # internal function:    \n",
    "    def expand_contractions(text, contractions_re=contractions_re):\n",
    "        def replace(match):\n",
    "            return contract_dict[match.group(0)]\n",
    "        return contractions_re.sub(replace, text.lower())\n",
    "\n",
    "    # process all words in the text\n",
    "    cleantoken = []\n",
    "    text = expand_contractions(text)\n",
    "    token = word_tokenize(text)\n",
    "    for word in token:        \n",
    "        if \"'\" not in word:\n",
    "            cleantoken.append(correct(word,nwords))\n",
    "        else:\n",
    "            cleantoken.append(word) \n",
    "    return cleantoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pos_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert NLTK default tagger output into a format that Wordnet\n",
    "    can use in order to properly lemmatize the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create some inner functions for simplicity\n",
    "    def is_noun(tag):\n",
    "        return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    def is_verb(tag):\n",
    "        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    def is_adverb(tag):\n",
    "        return tag in ['RB', 'RBR', 'RBS']\n",
    "    def is_adjective(tag):\n",
    "        return tag in ['JJ', 'JJR', 'JJS']\n",
    "    \n",
    "    # check each tag against possible categories\n",
    "    if is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    else:\n",
    "        return wn.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Lemmatize(tokenlist):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    defaultPos = nltk.pos_tag(tokenlist) # get the POS tags from NLTK default tagger\n",
    "    words_lemma = []\n",
    "    for item in defaultPos:  \n",
    "        words_lemma.append(lemmatizer.lemmatize(item[0],pos_to_wn(item[1]))) # need to convert POS tags to a format (NOUN, VERB, ADV, ADJ) that wordnet uses to lemmatize\n",
    "    return words_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ApplyPOSTagging(df,\n",
    "                    filename,\n",
    "                    add_stanford_tags=False,\n",
    "                    stanford_pos_path=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a dataframe of conversation turns, return a new\n",
    "    dataframe with part-of-speech tagging. Add filename\n",
    "    (given as string) as a new column in returned dataframe.\n",
    "    \n",
    "    By default, return only tags from the NLTK default POS \n",
    "    tagger. Optionally, also return Stanford POS tagger \n",
    "    results by setting `add_stanford_tags=True`.\n",
    "    \n",
    "    If Stanford POS tagging is desired, specify the\n",
    "    location of the Stanford POS tagger with the \n",
    "    `stanford_pos_path` argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if desired, import Stanford tagger\n",
    "    if add_stanford_tags == True:\n",
    "        if stanford_pos_path == None:\n",
    "            raise ValueError('Error! Specify path to Stanford POS tagger using the `stanford_pos_path` argument.')\n",
    "        else:\n",
    "            stanford_tagger = StanfordPOSTagger(stanford_pos_path + 'models/english-left3words-distsim.tagger',\n",
    "                                                stanford_pos_path + 'stanford-postagger.jar')\n",
    "    \n",
    "    # add new columns to dataframe\n",
    "    df['tagged_token'] = df['token'].apply(nltk.pos_tag)\n",
    "    df['tagged_lemma'] = df['lemma'].apply(nltk.pos_tag)\n",
    "    \n",
    "    # if desired, also tag with Stanford tagger\n",
    "    if add_stanford_tags == True:\n",
    "        df['tagged_stan_token'] = df['token'].apply(stanford_tagger.tag)\n",
    "        df['tagged_stan_lemma'] = df['lemma'].apply(stanford_tagger.tag)\n",
    "\n",
    "    df['file'] = filename\n",
    "        \n",
    "    # return finished dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RUN Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* For each original transcript file, saves new file with columns for:\n",
    "    * \"Clean\" text\n",
    "    * Tokenized words\n",
    "    * Tokenized lemmatized-words\n",
    "    * NLTK default POS-tagging on tokenized words\n",
    "    * NLTK default POS-tagging on lemmatized words\n",
    "    * Stanford POS-tagging on tokenized words\n",
    "    * Stanford POS-tagging on lemmatized-words\n",
    "* Also saves a single datasheet with all tokenized lemmatized utterances from all transcripts as individual rows\n",
    "    * called `align_concatenated_dataframe.txt`\n",
    "    * to be used in building semantic space for Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def PHASE1RUN(input_files, \n",
    "              output_file_directory,\n",
    "              training_dictionary,\n",
    "              minwords=2,\n",
    "              use_filler_list=None,\n",
    "              add_stanford_tags=False,\n",
    "              stanford_pos_path=None,\n",
    "              input_as_directory=True,\n",
    "              save_concatenated_dataframe=True):   \n",
    "\n",
    "    \"\"\"\n",
    "    Given individual .txt files of conversations, \n",
    "    return a completely prepared dataframe of transcribed \n",
    "    conversations for later ALIGN analysis, including: text \n",
    "    cleaning, merging adjacent turns, spell-checking, \n",
    "    tokenization, lemmatization, and part-of-speech tagging. \n",
    "    The output serve as the input for later ALIGN\n",
    "    analysis.\n",
    "    \n",
    "    By default, set a minimum number of words in a turn to\n",
    "    3. If desired, this may be chaged by changing the\n",
    "    `minwords` file.\n",
    "    \n",
    "    By default, return only the NLTK default \n",
    "    POS tagger values. Optionally, also return Stanford POS \n",
    "    tagger values with `add_stanford_tags=True`.\n",
    "    \n",
    "    If Stanford POS tagging is desired, specify the\n",
    "    location of the Stanford POS tagger with the \n",
    "    `stanford_pos_path` argument.\n",
    "    \n",
    "    By default, accept `input_files` as a directory\n",
    "    that includes `.txt` files of each individual \n",
    "    conversation. If desired, provide individual files\n",
    "    as a list of literal paths to the `input_files`\n",
    "    argument and set `input_as_directory=False`.\n",
    "    \n",
    "    By default, produce a single concatenated dataframe\n",
    "    of all processed conversations in the output directory. \n",
    "    If desired, suppress concatenated dataframe with \n",
    "    `save_concatenated_dataframe=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create an internal function to train the model\n",
    "    def train(features): \n",
    "        model = defaultdict(lambda: 1)\n",
    "        for f in features:\n",
    "            model[f] += 1\n",
    "        return model\n",
    "        \n",
    "    # train our spell-checking model\n",
    "    nwords = train(re.findall('[a-z]+',(file(training_dictionary).read().lower())))\n",
    "    \n",
    "    # grab the appropriate files\n",
    "    if input_as_directory==False:\n",
    "        file_list = glob.glob(input_files)\n",
    "    else: \n",
    "        file_list = glob.glob(input_files+\"*.txt\")\n",
    "    \n",
    "    # cycle through all files \n",
    "    main = pd.DataFrame()\n",
    "    for fileName in file_list:  \n",
    "        \n",
    "        # let us know which file we're processing\n",
    "        dataframe = pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        print \"Processing: \"+fileName\n",
    "\n",
    "        # clean up, merge, spellcheck, tokenize, lemmatize, and POS-tag\n",
    "        dataframe = InitialCleanup(dataframe,\n",
    "                                  minwords=minwords,\n",
    "                                  use_filler_list=use_filler_list)\n",
    "        dataframe = AdjacentMerge(dataframe)\n",
    "        \n",
    "        # tokenize and lemmatize \n",
    "        dataframe['token'] = dataframe['content'].apply(Tokenize,\n",
    "                                     args=(nwords,))\n",
    "        dataframe['lemma'] = dataframe['token'].apply(Lemmatize)\n",
    "\n",
    "        # apply part-of-speech tagging\n",
    "        dataframe = ApplyPOSTagging(dataframe,  \n",
    "                                    filename = os.path.basename(fileName),\n",
    "                                    add_stanford_tags=add_stanford_tags,\n",
    "                                    stanford_pos_path=stanford_pos_path\n",
    "                                    )\n",
    "        \n",
    "        # export the conversation's dataframe as a CSV\n",
    "        dataframe.to_csv(output_file_directory + os.path.basename(fileName), \n",
    "                         encoding='utf-8',index=False,sep='\\t')\n",
    "        main = main.append(dataframe)\n",
    "\n",
    "    # save the concatenated dataframe\n",
    "    if save_concatenated_dataframe != False:\n",
    "        main.to_csv(output_file_directory +'../' + \"align_concatenated_dataframe.txt\",\n",
    "                    encoding='utf-8',index=False, sep='\\t')\n",
    "    \n",
    "    # return the dataframe\n",
    "    return main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Nick**: Since you didn't want to add the concatenated dataframe, I went ahead and spun that out as a new argument (`save_concatenated_dataframe`). I've also made it so that the function can accept input files as a list of literal strings to file paths by setting `input_as_directory = False` and providing the list of paths to `input_files`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Alex** Sounds good. Fixed Line 58 to add glob.glob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phase 2: Generate alignment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* [**Create helper functions**](#Create-helper-functions) for processing turn- and conversation-level data.\n",
    "* **[Build semantic space](#Build-semantic-space)** from the `forSemantic.txt` generated in Phase 1 and return a `word2vec` semantic space and vocabulary list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[To top.](#ALIGN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ngram_pos(sequence1,sequence2,ngramsize=2,\n",
    "                   ignore_duplicates=True):\n",
    "    \"\"\"\n",
    "    Remove mimicked lexical sequences from two interlocutors'\n",
    "    sequences and return a dictionary of counts of ngrams\n",
    "    of the desired size for each sequence.\n",
    "    \n",
    "    By default, consider bigrams. If desired, this may be \n",
    "    changed by setting `ngramsize` to the appropriate \n",
    "    value.    \n",
    "    \n",
    "    By default, ignore duplicate lexical n-grams when\n",
    "    processing these sequences. If desired, this may\n",
    "    be changed with `ignore_duplicates=False`.\n",
    "    \"\"\"     \n",
    "    \n",
    "    # remove duplicates and recreate sequences\n",
    "    sequence1 = set(ngrams(sequence1,ngramsize))\n",
    "    sequence2 = set(ngrams(sequence2,ngramsize))\n",
    "\n",
    "    # if desired, remove duplicates from sequences\n",
    "    if ignore_duplicates==True:\n",
    "        new_sequence1 = [tuple([''.join(pair[1]) for pair in tup]) for tup in list(sequence1 - sequence2)]\n",
    "        new_sequence2 = [tuple([''.join(pair[1]) for pair in tup]) for tup in list(sequence2 - sequence1)]\n",
    "    else:\n",
    "        new_sequence1 = [tuple([''.join(pair[1]) for pair in tup]) for tup in sequence1]\n",
    "        new_sequence2 = [tuple([''.join(pair[1]) for pair in tup]) for tup in sequence2]\n",
    "        \n",
    "    # return counters\n",
    "    return Counter(new_sequence1), Counter(new_sequence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ngram_lexical(sequence1,sequence2,ngramsize=2):\n",
    "    \"\"\"\n",
    "    Create ngrams of the desired size for each of two\n",
    "    interlocutors' sequences and return a dictionary \n",
    "    of counts of ngrams for each sequence.\n",
    "    \n",
    "    By default, consider bigrams. If desired, this may be \n",
    "    changed by setting `ngramsize` to the appropriate \n",
    "    value.  \n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    # generate ngrams\n",
    "    sequence1 = list(ngrams(sequence1,ngramsize))\n",
    "    sequence2 = list(ngrams(sequence2,ngramsize)) \n",
    "\n",
    "    # join for counters\n",
    "    new_sequence1 = [' '.join(pair) for pair in sequence1]\n",
    "    new_sequence2 = [' '.join(pair) for pair in sequence2]\n",
    "    \n",
    "    # return counters\n",
    "    return Counter(new_sequence1), Counter(new_sequence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2): \n",
    "    \"\"\"\n",
    "    Derive cosine similarity metric, standard measure.\n",
    "    Adapted from <https://stackoverflow.com/a/33129724>.\n",
    "    \"\"\"     \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Alex** A problem with using the pre-trained vectors is that sometimes the words in vocablist, that are unique to our corpus, do not appear and throw an error. I have now fixed with code on Line 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_composite_semantic_vector(lemma_seq,vocablist,highDimModel):\n",
    "    \"\"\"\n",
    "    Function for producing vocablist and model is called in the main loop\n",
    "    \"\"\"\n",
    "    \n",
    "    ## filter out words in corpus that do not appear in vocablist (either too rare or too frequent)\n",
    "    filter_lemma_seq = [word for word in lemma_seq if word in vocablist]    \n",
    "    ## build composite vector\n",
    "    getComposite = [0] * len(highDimModel[vocablist[1]])        \n",
    "    for w1 in filter_lemma_seq:\n",
    "        if w1 in highDimModel.vocab:\n",
    "            semvector = highDimModel[w1]\n",
    "            getComposite = getComposite + semvector\n",
    "        else:\n",
    "            print w1\n",
    "    return getComposite\n",
    "\n",
    "# what we want to do here is find the union of the vocablist within the HDM and then sum over all of the columns.\n",
    "# should be faster/easier than current instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build semantic space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Alex** I now have added an option to build vectors from scratch based on corpus submitted for analysis or to load in pre-trained vectors from GoogleNews. But the pre-trained vectors options is also really slow. So I'm setting the \"build from scratch\" as the default. I think the optimal solution however is to build a semantic space smaller than GoogleNews but large enough where it is still representative of the language people are likely to submit to this tool. The best option from my experience working with HDM is the TASA corpus. I left a note on it below for more info. But we don't have a lot of time and this is something that can be added later. I will mention it in paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def BuildSemanticModel(semantic_model_input_file,   \n",
    "                        pretrained_input_file,\n",
    "                        use_pretrained_vectors=False,                     \n",
    "                        high_sd_cutoff=3,\n",
    "                        low_n_cutoff=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given an input file produced by the ALIGN Phase 1 functions, \n",
    "    build a semantic model from all transcripts in all conversations\n",
    "    in target corpus after removing high- and low-frequency words.\n",
    "    High-frequency words are determined by a user-defined number of\n",
    "    SDs over the mean (by default, `high_sd_cutoff=3`). Low-frequency\n",
    "    words must appear over a specified number of raw occurrences \n",
    "    (by default, `low_n_cutoff=1`).\n",
    "    \n",
    "    Frequency cutoffs can be removed by `high_sd_cutoff=None` and/or\n",
    "    `low_n_cutoff=0`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # build vocabulary list from transcripts\n",
    "    data1 = pd.read_csv(semantic_model_input_file, sep='\\t',encoding='utf-8')\n",
    "        \n",
    "    # get frequency count of all included words        \n",
    "    all_sentences = [re.sub('[^\\w\\s]+','',str(row)).split(' ') for row in list(data1['lemma'])]\n",
    "    all_words = list([a for b in all_sentences for a in b])  \n",
    "#     all_words = list([a for b in all_sentences for a in b if len(a) > 1])\n",
    "    frequency = defaultdict(int)\n",
    "    for word in all_words:\n",
    "        frequency[word] += 1\n",
    "\n",
    "    # remove words that only occur more frequently than our cutoff (defined in occurrences)\n",
    "    frequency = {word: freq for word, freq in frequency.iteritems() if freq > low_n_cutoff}\n",
    "    \n",
    "    # if desired, remove high-frequency words (over user-defined SDs above mean) \n",
    "    if high_sd_cutoff == None:\n",
    "        contentWords = [word for word in frequency.keys()] \n",
    "    else:\n",
    "        getOut = np.mean(frequency.values())+(np.std(frequency.values())*(high_sd_cutoff))\n",
    "        contentWords = {word: freq for word, freq in frequency.iteritems() if freq < getOut}.keys()\n",
    "    \n",
    "    # decide whether to build semantic model from scratch or load in pretrained vectors\n",
    "    if use_pretrained_vectors == False:\n",
    "        keepSentences = [[word for word in row if word in contentWords] for row in all_sentences]\n",
    "        semantic_model = word2vec.Word2Vec(keepSentences, min_count=low_n_cutoff)\n",
    "#         semantic_model = word2vec.Word2Vec(all_sentences, min_count=low_n_cutoff)\n",
    "    else:\n",
    "        if pretrained_input_file == None:\n",
    "            raise ValueError('Error! Specify path to pretrained vector file using the `pretrained_input_file` argument.')\n",
    "        else:\n",
    "            semantic_model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_input_file, binary=True)    \n",
    "        \n",
    "    # return all the content words and the trained word vectors\n",
    "    return contentWords, semantic_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Nick (2a)**: I think that might be related to the size of the corpus. I just re-ran it on the entire dataframe of all conversations (without the `all_sentences` line) and didn't get the warning.\n",
    "\n",
    "The problem with very low-frequency words is that they're more noise than nuance. If there are insufficient observations of those words when building the corpus, the model won't produce a stable estimate of the meaning. \n",
    "\n",
    "Using a pre-trained model would eliminate this concern, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Alex** Hmm. I still don't think having low-frequency words is that much of a problem in the building stage. My experience when you have very small contexts (like ours, which are a sentence long) and there are not many of them (ours only had like 2200 contexts), it can help. You can see this when you run Line 45 above with `all_sentences` or `keepSentences`. The former produces very sensible nearest neighbors when you run something like test_model.most_similar(['choice']). It also eliminates the annoying warning which might throw users.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Nick (3b)**: Output (above) is *much* more sensible from the pre-trained model. I would recommend that we use a pre-trained model for our analysis (or create a new model from another corpus we find or derive) *and* as an option for ALIGN. We should definitely include all relevant caveats about how to select and/or build a semantic space (e.g., don't use a university textbook-derived corpus to understand toddlers' speech), but given that most news is targeted at a 9th- or 10th-grade reading level, I don't think it's too far off from what we would expect colloquial language from college students to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Alex** I agree that the pre-trained is the best way to go. Very nice! It eliminates having to decide whether to use `all_sentences` or `keepSentences`. But as mentioned, the pre-trained vectors from GoogleNews is really slow. I think having an alternative, more stream-lined pre-trained set of vectors (based on TASA) will be the best default. TASA is the main space used in the older HDM that I have worked with quite a bit (HAL and LSA). So it's tried and true and works really well with conversational text. But GoogleNews is good enough now and we can optimize later for speed once we get the paper out. Oh- and I'll be sure to make the points about selecting semantic space criteria as you noted. Very good points.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculate lexical and POS alignment scores for each n-gram length across two comparison vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def LexicalPOSAlignment(tok1,lem1,penn_tok1,penn_lem1,\n",
    "                             tok2,lem2,penn_tok2,penn_lem2,\n",
    "                             stan_tok1=None,stan_lem1=None,\n",
    "                             stan_tok2=None,stan_lem2=None,\n",
    "                             maxngram=2,\n",
    "                             ignore_duplicates=True,\n",
    "                             add_stanford_tags=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Derive lexical and part-of-speech alignment scores\n",
    "    between interlocutors (suffix `1` and `2` in arguments\n",
    "    passed to function). \n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tags=True` and by providing appropriate \n",
    "    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and \n",
    "    `stan_lem2`.\n",
    "    \n",
    "    By default, consider only bigram when calculating\n",
    "    similarity. If desired, this window may be expanded \n",
    "    by changing the `maxngram` argument value.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty dictionaries for syntactic similarity\n",
    "    syntax_penn_tok = {}\n",
    "    syntax_penn_lex = {}\n",
    "    \n",
    "    # if desired, generate Stanford-based scores\n",
    "    if add_stanford_tags == True:\n",
    "        syntax_stan_tok = {}\n",
    "        syntax_stan_lem = {}\n",
    "    \n",
    "    # create empty dictionaries for lexical similarity\n",
    "    lexical_tok = {}\n",
    "    lexical_lem = {}\n",
    "    \n",
    "    # cycle through all desired ngram lengths\n",
    "    for ngram in range(2,maxngram+1):\n",
    "                \n",
    "        # calculate similarity for lexical ngrams (tokens and lemmas)\n",
    "        [vectorT1, vectorT2] = ngram_lexical(tok1,tok2,ngramsize=ngram)\n",
    "        [vectorL1, vectorL2] = ngram_lexical(lem1,lem2,ngramsize=ngram)        \n",
    "        lexical_tok['lexical_tok{0}'.format(ngram)] = get_cosine(vectorT1,vectorT2)\n",
    "        lexical_lem['lexical_lem{0}'.format(ngram)] = get_cosine(vectorL1, vectorL2)\n",
    "        \n",
    "        # calculate similarity for Penn POS ngrams (tokens)\n",
    "        [vector_penn_tok1, vector_penn_tok2] = ngram_pos(penn_tok1,penn_tok2,\n",
    "                                                ngramsize=ngram,\n",
    "                                                ignore_duplicates=ignore_duplicates) \n",
    "        syntax_penn_tok['syntax_penn_tok{0}'.format(ngram)] = get_cosine(vector_penn_tok1, \n",
    "                                                                                            vector_penn_tok2)\n",
    "        # calculate similarity for Penn POS ngrams (lemmas)\n",
    "        [vector_penn_lem1, vector_penn_lem2] = ngram_pos(penn_lem1,penn_lem2,\n",
    "                                                              ngramsize=ngram,\n",
    "                                                              ignore_duplicates=ignore_duplicates) \n",
    "        syntax_penn_lex['syntax_penn_lex{0}'.format(ngram)] = get_cosine(vector_penn_lem1, \n",
    "                                                                                            vector_penn_lem2) \n",
    "\n",
    "        # if desired, also calculate using Stanford POS\n",
    "        if add_stanford_tags == True:         \n",
    "          \n",
    "            # calculate similarity for Stanford POS ngrams (tokens)\n",
    "            [vector_stan_tok1, vector_stan_tok2] = ngram_pos(stan_tok1,stan_tok2,\n",
    "                                                                  ngramsize=ngram,\n",
    "                                                                  ignore_duplicates=ignore_duplicates) \n",
    "            syntax_stan_tok['syntax_stan_tok{0}'.format(ngram)] = get_cosine(vector_stan_tok1,\n",
    "                                                                                                vector_stan_tok2)\n",
    "                        \n",
    "            # calculate similarity for Stanford POS ngrams (lemmas)\n",
    "            [vector_stan_lem1, vector_stan_lem2] = ngram_pos(stan_lem1,stan_lem2,\n",
    "                                                                  ngramsize=ngram,\n",
    "                                                                  ignore_duplicates=ignore_duplicates) \n",
    "            syntax_stan_lem['syntax_stan_lem{0}'.format(ngram)] = get_cosine(vector_stan_lem1,\n",
    "                                                                                                vector_stan_lem2)\n",
    "        \n",
    "    # return requested information\n",
    "    if add_stanford_tags == True:\n",
    "        dictionaries_list = [syntax_penn_tok, syntax_penn_lex,\n",
    "                             syntax_stan_tok, syntax_stan_lem, \n",
    "                             lexical_tok, lexical_lem]      \n",
    "    else:\n",
    "        dictionaries_list = [syntax_penn_tok, syntax_penn_lex,\n",
    "                             lexical_tok, lexical_lem]      \n",
    "            \n",
    "    return dictionaries_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate turn-level analysis of alignment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conceptualAlignment(lem1, lem2, vocablist, highDimModel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate conceptual alignment scores from list of lemmas\n",
    "    from between two interocutors (suffix `1` and `2` in arguments\n",
    "    passed to function) using `word2vec`.\n",
    "    \"\"\"\n",
    "\n",
    "    # aggregate composite high-dimensional vectors of all words in utterance\n",
    "    W2Vec1 = build_composite_semantic_vector(lem1,vocablist,highDimModel)\n",
    "    W2Vec2 = build_composite_semantic_vector(lem2,vocablist,highDimModel)\n",
    "\n",
    "    # return cosine distance alignment score\n",
    "    return 1 - spatial.distance.cosine(W2Vec1, W2Vec2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def returnMultilevelAlignment(cond_info,\n",
    "                                   partnerA,tok1,lem1,penn_tok1,penn_lem1,\n",
    "                                   partnerB,tok2,lem2,penn_tok2,penn_lem2,\n",
    "                                   vocablist, highDimModel, \n",
    "                                   stan_tok1=None,stan_lem1=None,\n",
    "                                   stan_tok2=None,stan_lem2=None,\n",
    "                                   add_stanford_tags=False,\n",
    "                                   maxngram=2, \n",
    "                                   ignore_duplicates=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate lexical, syntactic, and conceptual alignment\n",
    "    between a pair of turns by individual interlocutors \n",
    "    (suffix `1` and `2` in arguments passed to function), \n",
    "    including leading/following comparison directionality.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tags=True` and by providing appropriate \n",
    "    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and \n",
    "    `stan_lem2`.\n",
    "    \n",
    "    By default, consider only bigrams when calculating\n",
    "    similarity. If desired, this window may be expanded \n",
    "    by changing the `maxngram` argument value.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create empty dictionaries \n",
    "    partner_direction = {}\n",
    "    condition_info = {}\n",
    "    cosine_semanticL = {}\n",
    "    \n",
    "    # calculate lexical and syntactic alignment\n",
    "    dictionaries_list = LexicalPOSAlignment(tok1=tok1,lem1=lem1,\n",
    "                                                 penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                 tok2=tok2,lem2=lem2,\n",
    "                                                 penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                 stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                 stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                 maxngram=maxngram,\n",
    "                                                 ignore_duplicates=ignore_duplicates,\n",
    "                                                 add_stanford_tags=add_stanford_tags)\n",
    "    \n",
    "    # calculate conceptual alignment\n",
    "    cosine_semanticL['cosine_semanticL'] = conceptualAlignment(lem1,lem2,vocablist,highDimModel)\n",
    "    dictionaries_list.append(cosine_semanticL.copy())\n",
    "    \n",
    "    # determine directionality of leading/following comparison\n",
    "    partner_direction['partner_direction'] = str(int(partnerA)) + \">\" + str(int(partnerB))\n",
    "    dictionaries_list.append(partner_direction.copy())\n",
    "\n",
    "    # add condition information\n",
    "    condition_info['condition_info'] = cond_info    \n",
    "    dictionaries_list.append(condition_info.copy())\n",
    "    \n",
    "    # return alignment scores\n",
    "    return dictionaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def TurnByTurnAnalysis(dataframe,\n",
    "                            vocablist,\n",
    "                            highDimModel, \n",
    "                            delay=1,\n",
    "                            maxngram=2,\n",
    "                            add_stanford_tags=False,\n",
    "                            ignore_duplicates=True):    \n",
    "\n",
    "    \"\"\"\n",
    "    Calculate lexical, syntactic, and conceptual alignment\n",
    "    between interlocutors over an entire conversation.\n",
    "    Automatically detect individual speakers by unique\n",
    "    speaker codes.\n",
    "    \n",
    "    By default, compare only adjacent turns. If desired,\n",
    "    the comparison distance may be changed by increasing\n",
    "    the `delay` argument.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 2. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tags=True`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if we don't want the Stanford tagger data, set defaults\n",
    "    if add_stanford_tags == False:\n",
    "        stan_tok1=None\n",
    "        stan_lem1=None\n",
    "        stan_tok2=None\n",
    "        stan_lem2=None\n",
    "    \n",
    "    # prepare the data to the appropriate type    \n",
    "    dataframe['token'] = dataframe['token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))    \n",
    "    dataframe['lemma'] = dataframe['lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_token'] = dataframe['tagged_token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_token'] = dataframe['tagged_token'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "    dataframe['tagged_lemma'] = dataframe['tagged_lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_lemma'] = dataframe['tagged_lemma'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "        \n",
    "    # if desired, prepare the Stanford tagger data\n",
    "    if add_stanford_tags == True:           \n",
    "        dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "        dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "        dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "        dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "        \n",
    "    # create lagged version of the dataframe\n",
    "    df_original = dataframe.drop(dataframe.tail(delay).index,inplace=False)\n",
    "    df_lagged = dataframe.shift(-delay).drop(dataframe.tail(delay).index,inplace=False)\n",
    "        \n",
    "    # cycle through each pair of turns\n",
    "    aggregated_df = pd.DataFrame()\n",
    "    for i in range(0,df_original.shape[0]):\n",
    "\n",
    "        # identify the condition for this dataframe\n",
    "        cond_info = dataframe['file'].unique()\n",
    "        if len(cond_info)==1: \n",
    "            cond_info = str(cond_info[0])\n",
    "        \n",
    "        # break and flag error if we have more than 1 condition per dataframe\n",
    "        else: \n",
    "            raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)\n",
    "\n",
    "        # grab all of first participant's data\n",
    "        first_row = df_original.iloc[i]\n",
    "        first_partner = first_row['participant']\n",
    "        tok1=first_row['token']\n",
    "        lem1=first_row['lemma']\n",
    "        penn_tok1=first_row['tagged_token']\n",
    "        penn_lem1=first_row['tagged_lemma']\n",
    "\n",
    "        # grab all of lagged participant's data\n",
    "        lagged_row = df_lagged.iloc[i]\n",
    "        lagged_partner = lagged_row['participant']\n",
    "        tok2=lagged_row['token']\n",
    "        lem2=lagged_row['lemma']\n",
    "        penn_tok2=lagged_row['tagged_token']\n",
    "        penn_lem2=lagged_row['tagged_lemma']\n",
    "                \n",
    "        # if desired, grab the Stanford tagger data for both participants\n",
    "        if add_stanford_tags == True:         \n",
    "            stan_tok1=first_row['tagged_stan_token']\n",
    "            stan_lem1=first_row['tagged_stan_lemma']\n",
    "            stan_tok2=lagged_row['tagged_stan_token']\n",
    "            stan_lem2=lagged_row['tagged_stan_lemma']\n",
    "   \n",
    "        # process multilevel alignment\n",
    "        dictionaries_list=returnMultilevelAlignment(cond_info=cond_info,\n",
    "                                                         partnerA=first_partner,\n",
    "                                                         tok1=tok1,lem1=lem1,\n",
    "                                                         penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                         partnerB=lagged_partner,\n",
    "                                                         tok2=tok2,lem2=lem2,\n",
    "                                                         penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                         vocablist=vocablist,\n",
    "                                                         highDimModel=highDimModel,\n",
    "                                                         stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                         stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                         maxngram = maxngram,\n",
    "                                                         ignore_duplicates = ignore_duplicates,\n",
    "                                                         add_stanford_tags = add_stanford_tags) \n",
    "                \n",
    "        # sort columns so they are in order, append data to existing structures   \n",
    "        next_df_line = pd.DataFrame.from_dict(OrderedDict(k for num, i in enumerate(d for d in dictionaries_list) for k in sorted(i.items())),\n",
    "                               orient='index').transpose()\n",
    "        aggregated_df = aggregated_df.append(next_df_line)\n",
    "        \n",
    "    # reformat turn information and add index\n",
    "    aggregated_df = aggregated_df.reset_index(drop=True).reset_index().rename(columns={\"index\":\"time\"})\n",
    "\n",
    "    # give us our finished dataframe\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generate conversation-level analysis of alignment scores\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ConvoByConvoAnalysis(dataframe,\n",
    "                          maxngram=2,\n",
    "                          ignore_duplicates=True,\n",
    "                          add_stanford_tags=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate analysis of multilevel similarity over\n",
    "    a conversation between two interlocutors from a \n",
    "    transcript dataframe prepared by Phase 1\n",
    "    of ALIGN. Automatically detect speakers by unique\n",
    "    speaker codes.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 2. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tags=True`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # identify the condition for this dataframe\n",
    "    cond_info = dataframe['file'].unique()\n",
    "    if len(cond_info)==1: \n",
    "        cond_info = str(cond_info[0])\n",
    "    \n",
    "    # break and flag error if we have more than 1 condition per dataframe\n",
    "    else: \n",
    "        raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)\n",
    "   \n",
    "    # if we don't want the Stanford info, set defaults \n",
    "    if add_stanford_tags == False:\n",
    "        stan_tok1 = None\n",
    "        stan_lem1 = None\n",
    "        stan_tok2 = None\n",
    "        stan_lem2 = None\n",
    "\n",
    "    # identify individual interlocutors\n",
    "    df_A = dataframe.loc[dataframe['participant'] == dataframe['participant'].unique()[0]]\n",
    "    df_B = dataframe.loc[dataframe['participant'] == dataframe['participant'].unique()[1]]\n",
    "   \n",
    "    # concatenate the token, lemma, and POS information for participant A\n",
    "    tok1 = [word for turn in df_A['token'] for word in turn]\n",
    "    lem1 = [word for turn in df_A['lemma'] for word in turn]\n",
    "    penn_tok1 = [POS for turn in df_A['tagged_token'] for POS in turn]    \n",
    "    penn_lem1 = [POS for turn in df_A['tagged_token'] for POS in turn] \n",
    "    if add_stanford_tags == True:\n",
    "        stan_tok1 = [POS for turn in df_A['tagged_stan_token'] for POS in turn]    \n",
    "        stan_lem1 = [POS for turn in df_A['tagged_stan_lemma'] for POS in turn] \n",
    "\n",
    "    # concatenate the token, lemma, and POS information for participant B\n",
    "    tok2 = [word for turn in df_B['token'] for word in turn]\n",
    "    lem2 = [word for turn in df_B['lemma'] for word in turn]\n",
    "    penn_tok2 = [POS for turn in df_B['tagged_token'] for POS in turn]    \n",
    "    penn_lem2 = [POS for turn in df_B['tagged_token'] for POS in turn] \n",
    "    if add_stanford_tags == True:\n",
    "        stan_tok2 = [POS for turn in df_B['tagged_stan_token'] for POS in turn]    \n",
    "        stan_lem2 = [POS for turn in df_B['tagged_stan_lemma'] for POS in turn] \n",
    "    \n",
    "    # process multilevel alignment\n",
    "    dictionaries_list = LexicalPOSAlignment(tok1=tok1,lem1=lem1,\n",
    "                                                 penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                 tok2=tok2,lem2=lem2,\n",
    "                                                 penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                 stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                 stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                 maxngram=maxngram,\n",
    "                                                 ignore_duplicates=ignore_duplicates,\n",
    "                                                 add_stanford_tags=add_stanford_tags)\n",
    "    \n",
    "    # append data to existing structures\n",
    "    dictionary_df = pd.DataFrame.from_dict(OrderedDict(k for num, i in enumerate(d for d in dictionaries_list) for k in sorted(i.items())),\n",
    "                       orient='index').transpose()    \n",
    "    dictionary_df['condition_info'] = cond_info\n",
    "            \n",
    "    # return the dataframe\n",
    "    return dictionary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RUN Phase 2: Actual Partners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* For each prepped transcript file, runs turn-level and conversational-level alignment scores\n",
    "* Saves output into single datasheet to be used in statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def PHASE2RUN_REAL(input_files, \n",
    "                    output_file_directory,\n",
    "                    semantic_model_input_file,\n",
    "                    pretrained_input_file,\n",
    "                    high_sd_cutoff=3,\n",
    "                    low_n_cutoff=1,\n",
    "                    delay=1,\n",
    "                    maxngram=2,\n",
    "                    use_pretrained_vectors=False,\n",
    "                    ignore_duplicates=True,\n",
    "                    add_stanford_tags=False,\n",
    "                    input_as_directory=True):   \n",
    "    \n",
    "    \"\"\"\n",
    "    Given a directory of individual .txt files and the\n",
    "    vocab list that have been generated by the `PHASE1RUN` \n",
    "    preparation stage, return multi-level alignment \n",
    "    scores with turn-by-turn and conversation-level metrics.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    high-frequency cutoff of 3 SD over the mean. If \n",
    "    desired, this can be changed with the \n",
    "    `high_sd_cutoff` argument and can be removed with\n",
    "    `high_sd_cutoff=None`.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    low-frequency cutoff in which a word will be \n",
    "    removed if they occur 1 or fewer times. if\n",
    "    desired, this can be changed with the \n",
    "    `low_n_cutoff` argument and can be removed with\n",
    "    `low_n_cutoff=0`.\n",
    "    \n",
    "    By default, compare only adjacent turns. If desired,\n",
    "    the comparison distance may be changed by increasing\n",
    "    the `delay` argument.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 2. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tags=True`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \n",
    "    By default, accept `input_files` as a directory\n",
    "    that includes `.txt` files of each individual \n",
    "    conversation. If desired, provide individual files\n",
    "    as a list of literal paths to the `input_files`\n",
    "    argument and set `input_as_directory=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # grab the files in the list\n",
    "    if input_as_directory == False:\n",
    "        file_list = glob.glob(input_files)\n",
    "    else:\n",
    "        file_list = glob.glob(input_files+\"*.txt\")\n",
    "    \n",
    "    # build the semantic model to be used for all conversations\n",
    "    [vocablist, highDimModel] = BuildSemanticModel(semantic_model_input_file=semantic_model_input_file,\n",
    "                                                       pretrained_input_file=pretrained_input_file,\n",
    "                                                       use_pretrained_vectors=use_pretrained_vectors,\n",
    "                                                       high_sd_cutoff=high_sd_cutoff,\n",
    "                                                       low_n_cutoff=low_n_cutoff)  \n",
    "    \n",
    "    # create containers for alignment values\n",
    "    AlignmentT2T = pd.DataFrame()\n",
    "    AlignmentC2C = pd.DataFrame()\n",
    "        \n",
    "    # cycle through each prepared file\n",
    "    for fileName in file_list:   \n",
    "        \n",
    "        # process the file if it's got a valid conversation\n",
    "        dataframe=pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        if len(dataframe) > 0:\n",
    "            \n",
    "            # let us know which filename we're processing\n",
    "            print \"Processing: \"+fileName   \n",
    "\n",
    "            # calculate turn-by-turn alignment scores\n",
    "            xT2T=TurnByTurnAnalysis(dataframe=dataframe,\n",
    "                                         delay=delay,\n",
    "                                         maxngram=maxngram,\n",
    "                                         vocablist=vocablist,\n",
    "                                         highDimModel=highDimModel,\n",
    "                                         add_stanford_tags=add_stanford_tags,\n",
    "                                         ignore_duplicates=ignore_duplicates)   \n",
    "            AlignmentT2T=AlignmentT2T.append(xT2T)\n",
    "            \n",
    "            # calculate conversation-level alignment scores\n",
    "            xC2C = ConvoByConvoAnalysis(dataframe=dataframe,\n",
    "                                             maxngram = maxngram,\n",
    "                                             ignore_duplicates=ignore_duplicates,\n",
    "                                             add_stanford_tags = add_stanford_tags)\n",
    "            AlignmentC2C=AlignmentC2C.append(xC2C)\n",
    "            \n",
    "        # if it's invalid, let us know\n",
    "        else:\n",
    "            print \"Invalid file: \"+fileName   \n",
    "            \n",
    "    # update final dataframes\n",
    "    FINAL_TURN = AlignmentT2T.reset_index(drop=True)\n",
    "    FINAL_CONVO = AlignmentC2C.reset_index(drop=True)\n",
    "    \n",
    "    # export the final files\n",
    "    FINAL_TURN.to_csv(output_file_directory+\"AlignmentT2T.txt\",\n",
    "                      encoding='utf-8',index=False,sep='\\t')   \n",
    "    FINAL_CONVO.to_csv(output_file_directory+\"AlignmentC2C.txt\",\n",
    "                       encoding='utf-8',index=False,sep='\\t') \n",
    "\n",
    "    # display the info, too\n",
    "    return FINAL_TURN, FINAL_CONVO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generate surrogate pairings\n",
    "-------------------------\n",
    "* Collects all possible pairs of participants across the dyads in each condition and creates surrogate pairings by combining their conversational turns, preserving turn order. Output saved as new separate conversational transcripts. \n",
    "* Main Function:\n",
    "    * GenerateSurrogate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Alex** Below was throwing an error when `keep_original_turn_order=True`. Please double check that my changes  correct the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Alex** What do you think is the most conservative default? What about pair each person with a random partner and preserve turn order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def GenerateSurrogate(original_conversation_list,\n",
    "                           surrogate_file_directory,\n",
    "                           all_surrogates = False,\n",
    "                           id_separator = '\\-',\n",
    "                           dyad_label='dyad',\n",
    "                           condition_label='cond',\n",
    "                           keep_original_turn_order = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create transcripts for surrogate pairs of \n",
    "    participants (i.e., participants who did not \n",
    "    genuinely interact in the experiment), which\n",
    "    will later be used to generate baseline levels \n",
    "    of alignment. Store surrogate files in a new\n",
    "    folder each time the surrogate generation is run.\n",
    "    \n",
    "    Returns a list of all surrogate files created.\n",
    "\n",
    "    By default, the separator between dyad ID and\n",
    "    condition ID is a hyphen ('\\-'). If desired,\n",
    "    this may be changed in the `id_separator` \n",
    "    argument.\n",
    "\n",
    "    By default, condition IDs will be identified as \n",
    "    any characters following `cond`. If desired,\n",
    "    this may be changed with the `condition_label`\n",
    "    argument.\n",
    "    \n",
    "    By default, dyad IDs will be identified as \n",
    "    any characters following `dyad`. If desired,\n",
    "    this may be changed with the `dyad_label`\n",
    "    argument.\n",
    "    \n",
    "    By default, generate surrogates only from a subset\n",
    "    of all possible pairings. If desired, instead \n",
    "    generate surrogates from all possible pairings\n",
    "    with `all_surrogates=True`.\n",
    "    \n",
    "    By default, create surrogates by shuffling all\n",
    "    turns within each surrogate partner's data. If \n",
    "    desired, retain the original ordering of each\n",
    "    surrogate partner's data with \n",
    "    `keep_original_turn_order = True`.\n",
    "    \"\"\"\n",
    "        \n",
    "    # create a subfolder for the new set of surrogates\n",
    "    import time\n",
    "    new_surrogate_path = surrogate_file_directory + 'surrogate_run-' + str(time.time()) +'/'\n",
    "    if not os.path.exists(new_surrogate_path):\n",
    "        os.makedirs(new_surrogate_path)\n",
    "        \n",
    "    # grab condition types from each file name\n",
    "    file_info = [re.sub('\\.txt','',os.path.basename(file_name)) for file_name in original_conversation_list]\n",
    "    condition_ids = list(set([re.findall('[^'+id_separator+']*'+condition_label+'.*',metadata)[0] for metadata in file_info]))\n",
    "    files_conditions = {}\n",
    "    for unique_condition in condition_ids:\n",
    "        next_condition_files = [add_file for add_file in original_conversation_list if unique_condition in add_file]\n",
    "        files_conditions[unique_condition] = next_condition_files\n",
    "    \n",
    "    # cycle through conditions\n",
    "    for condition in files_conditions.keys():\n",
    "        \n",
    "        # grab all possible pairs of conversations of this condition\n",
    "        paired_surrogates = [pair for pair in combinations(files_conditions[condition],2)]\n",
    "        \n",
    "        # default: randomly pull from all pairs to get target surrogate sample\n",
    "        if all_surrogates == False:\n",
    "            import math\n",
    "            paired_surrogates = random.sample(paired_surrogates, \n",
    "                                              int(math.ceil(len(files_conditions[condition])/2)))\n",
    "            \n",
    "        # cycle through surrogate pairings\n",
    "        for next_surrogate in paired_surrogates:\n",
    "            \n",
    "            # read in the files\n",
    "            original_file1 = os.path.basename(next_surrogate[0])\n",
    "            original_file2 = os.path.basename(next_surrogate[1])\n",
    "            original_df1=pd.read_csv(next_surrogate[0], sep='\\t',encoding='utf-8')\n",
    "            original_df2=pd.read_csv(next_surrogate[1], sep='\\t',encoding='utf-8')\n",
    "            \n",
    "            # get participants A and B from df1\n",
    "            participantA_1_code = min(original_df1['participant'].unique())\n",
    "            participantB_1_code = max(original_df1['participant'].unique())\n",
    "            participantA_1 = original_df1[original_df1['participant'] == participantA_1_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            participantB_1 = original_df1[original_df1['participant'] == participantB_1_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            \n",
    "            # get participants A and B from df2\n",
    "            participantA_2_code = min(original_df2['participant'].unique())\n",
    "            participantB_2_code = max(original_df2['participant'].unique())\n",
    "            participantA_2 = original_df2[original_df2['participant'] == participantA_2_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            participantB_2 = original_df2[original_df2['participant'] == participantB_2_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            \n",
    "            # identify truncation point for both surrogates (to have even number of turns)\n",
    "            surrogateX_turns=min([participantA_1.shape[0],\n",
    "                                  participantB_2.shape[0]])\n",
    "            surrogateY_turns=min([participantA_2.shape[0],\n",
    "                                  participantB_1.shape[0]])\n",
    "            \n",
    "            # if desired, preserve original turn order for surrogate pairs\n",
    "            if keep_original_turn_order == True:                \n",
    "                surrogateX = participantA_1.truncate(after=surrogateX_turns-1,copy=False).append(\n",
    "                                participantB_2.truncate(after=surrogateX_turns-1,copy=False)).sort_index(\n",
    "                                ).reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "#                 surrogateX.reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "#                                 participantB_2.truncate(after=surrogateX_turns-1,copy=False)).sort(\n",
    "#                                 ['index']).reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "\n",
    "                surrogateY = participantA_2.truncate(after=surrogateX_turns-1,copy=False).append(\n",
    "                                participantB_1.truncate(after=surrogateX_turns-1,copy=False)).sort_index(\n",
    "                                ).reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "#                 surrogateY = participantA_2.truncate(after=surrogateX_turns-1,copy=False).append(\n",
    "#                                 participantB_1.truncate(after=surrogateX_turns-1,copy=False)).sort(\n",
    "#                                 ['index']).reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "            \n",
    "            # otherwise, just shuffle all turns within participants\n",
    "            else:\n",
    "                \n",
    "                # shuffle for first surrogate pairing\n",
    "                surrogateX_A1 = participantA_1.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateX_B2 = participantB_2.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateX = surrogateX_A1.append(surrogateX_B2).sort_index().reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "                \n",
    "                # and for second surrogate pairing\n",
    "                surrogateY_A2 = participantA_2.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateY_B1 = participantB_1.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateY = surrogateY_A2.append(surrogateY_B1).sort_index().reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "\n",
    "            # create filename for our surrogate file\n",
    "            original_dyad1 = re.findall(dyad_label+'[^'+id_separator+']*',original_file1)[0]\n",
    "            original_dyad2 = re.findall(dyad_label+'[^'+id_separator+']*',original_file2)[0]\n",
    "            surrogateX['file'] = condition + '-' + original_dyad1 + '-' + original_dyad2\n",
    "            surrogateY['file'] = condition + '-' + original_dyad1 + '-' + original_dyad2\n",
    "            nameX='SurrogatePair-'+original_dyad1+'A'+'-'+original_dyad2+'B'+'-'+condition+'.txt'\n",
    "            nameY='SurrogatePair-'+original_dyad2+'A'+'-'+original_dyad1+'B'+'-'+condition+'.txt'\n",
    "            \n",
    "            # save to file\n",
    "            surrogateX.to_csv(new_surrogate_path + nameX, encoding='utf-8',index=False,sep='\\t')\n",
    "            surrogateY.to_csv(new_surrogate_path + nameY, encoding='utf-8',index=False,sep='\\t')\n",
    "            \n",
    "    # return list of all surrogate files\n",
    "    return glob.glob(new_surrogate_path+\"*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "RUN Phase 2: Surrogate Partners\n",
    "-------------------------------\n",
    "* Runs function to generate new surrogate transcript conversations (separate files)\n",
    "* For each surrogate transcript file, runs turn-level and conversational-level alignment scores\n",
    "* Saves output into single datasheet to be used in statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def PHASE2RUN_SURROGATE(\n",
    "                         input_files, \n",
    "                         surrogate_file_directory,\n",
    "                         output_file_directory,\n",
    "                         semantic_model_input_file,\n",
    "                         pretrained_input_file,   \n",
    "                         high_sd_cutoff=3,\n",
    "                         low_n_cutoff=1,\n",
    "                         id_separator = '\\-',\n",
    "                         condition_label='cond',\n",
    "                         dyad_label='dyad',\n",
    "                         all_surrogates=False,\n",
    "                         keep_original_turn_order=False,\n",
    "                         delay=1,\n",
    "                         maxngram=2,\n",
    "                         use_pretrained_vectors=False,   \n",
    "                         ignore_duplicates=True,\n",
    "                         add_stanford_tags=False,\n",
    "                         input_as_directory=True):   \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Given a directory of individual .txt files and the\n",
    "    vocab list that have been generated by the `PHASE1RUN` \n",
    "    preparation stage, return multi-level alignment \n",
    "    scores with turn-by-turn and conversation-level metrics\n",
    "    for surrogate baseline conversations.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    high-frequency cutoff of 3 SD over the mean. If \n",
    "    desired, this can be changed with the \n",
    "    `high_sd_cutoff` argument and can be removed with\n",
    "    `high_sd_cutoff=None`.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    low-frequency cutoff in which a word will be \n",
    "    removed if they occur 1 or fewer times. if\n",
    "    desired, this can be changed with the \n",
    "    `low_n_cutoff` argument and can be removed with\n",
    "    `low_n_cutoff=0`.\n",
    "    \n",
    "    By default, compare only adjacent turns. If desired,\n",
    "    the comparison distance may be changed by increasing\n",
    "    the `delay` argument.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 2. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `add_stanford_tags=True`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \n",
    "    By default, the separator between dyad ID and\n",
    "    condition ID in each file name is a hyphen ('\\-'). \n",
    "    If desired, this may be changed with the \n",
    "    `id_separator` argument.\n",
    "\n",
    "    By default, condition IDs in each file name\n",
    "    will be identified as any characters following \n",
    "    `cond`. If desired, this may be changed with the \n",
    "    `condition_label` argument.\n",
    "    \n",
    "    By default, dyad IDs in each file name\n",
    "    will be identified as any characters following \n",
    "    `dyad`. If desired, this may be changed with the \n",
    "    `dyad_label` argument.\n",
    "    \n",
    "    By default, generate surrogates only from a subset\n",
    "    of all possible pairings. If desired, instead \n",
    "    generate surrogates from all possible pairings\n",
    "    with `all_surrogates=True`.\n",
    "    \n",
    "    By default, accept `input_files` as a directory\n",
    "    that includes `.txt` files of each individual \n",
    "    conversation. If desired, provide individual files\n",
    "    as a list of literal paths to the `input_files`\n",
    "    argument and set `input_as_directory=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # grab the files in the input list\n",
    "    if input_as_directory==False:\n",
    "        file_list = glob.glob(input_files)\n",
    "    else:\n",
    "        file_list = glob.glob(input_files+\"*.txt\")\n",
    "    \n",
    "    # create a surrogate file list\n",
    "    surrogate_file_list = GenerateSurrogate(original_conversation_list = file_list,\n",
    "                                                   surrogate_file_directory = surrogate_file_directory,\n",
    "                                                   all_surrogates = all_surrogates,\n",
    "                                                   id_separator = id_separator,\n",
    "                                                   condition_label = condition_label,\n",
    "                                                   dyad_label = dyad_label,\n",
    "                                                   keep_original_turn_order = keep_original_turn_order) \n",
    "    \n",
    "    # build the semantic model to be used for all conversations\n",
    "    [vocablist, highDimModel] = BuildSemanticModel(semantic_model_input_file=semantic_model_input_file,\n",
    "                                                       pretrained_input_file=pretrained_input_file,\n",
    "                                                       use_pretrained_vectors=use_pretrained_vectors,\n",
    "                                                       high_sd_cutoff=high_sd_cutoff,\n",
    "                                                       low_n_cutoff=low_n_cutoff)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # create containers for alignment values\n",
    "    AlignmentT2T = pd.DataFrame()\n",
    "    AlignmentC2C = pd.DataFrame()\n",
    "    \n",
    "    # cycle through the files\n",
    "    for fileName in surrogate_file_list:\n",
    "        \n",
    "        # process the file if it's got a valid conversation\n",
    "        dataframe=pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        if len(dataframe) > 0:\n",
    "            \n",
    "            # let us know which filename we're processing\n",
    "            print \"Processing: \"+fileName   \n",
    "\n",
    "            # calculate turn-by-turn alignment scores\n",
    "            xT2T=TurnByTurnAnalysis(dataframe=dataframe,\n",
    "                                         delay=delay,\n",
    "                                         maxngram=maxngram,\n",
    "                                         vocablist=vocablist,\n",
    "                                         highDimModel=highDimModel)\n",
    "            AlignmentT2T=AlignmentT2T.append(xT2T)\n",
    "            \n",
    "            # calculate conversation-level alignment scores\n",
    "            xC2C = ConvoByConvoAnalysis(dataframe=dataframe,\n",
    "                                             maxngram = maxngram,\n",
    "                                             ignore_duplicates=ignore_duplicates,\n",
    "                                             add_stanford_tags = add_stanford_tags)\n",
    "            AlignmentC2C=AlignmentC2C.append(xC2C)\n",
    "        \n",
    "        # if it's invalid, let us know\n",
    "        else:\n",
    "            print \"Invalid file: \"+fileName   \n",
    "            \n",
    "    # update final dataframes\n",
    "    FINAL_TURN_SURROGATE = AlignmentT2T.reset_index(drop=True)\n",
    "    FINAL_CONVO_SURROGATE = AlignmentC2C.reset_index(drop=True)\n",
    "    \n",
    "    # export the final files\n",
    "    FINAL_TURN_SURROGATE.to_csv(output_file_directory+\"AlignmentT2T_Surrogate.txt\",\n",
    "                      encoding='utf-8',index=False,sep='\\t')   \n",
    "    FINAL_CONVO_SURROGATE.to_csv(output_file_directory+\"AlignmentC2C_Surrogate.txt\",\n",
    "                       encoding='utf-8',index=False,sep='\\t') \n",
    "\n",
    "    # display the info, too\n",
    "    return FINAL_TURN_SURROGATE, FINAL_CONVO_SURROGATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Run everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase 1: Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**To Alex** Note the use of the tasa.txt corpus to build training dictionary for spell-checking. The previous \"big.txt\" was based on Project Gutenberg texts and it was oddly missing critical words. I noticed this when the word \"lesbian\" kept getting \"spell-corrected\" to \"lesion.\" The TASA corpus is much better for our needs. It's based on excerpts from hundreds of textbooks with narrative and expository texts aimed to middle school and high school. It's also about the same size as \"big.txt\" with 1 million words. Also note that I intend to build a semantic space using TASA and each excerpt as a context. I would like to make this available to users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-original/dyad_10-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-original/dyad_10-condition_2.txt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nduran/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n",
      "/Users/nduran/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-original/dyad_10-condition_3.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-original/dyad_10-condition_4.txt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nduran/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n",
      "/Users/nduran/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-original/dyad_12-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-original/dyad_12-condition_2.txt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nduran/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n",
      "/Users/nduran/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-original/dyad_15-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-original/dyad_15-condition_1.txt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nduran/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n",
      "/Users/nduran/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-original/dyad_13-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-original/dyad_13-condition_2.txt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nduran/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n",
      "/Users/nduran/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>content</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tagged_token</th>\n",
       "      <th>tagged_lemma</th>\n",
       "      <th>tagged_stan_token</th>\n",
       "      <th>tagged_stan_lemma</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing</td>\n",
       "      <td>[what, are, we, doing]</td>\n",
       "      <td>[what, be, we, do]</td>\n",
       "      <td>[(what, WDT), (are, VBP), (we, PRP), (doing, V...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VBG)]</td>\n",
       "      <td>[(what, WDT), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, us, test, some, more]</td>\n",
       "      <td>[let, u, test, some, more]</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, JJ), (test, VB), (some, DT), (...</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, FW), (test, NN), (some, DT), (...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing this is just some sample text</td>\n",
       "      <td>[and, more, testing, this, is, just, some, sam...</td>\n",
       "      <td>[and, more, test, this, be, just, some, sample...</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, VBG), (this...</td>\n",
       "      <td>[(and, CC), (more, JJR), (test, NN), (this, DT...</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN), (this,...</td>\n",
       "      <td>[(and, CC), (more, RBR), (test, NN), (this, DT...</td>\n",
       "      <td>dyad_10-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>let's ours test some we're going to do more an...</td>\n",
       "      <td>[let, us, ours, test, some, we, are, going, to...</td>\n",
       "      <td>[let, u, ours, test, some, we, be, go, to, do,...</td>\n",
       "      <td>[(let, VB), (us, PRP), (ours, VB), (test, VB),...</td>\n",
       "      <td>[(let, VB), (u, JJ), (ours, VB), (test, JJS), ...</td>\n",
       "      <td>[(let, VB), (us, PRP), (ours, JJ), (test, NN),...</td>\n",
       "      <td>[(let, VB), (u, FW), (ours, JJ), (test, NN), (...</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>he's going mad</td>\n",
       "      <td>[he, is, going, mad]</td>\n",
       "      <td>[he, be, go, mad]</td>\n",
       "      <td>[(he, PRP), (is, VBZ), (going, VBG), (mad, JJ)]</td>\n",
       "      <td>[(he, PRP), (be, VB), (go, VBN), (mad, JJ)]</td>\n",
       "      <td>[(he, PRP), (is, VBZ), (going, VBG), (mad, JJ)]</td>\n",
       "      <td>[(he, PRP), (be, VB), (go, VB), (mad, JJ)]</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>this is some sapmple text</td>\n",
       "      <td>[this, is, some, sample, text]</td>\n",
       "      <td>[this, be, some, sample, text]</td>\n",
       "      <td>[(this, DT), (is, VBZ), (some, DT), (sample, J...</td>\n",
       "      <td>[(this, DT), (be, VB), (some, DT), (sample, JJ...</td>\n",
       "      <td>[(this, DT), (is, VBZ), (some, DT), (sample, N...</td>\n",
       "      <td>[(this, DT), (be, VB), (some, DT), (sample, NN...</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing</td>\n",
       "      <td>[what, are, we, doing]</td>\n",
       "      <td>[what, be, we, do]</td>\n",
       "      <td>[(what, WDT), (are, VBP), (we, PRP), (doing, V...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VBG)]</td>\n",
       "      <td>[(what, WDT), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>the task about what we're saying right now</td>\n",
       "      <td>[the, task, about, what, we, are, saying, righ...</td>\n",
       "      <td>[the, task, about, what, we, be, say, right, now]</td>\n",
       "      <td>[(the, DT), (task, NN), (about, IN), (what, WP...</td>\n",
       "      <td>[(the, DT), (task, NN), (about, IN), (what, WP...</td>\n",
       "      <td>[(the, DT), (task, NN), (about, IN), (what, WP...</td>\n",
       "      <td>[(the, DT), (task, NN), (about, IN), (what, WP...</td>\n",
       "      <td>dyad_10-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more and more testing</td>\n",
       "      <td>[let, us, test, some, more, and, more, testing]</td>\n",
       "      <td>[let, u, test, some, more, and, more, testing]</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, JJ), (test, VB), (some, DT), (...</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, FW), (test, NN), (some, DT), (...</td>\n",
       "      <td>dyad_10-condition_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_10-condition_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>this is just some sample text what're we doing</td>\n",
       "      <td>[this, is, just, some, sample, text, what, are...</td>\n",
       "      <td>[this, be, just, some, sample, text, what, be,...</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, VB), (just, RB), (some, DT),...</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, VB), (just, RB), (some, DT),...</td>\n",
       "      <td>dyad_10-condition_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_10-condition_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_10-condition_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more and more testing</td>\n",
       "      <td>[let, us, test, some, more, and, more, testing]</td>\n",
       "      <td>[let, u, test, some, more, and, more, testing]</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, JJ), (test, VB), (some, DT), (...</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, FW), (test, NN), (some, DT), (...</td>\n",
       "      <td>dyad_10-condition_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_10-condition_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>this is just some sample text</td>\n",
       "      <td>[this, is, just, some, sample, text]</td>\n",
       "      <td>[this, be, just, some, sample, text]</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, VB), (just, RB), (some, DT),...</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, VB), (just, RB), (some, DT),...</td>\n",
       "      <td>dyad_10-condition_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing</td>\n",
       "      <td>[what, are, we, doing]</td>\n",
       "      <td>[what, be, we, do]</td>\n",
       "      <td>[(what, WDT), (are, VBP), (we, PRP), (doing, V...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VBG)]</td>\n",
       "      <td>[(what, WDT), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>dyad_10-condition_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_10-condition_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_10-condition_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>this is just some sample text</td>\n",
       "      <td>[this, is, just, some, sample, text]</td>\n",
       "      <td>[this, be, just, some, sample, text]</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, VB), (just, RB), (some, DT),...</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, VB), (just, RB), (some, DT),...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, us, test, some, more]</td>\n",
       "      <td>[let, u, test, some, more]</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, JJ), (test, VB), (some, DT), (...</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, FW), (test, NN), (some, DT), (...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing</td>\n",
       "      <td>[what, are, we, doing]</td>\n",
       "      <td>[what, be, we, do]</td>\n",
       "      <td>[(what, WDT), (are, VBP), (we, PRP), (doing, V...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VBG)]</td>\n",
       "      <td>[(what, WDT), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_12-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing</td>\n",
       "      <td>[what, are, we, doing]</td>\n",
       "      <td>[what, be, we, do]</td>\n",
       "      <td>[(what, WDT), (are, VBP), (we, PRP), (doing, V...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VBG)]</td>\n",
       "      <td>[(what, WDT), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, us, test, some, more]</td>\n",
       "      <td>[let, u, test, some, more]</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, JJ), (test, VB), (some, DT), (...</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, FW), (test, NN), (some, DT), (...</td>\n",
       "      <td>dyad_12-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing</td>\n",
       "      <td>[what, are, we, doing]</td>\n",
       "      <td>[what, be, we, do]</td>\n",
       "      <td>[(what, WDT), (are, VBP), (we, PRP), (doing, V...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VBG)]</td>\n",
       "      <td>[(what, WDT), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, us, test, some, more]</td>\n",
       "      <td>[let, u, test, some, more]</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, JJ), (test, VB), (some, DT), (...</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, FW), (test, NN), (some, DT), (...</td>\n",
       "      <td>dyad_15-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>this is just some sample text</td>\n",
       "      <td>[this, is, just, some, sample, text]</td>\n",
       "      <td>[this, be, just, some, sample, text]</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, VB), (just, RB), (some, DT),...</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, VB), (just, RB), (some, DT),...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, us, test, some, more]</td>\n",
       "      <td>[let, u, test, some, more]</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, JJ), (test, VB), (some, DT), (...</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, FW), (test, NN), (some, DT), (...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[and, more, testing]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN)]</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing</td>\n",
       "      <td>[what, are, we, doing]</td>\n",
       "      <td>[what, be, we, do]</td>\n",
       "      <td>[(what, WDT), (are, VBP), (we, PRP), (doing, V...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VBG)]</td>\n",
       "      <td>[(what, WDT), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_15-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing</td>\n",
       "      <td>[what, are, we, doing]</td>\n",
       "      <td>[what, be, we, do]</td>\n",
       "      <td>[(what, WDT), (are, VBP), (we, PRP), (doing, V...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VBG)]</td>\n",
       "      <td>[(what, WDT), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more</td>\n",
       "      <td>[let, us, test, some, more]</td>\n",
       "      <td>[let, u, test, some, more]</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, JJ), (test, VB), (some, DT), (...</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, FW), (test, NN), (some, DT), (...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>and more testing this is just some sample text</td>\n",
       "      <td>[and, more, testing, this, is, just, some, sam...</td>\n",
       "      <td>[and, more, test, this, be, just, some, sample...</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, VBG), (this...</td>\n",
       "      <td>[(and, CC), (more, JJR), (test, NN), (this, DT...</td>\n",
       "      <td>[(and, CC), (more, RBR), (testing, NN), (this,...</td>\n",
       "      <td>[(and, CC), (more, RBR), (test, NN), (this, DT...</td>\n",
       "      <td>dyad_13-condition_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>let's test some more and more testing</td>\n",
       "      <td>[let, us, test, some, more, and, more, testing]</td>\n",
       "      <td>[let, u, test, some, more, and, more, testing]</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, JJ), (test, VB), (some, DT), (...</td>\n",
       "      <td>[(let, VB), (us, PRP), (test, VB), (some, DT),...</td>\n",
       "      <td>[(let, VB), (u, FW), (test, NN), (some, DT), (...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>more testing but we need more variety</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[more, testing, but, we, need, more, variety]</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, JJ), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>[(more, RBR), (testing, NN), (but, CC), (we, P...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>this is just some sample text</td>\n",
       "      <td>[this, is, just, some, sample, text]</td>\n",
       "      <td>[this, be, just, some, sample, text]</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, VB), (just, RB), (some, DT),...</td>\n",
       "      <td>[(this, DT), (is, VBZ), (just, RB), (some, DT)...</td>\n",
       "      <td>[(this, DT), (be, VB), (just, RB), (some, DT),...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>what're we doing</td>\n",
       "      <td>[what, are, we, doing]</td>\n",
       "      <td>[what, be, we, do]</td>\n",
       "      <td>[(what, WDT), (are, VBP), (we, PRP), (doing, V...</td>\n",
       "      <td>[(what, WP), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>[(what, WP), (are, VBP), (we, PRP), (doing, VBG)]</td>\n",
       "      <td>[(what, WDT), (be, VB), (we, PRP), (do, VBP)]</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>we just want to make sure that we're capturin...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, are...</td>\n",
       "      <td>[we, just, want, to, make, sure, that, we, be,...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>[(we, PRP), (just, RB), (want, VBP), (to, TO),...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>we don't really care about what we're saying r...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, a...</td>\n",
       "      <td>[we, do, not, really, care, about, what, we, b...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>[(we, PRP), (do, VBP), (not, RB), (really, RB)...</td>\n",
       "      <td>dyad_13-condition_2.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant                                            content  \\\n",
       "0             1                                  what're we doing    \n",
       "1             2   we just want to make sure that we're capturin...   \n",
       "2             1  we don't really care about what we're saying r...   \n",
       "3             2                               let's test some more   \n",
       "4             1              more testing but we need more variety   \n",
       "5             2     and more testing this is just some sample text   \n",
       "0             2  let's ours test some we're going to do more an...   \n",
       "1             1                                    he's going mad    \n",
       "2             2                          this is some sapmple text   \n",
       "3             1                                  what're we doing    \n",
       "4             2   we just want to make sure that we're capturin...   \n",
       "5             1         the task about what we're saying right now   \n",
       "0             2              let's test some more and more testing   \n",
       "1             1              more testing but we need more variety   \n",
       "2             2    this is just some sample text what're we doing    \n",
       "3             1   we just want to make sure that we're capturin...   \n",
       "4             2  we don't really care about what we're saying r...   \n",
       "0             2              let's test some more and more testing   \n",
       "1             1              more testing but we need more variety   \n",
       "2             2                      this is just some sample text   \n",
       "3             1                                  what're we doing    \n",
       "4             2   we just want to make sure that we're capturin...   \n",
       "5             1  we don't really care about what we're saying r...   \n",
       "0             2                      this is just some sample text   \n",
       "1             1  we don't really care about what we're saying r...   \n",
       "2             2                               let's test some more   \n",
       "3             1              more testing but we need more variety   \n",
       "4             2                                   and more testing   \n",
       "5             1                                  what're we doing    \n",
       "6             2   we just want to make sure that we're capturin...   \n",
       "..          ...                                                ...   \n",
       "1             2                                   and more testing   \n",
       "2             1                                  what're we doing    \n",
       "3             2   we just want to make sure that we're capturin...   \n",
       "4             1  we don't really care about what we're saying r...   \n",
       "5             2                               let's test some more   \n",
       "0             1              more testing but we need more variety   \n",
       "1             2                                   and more testing   \n",
       "2             1                                  what're we doing    \n",
       "3             2   we just want to make sure that we're capturin...   \n",
       "4             1  we don't really care about what we're saying r...   \n",
       "5             2                               let's test some more   \n",
       "0             2                      this is just some sample text   \n",
       "1             1  we don't really care about what we're saying r...   \n",
       "2             2                               let's test some more   \n",
       "3             1              more testing but we need more variety   \n",
       "4             2                                   and more testing   \n",
       "5             1                                  what're we doing    \n",
       "6             2   we just want to make sure that we're capturin...   \n",
       "0             1                                  what're we doing    \n",
       "1             2   we just want to make sure that we're capturin...   \n",
       "2             1  we don't really care about what we're saying r...   \n",
       "3             2                               let's test some more   \n",
       "4             1              more testing but we need more variety   \n",
       "5             2     and more testing this is just some sample text   \n",
       "0             2              let's test some more and more testing   \n",
       "1             1              more testing but we need more variety   \n",
       "2             2                      this is just some sample text   \n",
       "3             1                                  what're we doing    \n",
       "4             2   we just want to make sure that we're capturin...   \n",
       "5             1  we don't really care about what we're saying r...   \n",
       "\n",
       "                                                token  \\\n",
       "0                              [what, are, we, doing]   \n",
       "1   [we, just, want, to, make, sure, that, we, are...   \n",
       "2   [we, do, not, really, care, about, what, we, a...   \n",
       "3                         [let, us, test, some, more]   \n",
       "4       [more, testing, but, we, need, more, variety]   \n",
       "5   [and, more, testing, this, is, just, some, sam...   \n",
       "0   [let, us, ours, test, some, we, are, going, to...   \n",
       "1                                [he, is, going, mad]   \n",
       "2                      [this, is, some, sample, text]   \n",
       "3                              [what, are, we, doing]   \n",
       "4   [we, just, want, to, make, sure, that, we, are...   \n",
       "5   [the, task, about, what, we, are, saying, righ...   \n",
       "0     [let, us, test, some, more, and, more, testing]   \n",
       "1       [more, testing, but, we, need, more, variety]   \n",
       "2   [this, is, just, some, sample, text, what, are...   \n",
       "3   [we, just, want, to, make, sure, that, we, are...   \n",
       "4   [we, do, not, really, care, about, what, we, a...   \n",
       "0     [let, us, test, some, more, and, more, testing]   \n",
       "1       [more, testing, but, we, need, more, variety]   \n",
       "2                [this, is, just, some, sample, text]   \n",
       "3                              [what, are, we, doing]   \n",
       "4   [we, just, want, to, make, sure, that, we, are...   \n",
       "5   [we, do, not, really, care, about, what, we, a...   \n",
       "0                [this, is, just, some, sample, text]   \n",
       "1   [we, do, not, really, care, about, what, we, a...   \n",
       "2                         [let, us, test, some, more]   \n",
       "3       [more, testing, but, we, need, more, variety]   \n",
       "4                                [and, more, testing]   \n",
       "5                              [what, are, we, doing]   \n",
       "6   [we, just, want, to, make, sure, that, we, are...   \n",
       "..                                                ...   \n",
       "1                                [and, more, testing]   \n",
       "2                              [what, are, we, doing]   \n",
       "3   [we, just, want, to, make, sure, that, we, are...   \n",
       "4   [we, do, not, really, care, about, what, we, a...   \n",
       "5                         [let, us, test, some, more]   \n",
       "0       [more, testing, but, we, need, more, variety]   \n",
       "1                                [and, more, testing]   \n",
       "2                              [what, are, we, doing]   \n",
       "3   [we, just, want, to, make, sure, that, we, are...   \n",
       "4   [we, do, not, really, care, about, what, we, a...   \n",
       "5                         [let, us, test, some, more]   \n",
       "0                [this, is, just, some, sample, text]   \n",
       "1   [we, do, not, really, care, about, what, we, a...   \n",
       "2                         [let, us, test, some, more]   \n",
       "3       [more, testing, but, we, need, more, variety]   \n",
       "4                                [and, more, testing]   \n",
       "5                              [what, are, we, doing]   \n",
       "6   [we, just, want, to, make, sure, that, we, are...   \n",
       "0                              [what, are, we, doing]   \n",
       "1   [we, just, want, to, make, sure, that, we, are...   \n",
       "2   [we, do, not, really, care, about, what, we, a...   \n",
       "3                         [let, us, test, some, more]   \n",
       "4       [more, testing, but, we, need, more, variety]   \n",
       "5   [and, more, testing, this, is, just, some, sam...   \n",
       "0     [let, us, test, some, more, and, more, testing]   \n",
       "1       [more, testing, but, we, need, more, variety]   \n",
       "2                [this, is, just, some, sample, text]   \n",
       "3                              [what, are, we, doing]   \n",
       "4   [we, just, want, to, make, sure, that, we, are...   \n",
       "5   [we, do, not, really, care, about, what, we, a...   \n",
       "\n",
       "                                                lemma  \\\n",
       "0                                  [what, be, we, do]   \n",
       "1   [we, just, want, to, make, sure, that, we, be,...   \n",
       "2   [we, do, not, really, care, about, what, we, b...   \n",
       "3                          [let, u, test, some, more]   \n",
       "4       [more, testing, but, we, need, more, variety]   \n",
       "5   [and, more, test, this, be, just, some, sample...   \n",
       "0   [let, u, ours, test, some, we, be, go, to, do,...   \n",
       "1                                   [he, be, go, mad]   \n",
       "2                      [this, be, some, sample, text]   \n",
       "3                                  [what, be, we, do]   \n",
       "4   [we, just, want, to, make, sure, that, we, be,...   \n",
       "5   [the, task, about, what, we, be, say, right, now]   \n",
       "0      [let, u, test, some, more, and, more, testing]   \n",
       "1       [more, testing, but, we, need, more, variety]   \n",
       "2   [this, be, just, some, sample, text, what, be,...   \n",
       "3   [we, just, want, to, make, sure, that, we, be,...   \n",
       "4   [we, do, not, really, care, about, what, we, b...   \n",
       "0      [let, u, test, some, more, and, more, testing]   \n",
       "1       [more, testing, but, we, need, more, variety]   \n",
       "2                [this, be, just, some, sample, text]   \n",
       "3                                  [what, be, we, do]   \n",
       "4   [we, just, want, to, make, sure, that, we, be,...   \n",
       "5   [we, do, not, really, care, about, what, we, b...   \n",
       "0                [this, be, just, some, sample, text]   \n",
       "1   [we, do, not, really, care, about, what, we, b...   \n",
       "2                          [let, u, test, some, more]   \n",
       "3       [more, testing, but, we, need, more, variety]   \n",
       "4                                [and, more, testing]   \n",
       "5                                  [what, be, we, do]   \n",
       "6   [we, just, want, to, make, sure, that, we, be,...   \n",
       "..                                                ...   \n",
       "1                                [and, more, testing]   \n",
       "2                                  [what, be, we, do]   \n",
       "3   [we, just, want, to, make, sure, that, we, be,...   \n",
       "4   [we, do, not, really, care, about, what, we, b...   \n",
       "5                          [let, u, test, some, more]   \n",
       "0       [more, testing, but, we, need, more, variety]   \n",
       "1                                [and, more, testing]   \n",
       "2                                  [what, be, we, do]   \n",
       "3   [we, just, want, to, make, sure, that, we, be,...   \n",
       "4   [we, do, not, really, care, about, what, we, b...   \n",
       "5                          [let, u, test, some, more]   \n",
       "0                [this, be, just, some, sample, text]   \n",
       "1   [we, do, not, really, care, about, what, we, b...   \n",
       "2                          [let, u, test, some, more]   \n",
       "3       [more, testing, but, we, need, more, variety]   \n",
       "4                                [and, more, testing]   \n",
       "5                                  [what, be, we, do]   \n",
       "6   [we, just, want, to, make, sure, that, we, be,...   \n",
       "0                                  [what, be, we, do]   \n",
       "1   [we, just, want, to, make, sure, that, we, be,...   \n",
       "2   [we, do, not, really, care, about, what, we, b...   \n",
       "3                          [let, u, test, some, more]   \n",
       "4       [more, testing, but, we, need, more, variety]   \n",
       "5   [and, more, test, this, be, just, some, sample...   \n",
       "0      [let, u, test, some, more, and, more, testing]   \n",
       "1       [more, testing, but, we, need, more, variety]   \n",
       "2                [this, be, just, some, sample, text]   \n",
       "3                                  [what, be, we, do]   \n",
       "4   [we, just, want, to, make, sure, that, we, be,...   \n",
       "5   [we, do, not, really, care, about, what, we, b...   \n",
       "\n",
       "                                         tagged_token  \\\n",
       "0   [(what, WDT), (are, VBP), (we, PRP), (doing, V...   \n",
       "1   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "2   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "3   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "4   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "5   [(and, CC), (more, RBR), (testing, VBG), (this...   \n",
       "0   [(let, VB), (us, PRP), (ours, VB), (test, VB),...   \n",
       "1     [(he, PRP), (is, VBZ), (going, VBG), (mad, JJ)]   \n",
       "2   [(this, DT), (is, VBZ), (some, DT), (sample, J...   \n",
       "3   [(what, WDT), (are, VBP), (we, PRP), (doing, V...   \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "5   [(the, DT), (task, NN), (about, IN), (what, WP...   \n",
       "0   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "1   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "2   [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "0   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "1   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "2   [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "3   [(what, WDT), (are, VBP), (we, PRP), (doing, V...   \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "5   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "0   [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "1   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "2   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "3   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "4             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "5   [(what, WDT), (are, VBP), (we, PRP), (doing, V...   \n",
       "6   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "..                                                ...   \n",
       "1             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "2   [(what, WDT), (are, VBP), (we, PRP), (doing, V...   \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "5   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "0   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "1             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "2   [(what, WDT), (are, VBP), (we, PRP), (doing, V...   \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "5   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "0   [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "1   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "2   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "3   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "4             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "5   [(what, WDT), (are, VBP), (we, PRP), (doing, V...   \n",
       "6   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "0   [(what, WDT), (are, VBP), (we, PRP), (doing, V...   \n",
       "1   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "2   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "3   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "4   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "5   [(and, CC), (more, RBR), (testing, VBG), (this...   \n",
       "0   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "1   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "2   [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "3   [(what, WDT), (are, VBP), (we, PRP), (doing, V...   \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "5   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "\n",
       "                                         tagged_lemma  \\\n",
       "0        [(what, WP), (be, VB), (we, PRP), (do, VBP)]   \n",
       "1   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "2   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "3   [(let, VB), (u, JJ), (test, VB), (some, DT), (...   \n",
       "4   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "5   [(and, CC), (more, JJR), (test, NN), (this, DT...   \n",
       "0   [(let, VB), (u, JJ), (ours, VB), (test, JJS), ...   \n",
       "1         [(he, PRP), (be, VB), (go, VBN), (mad, JJ)]   \n",
       "2   [(this, DT), (be, VB), (some, DT), (sample, JJ...   \n",
       "3        [(what, WP), (be, VB), (we, PRP), (do, VBP)]   \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "5   [(the, DT), (task, NN), (about, IN), (what, WP...   \n",
       "0   [(let, VB), (u, JJ), (test, VB), (some, DT), (...   \n",
       "1   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "2   [(this, DT), (be, VB), (just, RB), (some, DT),...   \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "0   [(let, VB), (u, JJ), (test, VB), (some, DT), (...   \n",
       "1   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "2   [(this, DT), (be, VB), (just, RB), (some, DT),...   \n",
       "3        [(what, WP), (be, VB), (we, PRP), (do, VBP)]   \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "5   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "0   [(this, DT), (be, VB), (just, RB), (some, DT),...   \n",
       "1   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "2   [(let, VB), (u, JJ), (test, VB), (some, DT), (...   \n",
       "3   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "4             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "5        [(what, WP), (be, VB), (we, PRP), (do, VBP)]   \n",
       "6   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "..                                                ...   \n",
       "1             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "2        [(what, WP), (be, VB), (we, PRP), (do, VBP)]   \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "5   [(let, VB), (u, JJ), (test, VB), (some, DT), (...   \n",
       "0   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "1             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "2        [(what, WP), (be, VB), (we, PRP), (do, VBP)]   \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "5   [(let, VB), (u, JJ), (test, VB), (some, DT), (...   \n",
       "0   [(this, DT), (be, VB), (just, RB), (some, DT),...   \n",
       "1   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "2   [(let, VB), (u, JJ), (test, VB), (some, DT), (...   \n",
       "3   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "4             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "5        [(what, WP), (be, VB), (we, PRP), (do, VBP)]   \n",
       "6   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "0        [(what, WP), (be, VB), (we, PRP), (do, VBP)]   \n",
       "1   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "2   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "3   [(let, VB), (u, JJ), (test, VB), (some, DT), (...   \n",
       "4   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "5   [(and, CC), (more, JJR), (test, NN), (this, DT...   \n",
       "0   [(let, VB), (u, JJ), (test, VB), (some, DT), (...   \n",
       "1   [(more, RBR), (testing, JJ), (but, CC), (we, P...   \n",
       "2   [(this, DT), (be, VB), (just, RB), (some, DT),...   \n",
       "3        [(what, WP), (be, VB), (we, PRP), (do, VBP)]   \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "5   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "\n",
       "                                    tagged_stan_token  \\\n",
       "0   [(what, WP), (are, VBP), (we, PRP), (doing, VBG)]   \n",
       "1   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "2   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "3   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "4   [(more, RBR), (testing, NN), (but, CC), (we, P...   \n",
       "5   [(and, CC), (more, RBR), (testing, NN), (this,...   \n",
       "0   [(let, VB), (us, PRP), (ours, JJ), (test, NN),...   \n",
       "1     [(he, PRP), (is, VBZ), (going, VBG), (mad, JJ)]   \n",
       "2   [(this, DT), (is, VBZ), (some, DT), (sample, N...   \n",
       "3   [(what, WP), (are, VBP), (we, PRP), (doing, VBG)]   \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "5   [(the, DT), (task, NN), (about, IN), (what, WP...   \n",
       "0   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "1   [(more, RBR), (testing, NN), (but, CC), (we, P...   \n",
       "2   [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "0   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "1   [(more, RBR), (testing, NN), (but, CC), (we, P...   \n",
       "2   [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "3   [(what, WP), (are, VBP), (we, PRP), (doing, VBG)]   \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "5   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "0   [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "1   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "2   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "3   [(more, RBR), (testing, NN), (but, CC), (we, P...   \n",
       "4             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "5   [(what, WP), (are, VBP), (we, PRP), (doing, VBG)]   \n",
       "6   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "..                                                ...   \n",
       "1             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "2   [(what, WP), (are, VBP), (we, PRP), (doing, VBG)]   \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "5   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "0   [(more, RBR), (testing, NN), (but, CC), (we, P...   \n",
       "1             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "2   [(what, WP), (are, VBP), (we, PRP), (doing, VBG)]   \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "5   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "0   [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "1   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "2   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "3   [(more, RBR), (testing, NN), (but, CC), (we, P...   \n",
       "4             [(and, CC), (more, RBR), (testing, NN)]   \n",
       "5   [(what, WP), (are, VBP), (we, PRP), (doing, VBG)]   \n",
       "6   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "0   [(what, WP), (are, VBP), (we, PRP), (doing, VBG)]   \n",
       "1   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "2   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "3   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "4   [(more, RBR), (testing, NN), (but, CC), (we, P...   \n",
       "5   [(and, CC), (more, RBR), (testing, NN), (this,...   \n",
       "0   [(let, VB), (us, PRP), (test, VB), (some, DT),...   \n",
       "1   [(more, RBR), (testing, NN), (but, CC), (we, P...   \n",
       "2   [(this, DT), (is, VBZ), (just, RB), (some, DT)...   \n",
       "3   [(what, WP), (are, VBP), (we, PRP), (doing, VBG)]   \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...   \n",
       "5   [(we, PRP), (do, VBP), (not, RB), (really, RB)...   \n",
       "\n",
       "                                    tagged_stan_lemma                     file  \n",
       "0       [(what, WDT), (be, VB), (we, PRP), (do, VBP)]  dyad_10-condition_1.txt  \n",
       "1   [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_10-condition_1.txt  \n",
       "2   [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_10-condition_1.txt  \n",
       "3   [(let, VB), (u, FW), (test, NN), (some, DT), (...  dyad_10-condition_1.txt  \n",
       "4   [(more, RBR), (testing, NN), (but, CC), (we, P...  dyad_10-condition_1.txt  \n",
       "5   [(and, CC), (more, RBR), (test, NN), (this, DT...  dyad_10-condition_1.txt  \n",
       "0   [(let, VB), (u, FW), (ours, JJ), (test, NN), (...  dyad_10-condition_2.txt  \n",
       "1          [(he, PRP), (be, VB), (go, VB), (mad, JJ)]  dyad_10-condition_2.txt  \n",
       "2   [(this, DT), (be, VB), (some, DT), (sample, NN...  dyad_10-condition_2.txt  \n",
       "3       [(what, WDT), (be, VB), (we, PRP), (do, VBP)]  dyad_10-condition_2.txt  \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_10-condition_2.txt  \n",
       "5   [(the, DT), (task, NN), (about, IN), (what, WP...  dyad_10-condition_2.txt  \n",
       "0   [(let, VB), (u, FW), (test, NN), (some, DT), (...  dyad_10-condition_3.txt  \n",
       "1   [(more, RBR), (testing, NN), (but, CC), (we, P...  dyad_10-condition_3.txt  \n",
       "2   [(this, DT), (be, VB), (just, RB), (some, DT),...  dyad_10-condition_3.txt  \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_10-condition_3.txt  \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_10-condition_3.txt  \n",
       "0   [(let, VB), (u, FW), (test, NN), (some, DT), (...  dyad_10-condition_4.txt  \n",
       "1   [(more, RBR), (testing, NN), (but, CC), (we, P...  dyad_10-condition_4.txt  \n",
       "2   [(this, DT), (be, VB), (just, RB), (some, DT),...  dyad_10-condition_4.txt  \n",
       "3       [(what, WDT), (be, VB), (we, PRP), (do, VBP)]  dyad_10-condition_4.txt  \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_10-condition_4.txt  \n",
       "5   [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_10-condition_4.txt  \n",
       "0   [(this, DT), (be, VB), (just, RB), (some, DT),...  dyad_12-condition_1.txt  \n",
       "1   [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_12-condition_1.txt  \n",
       "2   [(let, VB), (u, FW), (test, NN), (some, DT), (...  dyad_12-condition_1.txt  \n",
       "3   [(more, RBR), (testing, NN), (but, CC), (we, P...  dyad_12-condition_1.txt  \n",
       "4             [(and, CC), (more, RBR), (testing, NN)]  dyad_12-condition_1.txt  \n",
       "5       [(what, WDT), (be, VB), (we, PRP), (do, VBP)]  dyad_12-condition_1.txt  \n",
       "6   [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_12-condition_1.txt  \n",
       "..                                                ...                      ...  \n",
       "1             [(and, CC), (more, RBR), (testing, NN)]  dyad_12-condition_2.txt  \n",
       "2       [(what, WDT), (be, VB), (we, PRP), (do, VBP)]  dyad_12-condition_2.txt  \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_12-condition_2.txt  \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_12-condition_2.txt  \n",
       "5   [(let, VB), (u, FW), (test, NN), (some, DT), (...  dyad_12-condition_2.txt  \n",
       "0   [(more, RBR), (testing, NN), (but, CC), (we, P...  dyad_15-condition_2.txt  \n",
       "1             [(and, CC), (more, RBR), (testing, NN)]  dyad_15-condition_2.txt  \n",
       "2       [(what, WDT), (be, VB), (we, PRP), (do, VBP)]  dyad_15-condition_2.txt  \n",
       "3   [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_15-condition_2.txt  \n",
       "4   [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_15-condition_2.txt  \n",
       "5   [(let, VB), (u, FW), (test, NN), (some, DT), (...  dyad_15-condition_2.txt  \n",
       "0   [(this, DT), (be, VB), (just, RB), (some, DT),...  dyad_15-condition_1.txt  \n",
       "1   [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_15-condition_1.txt  \n",
       "2   [(let, VB), (u, FW), (test, NN), (some, DT), (...  dyad_15-condition_1.txt  \n",
       "3   [(more, RBR), (testing, NN), (but, CC), (we, P...  dyad_15-condition_1.txt  \n",
       "4             [(and, CC), (more, RBR), (testing, NN)]  dyad_15-condition_1.txt  \n",
       "5       [(what, WDT), (be, VB), (we, PRP), (do, VBP)]  dyad_15-condition_1.txt  \n",
       "6   [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_15-condition_1.txt  \n",
       "0       [(what, WDT), (be, VB), (we, PRP), (do, VBP)]  dyad_13-condition_1.txt  \n",
       "1   [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_13-condition_1.txt  \n",
       "2   [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_13-condition_1.txt  \n",
       "3   [(let, VB), (u, FW), (test, NN), (some, DT), (...  dyad_13-condition_1.txt  \n",
       "4   [(more, RBR), (testing, NN), (but, CC), (we, P...  dyad_13-condition_1.txt  \n",
       "5   [(and, CC), (more, RBR), (test, NN), (this, DT...  dyad_13-condition_1.txt  \n",
       "0   [(let, VB), (u, FW), (test, NN), (some, DT), (...  dyad_13-condition_2.txt  \n",
       "1   [(more, RBR), (testing, NN), (but, CC), (we, P...  dyad_13-condition_2.txt  \n",
       "2   [(this, DT), (be, VB), (just, RB), (some, DT),...  dyad_13-condition_2.txt  \n",
       "3       [(what, WDT), (be, VB), (we, PRP), (do, VBP)]  dyad_13-condition_2.txt  \n",
       "4   [(we, PRP), (just, RB), (want, VBP), (to, TO),...  dyad_13-condition_2.txt  \n",
       "5   [(we, PRP), (do, VBP), (not, RB), (really, RB)...  dyad_13-condition_2.txt  \n",
       "\n",
       "[61 rows x 9 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specified_filler_list = ['ignore','these','words']\n",
    "loaded_filler_list = list(pd.read_csv(INPUT_PATH+'package_files/fillers.txt',squeeze=True))\n",
    "\n",
    "model_store = PHASE1RUN(\n",
    "          input_files=INPUT_PATH+TRANSCRIPTS,\n",
    "          input_as_directory=True,\n",
    "          save_concatenated_dataframe=True,\n",
    "          output_file_directory=INPUT_PATH+PREPPED_TRANSCRIPTS,\n",
    "          training_dictionary=INPUT_PATH+'package_files/tasa.txt',\n",
    "          minwords=3,\n",
    "          use_filler_list=None,\n",
    "          add_stanford_tags=True,\n",
    "          stanford_pos_path=STANFORD_POS_PATH)\n",
    "model_store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase 2: Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-prepped/dyad_10-condition_1.txt\n",
      "to\n",
      "to\n",
      "and\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-prepped/dyad_10-condition_2.txt\n",
      "to\n",
      "and\n",
      "to\n",
      "to\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-prepped/dyad_10-condition_3.txt\n",
      "and\n",
      "to\n",
      "to\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-prepped/dyad_10-condition_4.txt\n",
      "and\n",
      "to\n",
      "to\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-prepped/dyad_12-condition_1.txt\n",
      "and\n",
      "and\n",
      "to\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-prepped/dyad_12-condition_2.txt\n",
      "and\n",
      "and\n",
      "to\n",
      "to\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-prepped/dyad_15-condition_2.txt\n",
      "and\n",
      "and\n",
      "to\n",
      "to\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-prepped/dyad_15-condition_1.txt\n",
      "and\n",
      "and\n",
      "to\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-prepped/dyad_13-condition_1.txt\n",
      "to\n",
      "to\n",
      "and\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-prepped/dyad_13-condition_2.txt\n",
      "and\n",
      "to\n",
      "to\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    time syntax_penn_tok2 syntax_penn_tok3 syntax_penn_lex2 syntax_penn_lex3  \\\n",
       " 0      0                0                0                0                0   \n",
       " 1      1        0.0870388        0.0953463                0                0   \n",
       " 2      2                0                0                0                0   \n",
       " 3      3                0                0                0                0   \n",
       " 4      4                0                0         0.144338                0   \n",
       " 5      0                0                0         0.204124         0.204124   \n",
       " 6      1                0                0                0                0   \n",
       " 7      2                0                0                0                0   \n",
       " 8      3                0                0                0                0   \n",
       " 9      4         0.227921         0.113961         0.113961                0   \n",
       " 10     0                0                0                0                0   \n",
       " 11     1                0                0         0.136083                0   \n",
       " 12     2                0                0                0                0   \n",
       " 13     3        0.0870388        0.0953463                0                0   \n",
       " 14     0                0                0                0                0   \n",
       " 15     1                0                0                0                0   \n",
       " 16     2                0                0                0                0   \n",
       " 17     3                0                0                0                0   \n",
       " 18     4        0.0870388        0.0953463                0                0   \n",
       " 19     0                0                0                0                0   \n",
       " 20     1                0                0                0                0   \n",
       " 21     2                0                0                0                0   \n",
       " 22     3                0                0                0                0   \n",
       " 23     4                0                0                0                0   \n",
       " 24     5                0                0                0                0   \n",
       " 25     0                0                0                0                0   \n",
       " 26     1                0                0                0                0   \n",
       " 27     2                0                0                0                0   \n",
       " 28     3         0.070014        0.0766965                0                0   \n",
       " 29     4                0                0                0                0   \n",
       " 30     0                0                0                0                0   \n",
       " 31     1                0                0                0                0   \n",
       " 32     2                0                0                0                0   \n",
       " 33     3         0.070014        0.0766965                0                0   \n",
       " 34     4                0                0                0                0   \n",
       " 35     0                0                0                0                0   \n",
       " 36     1                0                0                0                0   \n",
       " 37     2                0                0                0                0   \n",
       " 38     3                0                0                0                0   \n",
       " 39     4                0                0                0                0   \n",
       " 40     5                0                0                0                0   \n",
       " 41     0                0                0                0                0   \n",
       " 42     1        0.0870388        0.0953463                0                0   \n",
       " 43     2                0                0                0                0   \n",
       " 44     3                0                0                0                0   \n",
       " 45     4                0                0         0.144338                0   \n",
       " 46     0                0                0                0                0   \n",
       " 47     1                0                0                0                0   \n",
       " 48     2                0                0                0                0   \n",
       " 49     3                0                0                0                0   \n",
       " 50     4        0.0870388        0.0953463                0                0   \n",
       " \n",
       "    syntax_stan_tok2 syntax_stan_tok3 syntax_stan_lem2 syntax_stan_lem3  \\\n",
       " 0                 0                0                0                0   \n",
       " 1         0.0870388        0.0953463        0.0870388        0.0953463   \n",
       " 2                 0                0                0                0   \n",
       " 3                 0                0                0                0   \n",
       " 4                 0                0             0.25                0   \n",
       " 5                 0                0         0.204124         0.204124   \n",
       " 6                 0                0                0                0   \n",
       " 7                 0                0                0                0   \n",
       " 8                 0                0                0                0   \n",
       " 9          0.113961         0.113961         0.113961         0.113961   \n",
       " 10                0                0                0                0   \n",
       " 11                0                0         0.117851                0   \n",
       " 12                0                0                0                0   \n",
       " 13        0.0870388        0.0953463        0.0870388        0.0953463   \n",
       " 14                0                0                0                0   \n",
       " 15                0                0                0                0   \n",
       " 16                0                0                0                0   \n",
       " 17                0                0                0                0   \n",
       " 18        0.0870388        0.0953463        0.0870388        0.0953463   \n",
       " 19                0                0         0.124035                0   \n",
       " 20                0                0                0                0   \n",
       " 21                0                0                0                0   \n",
       " 22                0                0                0                0   \n",
       " 23                0                0                0                0   \n",
       " 24                0                0                0                0   \n",
       " 25                0                0                0                0   \n",
       " 26                0                0                0                0   \n",
       " 27                0                0                0                0   \n",
       " 28         0.070014        0.0766965         0.140028        0.0766965   \n",
       " 29                0                0                0                0   \n",
       " 30                0                0                0                0   \n",
       " 31                0                0                0                0   \n",
       " 32                0                0                0                0   \n",
       " 33         0.070014        0.0766965         0.140028        0.0766965   \n",
       " 34                0                0                0                0   \n",
       " 35                0                0         0.124035                0   \n",
       " 36                0                0                0                0   \n",
       " 37                0                0                0                0   \n",
       " 38                0                0                0                0   \n",
       " 39                0                0                0                0   \n",
       " 40                0                0                0                0   \n",
       " 41                0                0                0                0   \n",
       " 42        0.0870388        0.0953463        0.0870388        0.0953463   \n",
       " 43                0                0                0                0   \n",
       " 44                0                0                0                0   \n",
       " 45                0                0             0.25                0   \n",
       " 46                0                0                0                0   \n",
       " 47                0                0                0                0   \n",
       " 48                0                0                0                0   \n",
       " 49                0                0                0                0   \n",
       " 50        0.0870388        0.0953463        0.0870388        0.0953463   \n",
       " \n",
       "    lexical_tok2 lexical_tok3 lexical_lem2 lexical_lem3 cosine_semanticL  \\\n",
       " 0             0            0            0            0         0.818851   \n",
       " 1     0.0870388            0    0.0870388            0         0.847663   \n",
       " 2             0            0            0            0          0.58553   \n",
       " 3             0            0            0            0         0.682008   \n",
       " 4      0.144338            0            0            0         0.716008   \n",
       " 5             0            0     0.160128            0         0.628851   \n",
       " 6             0            0            0            0         0.496539   \n",
       " 7             0            0            0            0         0.551167   \n",
       " 8             0            0            0            0         0.818851   \n",
       " 9      0.102062            0     0.102062            0          0.80419   \n",
       " 10     0.154303            0     0.154303            0          0.80866   \n",
       " 11            0            0            0            0         0.600937   \n",
       " 12            0            0            0            0         0.792182   \n",
       " 13    0.0870388            0    0.0870388            0         0.847663   \n",
       " 14     0.154303            0     0.154303            0          0.80866   \n",
       " 15            0            0            0            0         0.564774   \n",
       " 16            0            0            0            0         0.620693   \n",
       " 17            0            0            0            0         0.818851   \n",
       " 18    0.0870388            0    0.0870388            0         0.847663   \n",
       " 19            0            0            0            0         0.607045   \n",
       " 20            0            0            0            0          0.58553   \n",
       " 21            0            0            0            0         0.682008   \n",
       " 22     0.288675            0     0.288675            0         0.820712   \n",
       " 23            0            0            0            0         0.279527   \n",
       " 24            0            0            0            0         0.818851   \n",
       " 25     0.288675            0     0.288675            0         0.820712   \n",
       " 26            0            0            0            0         0.279527   \n",
       " 27            0            0            0            0         0.810081   \n",
       " 28    0.0710669            0    0.0710669            0          0.82339   \n",
       " 29            0            0            0            0          0.58553   \n",
       " 30     0.288675            0     0.288675            0         0.820712   \n",
       " 31            0            0            0            0         0.279527   \n",
       " 32            0            0            0            0         0.810081   \n",
       " 33    0.0710669            0    0.0710669            0          0.82339   \n",
       " 34            0            0            0            0          0.58553   \n",
       " 35            0            0            0            0         0.607045   \n",
       " 36            0            0            0            0          0.58553   \n",
       " 37            0            0            0            0         0.682008   \n",
       " 38     0.288675            0     0.288675            0         0.820712   \n",
       " 39            0            0            0            0         0.279527   \n",
       " 40            0            0            0            0         0.818851   \n",
       " 41            0            0            0            0         0.818851   \n",
       " 42    0.0870388            0    0.0870388            0         0.847663   \n",
       " 43            0            0            0            0          0.58553   \n",
       " 44            0            0            0            0         0.682008   \n",
       " 45     0.144338            0            0            0         0.716008   \n",
       " 46     0.154303            0     0.154303            0          0.80866   \n",
       " 47            0            0            0            0         0.564774   \n",
       " 48            0            0            0            0         0.620693   \n",
       " 49            0            0            0            0         0.818851   \n",
       " 50    0.0870388            0    0.0870388            0         0.847663   \n",
       " \n",
       "    partner_direction           condition_info  \n",
       " 0                1>2  dyad_10-condition_1.txt  \n",
       " 1                2>1  dyad_10-condition_1.txt  \n",
       " 2                1>2  dyad_10-condition_1.txt  \n",
       " 3                2>1  dyad_10-condition_1.txt  \n",
       " 4                1>2  dyad_10-condition_1.txt  \n",
       " 5                2>1  dyad_10-condition_2.txt  \n",
       " 6                1>2  dyad_10-condition_2.txt  \n",
       " 7                2>1  dyad_10-condition_2.txt  \n",
       " 8                1>2  dyad_10-condition_2.txt  \n",
       " 9                2>1  dyad_10-condition_2.txt  \n",
       " 10               2>1  dyad_10-condition_3.txt  \n",
       " 11               1>2  dyad_10-condition_3.txt  \n",
       " 12               2>1  dyad_10-condition_3.txt  \n",
       " 13               1>2  dyad_10-condition_3.txt  \n",
       " 14               2>1  dyad_10-condition_4.txt  \n",
       " 15               1>2  dyad_10-condition_4.txt  \n",
       " 16               2>1  dyad_10-condition_4.txt  \n",
       " 17               1>2  dyad_10-condition_4.txt  \n",
       " 18               2>1  dyad_10-condition_4.txt  \n",
       " 19               2>1  dyad_12-condition_1.txt  \n",
       " 20               1>2  dyad_12-condition_1.txt  \n",
       " 21               2>1  dyad_12-condition_1.txt  \n",
       " 22               1>2  dyad_12-condition_1.txt  \n",
       " 23               2>1  dyad_12-condition_1.txt  \n",
       " 24               1>2  dyad_12-condition_1.txt  \n",
       " 25               1>2  dyad_12-condition_2.txt  \n",
       " 26               2>1  dyad_12-condition_2.txt  \n",
       " 27               1>2  dyad_12-condition_2.txt  \n",
       " 28               2>1  dyad_12-condition_2.txt  \n",
       " 29               1>2  dyad_12-condition_2.txt  \n",
       " 30               1>2  dyad_15-condition_2.txt  \n",
       " 31               2>1  dyad_15-condition_2.txt  \n",
       " 32               1>2  dyad_15-condition_2.txt  \n",
       " 33               2>1  dyad_15-condition_2.txt  \n",
       " 34               1>2  dyad_15-condition_2.txt  \n",
       " 35               2>1  dyad_15-condition_1.txt  \n",
       " 36               1>2  dyad_15-condition_1.txt  \n",
       " 37               2>1  dyad_15-condition_1.txt  \n",
       " 38               1>2  dyad_15-condition_1.txt  \n",
       " 39               2>1  dyad_15-condition_1.txt  \n",
       " 40               1>2  dyad_15-condition_1.txt  \n",
       " 41               1>2  dyad_13-condition_1.txt  \n",
       " 42               2>1  dyad_13-condition_1.txt  \n",
       " 43               1>2  dyad_13-condition_1.txt  \n",
       " 44               2>1  dyad_13-condition_1.txt  \n",
       " 45               1>2  dyad_13-condition_1.txt  \n",
       " 46               2>1  dyad_13-condition_2.txt  \n",
       " 47               1>2  dyad_13-condition_2.txt  \n",
       " 48               2>1  dyad_13-condition_2.txt  \n",
       " 49               1>2  dyad_13-condition_2.txt  \n",
       " 50               2>1  dyad_13-condition_2.txt  ,\n",
       "    syntax_penn_tok2  syntax_penn_tok3  syntax_penn_lex2  syntax_penn_lex3  \\\n",
       " 0          0.035921          0.041996          0.035921          0.041996   \n",
       " 1          0.179787          0.136931          0.179787          0.136931   \n",
       " 2          0.086066          0.044544          0.086066          0.044544   \n",
       " 3          0.037139          0.041996          0.037139          0.041996   \n",
       " 4          0.037139          0.041996          0.037139          0.041996   \n",
       " 5          0.040000          0.043644          0.040000          0.043644   \n",
       " 6          0.040000          0.043644          0.040000          0.043644   \n",
       " 7          0.037139          0.041996          0.037139          0.041996   \n",
       " 8          0.035921          0.041996          0.035921          0.041996   \n",
       " 9          0.037139          0.041996          0.037139          0.041996   \n",
       " \n",
       "    syntax_stan_tok2  syntax_stan_tok3  syntax_stan_lem2  syntax_stan_lem3  \\\n",
       " 0          0.041667          0.043644          0.150970          0.043644   \n",
       " 1          0.169791          0.091287          0.133631          0.171499   \n",
       " 2          0.081650          0.044544          0.077850          0.044544   \n",
       " 3          0.038490          0.043644          0.075485          0.043644   \n",
       " 4          0.041667          0.043644          0.085126          0.043644   \n",
       " 5          0.041667          0.043644          0.081786          0.043644   \n",
       " 6          0.041667          0.043644          0.081786          0.043644   \n",
       " 7          0.041667          0.043644          0.085126          0.043644   \n",
       " 8          0.041667          0.043644          0.150970          0.043644   \n",
       " 9          0.038490          0.043644          0.075485          0.043644   \n",
       " \n",
       "    lexical_tok2  lexical_tok3  lexical_lem2  lexical_lem3  \\\n",
       " 0      0.083624           0.0      0.040032           0.0   \n",
       " 1      0.087039           0.0      0.130558           0.0   \n",
       " 2      0.085203           0.0      0.082409           0.0   \n",
       " 3      0.083624           0.0      0.080064           0.0   \n",
       " 4      0.083624           0.0      0.080064           0.0   \n",
       " 5      0.083624           0.0      0.080064           0.0   \n",
       " 6      0.083624           0.0      0.080064           0.0   \n",
       " 7      0.083624           0.0      0.080064           0.0   \n",
       " 8      0.083624           0.0      0.040032           0.0   \n",
       " 9      0.083624           0.0      0.080064           0.0   \n",
       " \n",
       "             condition_info  \n",
       " 0  dyad_10-condition_1.txt  \n",
       " 1  dyad_10-condition_2.txt  \n",
       " 2  dyad_10-condition_3.txt  \n",
       " 3  dyad_10-condition_4.txt  \n",
       " 4  dyad_12-condition_1.txt  \n",
       " 5  dyad_12-condition_2.txt  \n",
       " 6  dyad_15-condition_2.txt  \n",
       " 7  dyad_15-condition_1.txt  \n",
       " 8  dyad_13-condition_1.txt  \n",
       " 9  dyad_13-condition_2.txt  )"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[turn_real,convo_real]= PHASE2RUN_REAL(\n",
    "                        input_files = INPUT_PATH+PREPPED_TRANSCRIPTS,\n",
    "                        input_as_directory=True,\n",
    "                        output_file_directory = INPUT_PATH+ANALYSIS_READY,\n",
    "                        delay=1,\n",
    "                        maxngram=3,\n",
    "                        ignore_duplicates=True,\n",
    "                        add_stanford_tags=True,       \n",
    "                        use_pretrained_vectors=True,\n",
    "                        pretrained_input_file=INPUT_PATH+'package_files/GoogleNews-vectors-negative300.bin',\n",
    "                        semantic_model_input_file=INPUT_PATH+'align_concatenated_dataframe.txt', \n",
    "                        high_sd_cutoff=3,\n",
    "                        low_n_cutoff=1)\n",
    "turn_real,convo_real "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase 2: Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_15A-dyad_13B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_13A-dyad_10B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_15A-dyad_13B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_13A-dyad_10B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_12A-dyad_13B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_10A-dyad_12B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_12A-dyad_13B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_10A-dyad_12B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_15A-dyad_12B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_15A-dyad_12B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_10A-dyad_13B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_13A-dyad_15B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_10A-dyad_13B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_13A-dyad_15B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_13A-dyad_12B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_12A-dyad_15B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_13A-dyad_12B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_12A-dyad_15B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_10A-dyad_15B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_15A-dyad_10B-condition_2.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_10A-dyad_15B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_15A-dyad_10B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_12A-dyad_10B-condition_1.txt\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/toy_data-surrogate/surrogate_run-1513997423.34/SurrogatePair-dyad_12A-dyad_10B-condition_2.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     time syntax_penn_tok2 syntax_penn_tok3 syntax_penn_lex2 syntax_penn_lex3  \\\n",
       " 0       0        0.0870388        0.0953463                0                0   \n",
       " 1       1         0.117851                0                0                0   \n",
       " 2       2                0                0                0                0   \n",
       " 3       3                0                0                0                0   \n",
       " 4       4                0                0                0                0   \n",
       " 5       0         0.113228                0                0                0   \n",
       " 6       1                0                0                0                0   \n",
       " 7       2                0                0                0                0   \n",
       " 8       3                0                0                0                0   \n",
       " 9       4        0.0870388        0.0953463                0                0   \n",
       " 10      0                0                0                0                0   \n",
       " 11      1                0                0                0                0   \n",
       " 12      2                0                0                0                0   \n",
       " 13      3                0                0                0                0   \n",
       " 14      4        0.0870388        0.0953463                0                0   \n",
       " 15      0                0                0                0                0   \n",
       " 16      1        0.0870388        0.0953463                0                0   \n",
       " 17      2                0                0                0                0   \n",
       " 18      3                0                0                0                0   \n",
       " 19      4                0                0         0.144338                0   \n",
       " 20      0                0                0                0                0   \n",
       " 21      1                0                0                0                0   \n",
       " 22      2                0                0                0                0   \n",
       " 23      3                0                0                0                0   \n",
       " 24      4        0.0870388        0.0953463                0                0   \n",
       " 25      0                0                0                0                0   \n",
       " 26      1                0                0                0                0   \n",
       " 27      2                0                0                0                0   \n",
       " 28      3          0.18334        0.0916698        0.0916698                0   \n",
       " 29      4                0                0                0                0   \n",
       " ..    ...              ...              ...              ...              ...   \n",
       " 90      0                0                0                0                0   \n",
       " 91      1                0                0                0                0   \n",
       " 92      2                0                0                0                0   \n",
       " 93      3          0.18334        0.0916698        0.0916698                0   \n",
       " 94      4                0                0                0                0   \n",
       " 95      0         0.113228                0                0                0   \n",
       " 96      1                0                0                0                0   \n",
       " 97      2                0                0                0                0   \n",
       " 98      3                0                0                0                0   \n",
       " 99      4        0.0870388        0.0953463                0                0   \n",
       " 100     0                0                0                0                0   \n",
       " 101     1                0                0                0                0   \n",
       " 102     2                0                0                0                0   \n",
       " 103     3                0                0                0                0   \n",
       " 104     4                0                0                0                0   \n",
       " 105     0        0.0870388        0.0953463                0                0   \n",
       " 106     1         0.117851                0                0                0   \n",
       " 107     2                0                0                0                0   \n",
       " 108     3                0                0                0                0   \n",
       " 109     4                0                0                0                0   \n",
       " 110     0        0.0870388        0.0953463                0                0   \n",
       " 111     1         0.117851                0                0                0   \n",
       " 112     2                0                0                0                0   \n",
       " 113     3                0                0                0                0   \n",
       " 114     4                0                0                0                0   \n",
       " 115     0         0.113228                0                0                0   \n",
       " 116     1                0                0                0                0   \n",
       " 117     2                0                0                0                0   \n",
       " 118     3                0                0                0                0   \n",
       " 119     4        0.0870388        0.0953463                0                0   \n",
       " \n",
       "     lexical_tok2 lexical_tok3 lexical_lem2 lexical_lem3 cosine_semanticL  \\\n",
       " 0      0.0870388            0    0.0870388            0         0.523374   \n",
       " 1              0            0            0            0         0.158919   \n",
       " 2              0            0            0            0         0.362531   \n",
       " 3              0            0            0            0       0.00998015   \n",
       " 4              0            0            0            0         0.300737   \n",
       " 5       0.113228            0     0.113228            0          0.49971   \n",
       " 6              0            0            0            0         0.331973   \n",
       " 7              0            0            0            0         0.357439   \n",
       " 8              0            0            0            0         0.357512   \n",
       " 9      0.0870388            0    0.0870388            0         0.523374   \n",
       " 10      0.154303            0     0.154303            0         0.533491   \n",
       " 11             0            0            0            0        0.0134148   \n",
       " 12             0            0            0            0         0.364164   \n",
       " 13             0            0            0            0         0.430049   \n",
       " 14     0.0870388            0    0.0870388            0         0.523374   \n",
       " 15             0            0            0            0          0.33266   \n",
       " 16     0.0870388            0    0.0870388            0         0.523374   \n",
       " 17             0            0            0            0         0.240605   \n",
       " 18             0            0            0            0         0.362531   \n",
       " 19      0.144338            0            0            0         0.367648   \n",
       " 20      0.154303            0     0.154303            0         0.533491   \n",
       " 21             0            0            0            0        0.0134148   \n",
       " 22             0            0            0            0         0.364164   \n",
       " 23             0            0            0            0         0.430049   \n",
       " 24     0.0870388            0    0.0870388            0         0.523374   \n",
       " 25             0            0            0            0        0.0480764   \n",
       " 26             0            0            0            0        0.0125286   \n",
       " 27             0            0            0            0         0.403722   \n",
       " 28     0.0833333            0    0.0833333            0         0.563045   \n",
       " 29             0            0            0            0         0.294146   \n",
       " ..           ...          ...          ...          ...              ...   \n",
       " 90             0            0            0            0        0.0480764   \n",
       " 91             0            0            0            0        0.0125286   \n",
       " 92             0            0            0            0         0.403722   \n",
       " 93     0.0833333            0    0.0833333            0         0.563045   \n",
       " 94             0            0            0            0         0.294146   \n",
       " 95      0.113228            0     0.113228            0          0.49971   \n",
       " 96             0            0            0            0         0.331973   \n",
       " 97             0            0            0            0         0.357439   \n",
       " 98             0            0            0            0         0.357512   \n",
       " 99     0.0870388            0    0.0870388            0         0.523374   \n",
       " 100            0            0            0            0         0.364164   \n",
       " 101            0            0            0            0         0.430049   \n",
       " 102            0            0            0            0         0.240605   \n",
       " 103            0            0            0            0         0.362531   \n",
       " 104     0.288675            0     0.288675            0         0.543643   \n",
       " 105    0.0870388            0    0.0870388            0         0.523374   \n",
       " 106            0            0            0            0         0.158919   \n",
       " 107            0            0            0            0         0.362531   \n",
       " 108            0            0            0            0       0.00998015   \n",
       " 109            0            0            0            0         0.300737   \n",
       " 110    0.0870388            0    0.0870388            0         0.523374   \n",
       " 111            0            0            0            0         0.158919   \n",
       " 112            0            0            0            0         0.362531   \n",
       " 113            0            0            0            0       0.00998015   \n",
       " 114            0            0            0            0         0.300737   \n",
       " 115     0.113228            0     0.113228            0          0.49971   \n",
       " 116            0            0            0            0         0.331973   \n",
       " 117            0            0            0            0         0.357439   \n",
       " 118            0            0            0            0         0.357512   \n",
       " 119    0.0870388            0    0.0870388            0         0.523374   \n",
       " \n",
       "     partner_direction               condition_info  \n",
       " 0                 1>2  condition_1-dyad_15-dyad_13  \n",
       " 1                 2>1  condition_1-dyad_15-dyad_13  \n",
       " 2                 1>2  condition_1-dyad_15-dyad_13  \n",
       " 3                 2>1  condition_1-dyad_15-dyad_13  \n",
       " 4                 1>2  condition_1-dyad_15-dyad_13  \n",
       " 5                 1>2  condition_2-dyad_10-dyad_13  \n",
       " 6                 2>1  condition_2-dyad_10-dyad_13  \n",
       " 7                 1>2  condition_2-dyad_10-dyad_13  \n",
       " 8                 2>1  condition_2-dyad_10-dyad_13  \n",
       " 9                 1>2  condition_2-dyad_10-dyad_13  \n",
       " 10                1>2  condition_2-dyad_15-dyad_13  \n",
       " 11                2>1  condition_2-dyad_15-dyad_13  \n",
       " 12                1>2  condition_2-dyad_15-dyad_13  \n",
       " 13                2>1  condition_2-dyad_15-dyad_13  \n",
       " 14                1>2  condition_2-dyad_15-dyad_13  \n",
       " 15                1>2  condition_1-dyad_10-dyad_13  \n",
       " 16                2>1  condition_1-dyad_10-dyad_13  \n",
       " 17                1>2  condition_1-dyad_10-dyad_13  \n",
       " 18                2>1  condition_1-dyad_10-dyad_13  \n",
       " 19                1>2  condition_1-dyad_10-dyad_13  \n",
       " 20                1>2  condition_2-dyad_12-dyad_13  \n",
       " 21                2>1  condition_2-dyad_12-dyad_13  \n",
       " 22                1>2  condition_2-dyad_12-dyad_13  \n",
       " 23                2>1  condition_2-dyad_12-dyad_13  \n",
       " 24                1>2  condition_2-dyad_12-dyad_13  \n",
       " 25                1>2  condition_2-dyad_10-dyad_12  \n",
       " 26                2>1  condition_2-dyad_10-dyad_12  \n",
       " 27                1>2  condition_2-dyad_10-dyad_12  \n",
       " 28                2>1  condition_2-dyad_10-dyad_12  \n",
       " 29                1>2  condition_2-dyad_10-dyad_12  \n",
       " ..                ...                          ...  \n",
       " 90                1>2  condition_2-dyad_10-dyad_15  \n",
       " 91                2>1  condition_2-dyad_10-dyad_15  \n",
       " 92                1>2  condition_2-dyad_10-dyad_15  \n",
       " 93                2>1  condition_2-dyad_10-dyad_15  \n",
       " 94                1>2  condition_2-dyad_10-dyad_15  \n",
       " 95                1>2  condition_2-dyad_10-dyad_15  \n",
       " 96                2>1  condition_2-dyad_10-dyad_15  \n",
       " 97                1>2  condition_2-dyad_10-dyad_15  \n",
       " 98                2>1  condition_2-dyad_10-dyad_15  \n",
       " 99                1>2  condition_2-dyad_10-dyad_15  \n",
       " 100               1>2  condition_1-dyad_10-dyad_15  \n",
       " 101               2>1  condition_1-dyad_10-dyad_15  \n",
       " 102               1>2  condition_1-dyad_10-dyad_15  \n",
       " 103               2>1  condition_1-dyad_10-dyad_15  \n",
       " 104               1>2  condition_1-dyad_10-dyad_15  \n",
       " 105               1>2  condition_1-dyad_10-dyad_15  \n",
       " 106               2>1  condition_1-dyad_10-dyad_15  \n",
       " 107               1>2  condition_1-dyad_10-dyad_15  \n",
       " 108               2>1  condition_1-dyad_10-dyad_15  \n",
       " 109               1>2  condition_1-dyad_10-dyad_15  \n",
       " 110               1>2  condition_1-dyad_10-dyad_12  \n",
       " 111               2>1  condition_1-dyad_10-dyad_12  \n",
       " 112               1>2  condition_1-dyad_10-dyad_12  \n",
       " 113               2>1  condition_1-dyad_10-dyad_12  \n",
       " 114               1>2  condition_1-dyad_10-dyad_12  \n",
       " 115               1>2  condition_2-dyad_10-dyad_12  \n",
       " 116               2>1  condition_2-dyad_10-dyad_12  \n",
       " 117               1>2  condition_2-dyad_10-dyad_12  \n",
       " 118               2>1  condition_2-dyad_10-dyad_12  \n",
       " 119               1>2  condition_2-dyad_10-dyad_12  \n",
       " \n",
       " [120 rows x 12 columns],\n",
       "     syntax_penn_tok2  syntax_penn_tok3  syntax_penn_lex2  syntax_penn_lex3  \\\n",
       " 0           0.035921          0.041996          0.035921          0.041996   \n",
       " 1           0.069631          0.077152          0.069631          0.077152   \n",
       " 2           0.037139          0.041996          0.037139          0.041996   \n",
       " 3           0.035921          0.041996          0.035921          0.041996   \n",
       " 4           0.037139          0.041996          0.037139          0.041996   \n",
       " 5           0.154919          0.051640          0.154919          0.051640   \n",
       " 6           0.035921          0.041996          0.035921          0.041996   \n",
       " 7           0.000000          0.000000          0.000000          0.000000   \n",
       " 8           0.000000          0.000000          0.000000          0.000000   \n",
       " 9           0.040000          0.043644          0.040000          0.043644   \n",
       " 10          0.191785          0.099381          0.191785          0.099381   \n",
       " 11          0.000000          0.000000          0.000000          0.000000   \n",
       " 12          0.035921          0.041996          0.035921          0.041996   \n",
       " 13          0.040000          0.043644          0.040000          0.043644   \n",
       " 14          0.000000          0.000000          0.000000          0.000000   \n",
       " 15          0.040000          0.043644          0.040000          0.043644   \n",
       " 16          0.040000          0.043644          0.040000          0.043644   \n",
       " 17          0.000000          0.000000          0.000000          0.000000   \n",
       " 18          0.154919          0.051640          0.154919          0.051640   \n",
       " 19          0.069631          0.077152          0.069631          0.077152   \n",
       " 20          0.000000          0.000000          0.000000          0.000000   \n",
       " 21          0.035921          0.041996          0.035921          0.041996   \n",
       " 22          0.035921          0.041996          0.035921          0.041996   \n",
       " 23          0.069631          0.077152          0.069631          0.077152   \n",
       " \n",
       "     lexical_tok2  lexical_tok3  lexical_lem2  lexical_lem3  \\\n",
       " 0       0.083624           0.0      0.040032           0.0   \n",
       " 1       0.111340           0.0      0.106600           0.0   \n",
       " 2       0.083624           0.0      0.080064           0.0   \n",
       " 3       0.083624           0.0      0.040032           0.0   \n",
       " 4       0.083624           0.0      0.080064           0.0   \n",
       " 5       0.049029           0.0      0.049029           0.0   \n",
       " 6       0.083624           0.0      0.040032           0.0   \n",
       " 7       0.059131           0.0      0.056614           0.0   \n",
       " 8       0.059131           0.0      0.056614           0.0   \n",
       " 9       0.083624           0.0      0.080064           0.0   \n",
       " 10      0.049029           0.0      0.049029           0.0   \n",
       " 11      0.059131           0.0      0.056614           0.0   \n",
       " 12      0.083624           0.0      0.040032           0.0   \n",
       " 13      0.083624           0.0      0.080064           0.0   \n",
       " 14      0.059131           0.0      0.056614           0.0   \n",
       " 15      0.083624           0.0      0.080064           0.0   \n",
       " 16      0.083624           0.0      0.080064           0.0   \n",
       " 17      0.059131           0.0      0.056614           0.0   \n",
       " 18      0.049029           0.0      0.049029           0.0   \n",
       " 19      0.111340           0.0      0.106600           0.0   \n",
       " 20      0.059131           0.0      0.056614           0.0   \n",
       " 21      0.083624           0.0      0.040032           0.0   \n",
       " 22      0.083624           0.0      0.040032           0.0   \n",
       " 23      0.111340           0.0      0.106600           0.0   \n",
       " \n",
       "                  condition_info  \n",
       " 0   condition_1-dyad_15-dyad_13  \n",
       " 1   condition_2-dyad_10-dyad_13  \n",
       " 2   condition_2-dyad_15-dyad_13  \n",
       " 3   condition_1-dyad_10-dyad_13  \n",
       " 4   condition_2-dyad_12-dyad_13  \n",
       " 5   condition_2-dyad_10-dyad_12  \n",
       " 6   condition_1-dyad_12-dyad_13  \n",
       " 7   condition_1-dyad_10-dyad_12  \n",
       " 8   condition_1-dyad_12-dyad_15  \n",
       " 9   condition_2-dyad_12-dyad_15  \n",
       " 10  condition_2-dyad_10-dyad_13  \n",
       " 11  condition_1-dyad_15-dyad_13  \n",
       " 12  condition_1-dyad_10-dyad_13  \n",
       " 13  condition_2-dyad_15-dyad_13  \n",
       " 14  condition_1-dyad_12-dyad_13  \n",
       " 15  condition_2-dyad_12-dyad_15  \n",
       " 16  condition_2-dyad_12-dyad_13  \n",
       " 17  condition_1-dyad_12-dyad_15  \n",
       " 18  condition_2-dyad_10-dyad_15  \n",
       " 19  condition_2-dyad_10-dyad_15  \n",
       " 20  condition_1-dyad_10-dyad_15  \n",
       " 21  condition_1-dyad_10-dyad_15  \n",
       " 22  condition_1-dyad_10-dyad_12  \n",
       " 23  condition_2-dyad_10-dyad_12  )"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[turn_surrogate,convo_surrogate] = PHASE2RUN_SURROGATE(\n",
    "                             input_files = INPUT_PATH+PREPPED_TRANSCRIPTS, \n",
    "                             surrogate_file_directory= INPUT_PATH+SURROGATE_TRANSCRIPTS,\n",
    "                             input_as_directory=True,\n",
    "                             output_file_directory=INPUT_PATH+ANALYSIS_READY,\n",
    "                             semantic_model_input_file=INPUT_PATH+'align_concatenated_dataframe.txt',\n",
    "                             pretrained_input_file=INPUT_PATH+'package_files/GoogleNews-vectors-negative300.bin',\n",
    "                             high_sd_cutoff=3,\n",
    "                             low_n_cutoff=1,\n",
    "                             id_separator = '\\-',\n",
    "                             condition_label='cond',\n",
    "                             dyad_label='dyad',\n",
    "                             all_surrogates=True,\n",
    "                             keep_original_turn_order=True,\n",
    "                             delay=1,\n",
    "                             maxngram=3,\n",
    "                             use_pretrained_vectors=False,\n",
    "                             ignore_duplicates=True,\n",
    "                             add_stanford_tags=False)\n",
    "turn_surrogate,convo_surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Speed calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Phase 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2real - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Phase 2 real time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2surrogate - start_phase2real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Phase 2 surrogate time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end - start_phase2surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All 3 phases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Printouts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "turn_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convo_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "turn_surrogate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convo_surrogate.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
