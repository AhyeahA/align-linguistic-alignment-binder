{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes with Python 3 upgrade: \n",
    "\n",
    "* Initially ran 2to3 function to deal with most issues \n",
    "* No major issues were found\n",
    "* Corrected Ludvig's issue where the convo alignment scores for syntax_penn_tok2 and syntax_penn_lem2 were identical because the tagged_token were used for both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP\n",
    "## Import align packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import os,re,math,csv,string,random,logging,glob,itertools,operator, sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_pos(sequence1,sequence2,ngramsize=2,\n",
    "                   ignore_duplicates=True):\n",
    "    \"\"\"\n",
    "    Remove mimicked lexical sequences from two interlocutors'\n",
    "    sequences and return a dictionary of counts of ngrams\n",
    "    of the desired size for each sequence.\n",
    "\n",
    "    By default, consider bigrams. If desired, this may be\n",
    "    changed by setting `ngramsize` to the appropriate\n",
    "    value.\n",
    "\n",
    "    By default, ignore duplicate lexical n-grams when\n",
    "    processing these sequences. If desired, this may\n",
    "    be changed with `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # remove duplicates and recreate sequences\n",
    "    sequence1 = set(ngrams(sequence1,ngramsize))\n",
    "    sequence2 = set(ngrams(sequence2,ngramsize))\n",
    "\n",
    "    # if desired, remove duplicates from sequences\n",
    "    if ignore_duplicates:\n",
    "        new_sequence1 = [tuple([''.join(pair[1]) for pair in tup]) for tup in list(sequence1 - sequence2)]\n",
    "        new_sequence2 = [tuple([''.join(pair[1]) for pair in tup]) for tup in list(sequence2 - sequence1)]\n",
    "    else:\n",
    "        new_sequence1 = [tuple([''.join(pair[1]) for pair in tup]) for tup in sequence1]\n",
    "        new_sequence2 = [tuple([''.join(pair[1]) for pair in tup]) for tup in sequence2]\n",
    "\n",
    "    # return counters\n",
    "    return Counter(new_sequence1), Counter(new_sequence2)\n",
    "\n",
    "\n",
    "def ngram_lexical(sequence1,sequence2,ngramsize=2):\n",
    "    \"\"\"\n",
    "    Create ngrams of the desired size for each of two\n",
    "    interlocutors' sequences and return a dictionary\n",
    "    of counts of ngrams for each sequence.\n",
    "\n",
    "    By default, consider bigrams. If desired, this may be\n",
    "    changed by setting `ngramsize` to the appropriate\n",
    "    value.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate ngrams\n",
    "    sequence1 = list(ngrams(sequence1,ngramsize))\n",
    "    sequence2 = list(ngrams(sequence2,ngramsize))\n",
    "\n",
    "    # join for counters\n",
    "    new_sequence1 = [' '.join(pair) for pair in sequence1]\n",
    "    new_sequence2 = [' '.join(pair) for pair in sequence2]\n",
    "\n",
    "    # return counters\n",
    "    return Counter(new_sequence1), Counter(new_sequence2)\n",
    "\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Derive cosine similarity metric, standard measure.\n",
    "    Adapted from <https://stackoverflow.com/a/33129724>.\n",
    "    \"\"\"\n",
    "\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x]**2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def build_composite_semantic_vector(lemma_seq,vocablist,highDimModel):\n",
    "    \"\"\"\n",
    "    Function for producing vocablist and model is called in the main loop\n",
    "    \"\"\"\n",
    "\n",
    "    ## filter out words in corpus that do not appear in vocablist (either too rare or too frequent)\n",
    "    filter_lemma_seq = [word for word in lemma_seq if word in vocablist]\n",
    "    ## build composite vector\n",
    "    getComposite = [0] * len(highDimModel[vocablist[1]])\n",
    "    for w1 in filter_lemma_seq:\n",
    "        if w1 in highDimModel.vocab:\n",
    "            semvector = highDimModel[w1]\n",
    "            getComposite = getComposite + semvector\n",
    "    return getComposite\n",
    "\n",
    "\n",
    "def BuildSemanticModel(semantic_model_input_file,\n",
    "                        pretrained_input_file,\n",
    "                        use_pretrained_vectors=True,\n",
    "                        high_sd_cutoff=3,\n",
    "                        low_n_cutoff=1):\n",
    "\n",
    "    \"\"\"\n",
    "    Given an input file produced by the ALIGN Phase 1 functions,\n",
    "    build a semantic model from all transcripts in all conversations\n",
    "    in target corpus after removing high- and low-frequency words.\n",
    "    High-frequency words are determined by a user-defined number of\n",
    "    SDs over the mean (by default, `high_sd_cutoff=3`). Low-frequency\n",
    "    words must appear over a specified number of raw occurrences\n",
    "    (by default, `low_n_cutoff=1`).\n",
    "\n",
    "    Frequency cutoffs can be removed by `high_sd_cutoff=None` and/or\n",
    "    `low_n_cutoff=0`.\n",
    "    \"\"\"\n",
    "\n",
    "    # build vocabulary list from transcripts\n",
    "    data1 = pd.read_csv(semantic_model_input_file, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    # get frequency count of all included words\n",
    "    all_sentences = [re.sub('[^\\w\\s]+','',str(row)).split(' ') for row in list(data1['lemma'])]\n",
    "    all_words = list([a for b in all_sentences for a in b])\n",
    "    frequency = defaultdict(int)\n",
    "    for word in all_words:\n",
    "        frequency[word] += 1\n",
    "\n",
    "    # remove words that only occur more frequently than our cutoff (defined in occurrences)\n",
    "    frequency = {word: freq for word, freq in frequency.items() if freq > low_n_cutoff}\n",
    "\n",
    "    # if desired, remove high-frequency words (over user-defined SDs above mean)\n",
    "    if high_sd_cutoff is None:\n",
    "        contentWords = [word for word in list(frequency.keys())]\n",
    "    else:\n",
    "        getOut = np.mean(list(frequency.values()))+(np.std(list(frequency.values()))*(high_sd_cutoff))\n",
    "        contentWords = list({word: freq for word, freq in frequency.items() if freq < getOut}.keys())\n",
    "\n",
    "    # decide whether to build semantic model from scratch or load in pretrained vectors\n",
    "    if not use_pretrained_vectors:\n",
    "        keepSentences = [[word for word in row if word in contentWords] for row in all_sentences]\n",
    "        semantic_model = word2vec.Word2Vec(all_sentences, min_count=low_n_cutoff)\n",
    "    else:\n",
    "        if pretrained_input_file is None:\n",
    "            raise ValueError('Error! Specify path to pretrained vector file using the `pretrained_input_file` argument.')\n",
    "        else:\n",
    "            semantic_model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_input_file, binary=True)\n",
    "\n",
    "    # return all the content words and the trained word vectors\n",
    "    return contentWords, semantic_model.wv\n",
    "\n",
    "\n",
    "def LexicalPOSAlignment(tok1,lem1,penn_tok1,penn_lem1,\n",
    "                             tok2,lem2,penn_tok2,penn_lem2,\n",
    "                             stan_tok1=None,stan_lem1=None,\n",
    "                             stan_tok2=None,stan_lem2=None,\n",
    "                             maxngram=2,\n",
    "                             ignore_duplicates=True,\n",
    "                             add_stanford_tags=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Derive lexical and part-of-speech alignment scores\n",
    "    between interlocutors (suffix `1` and `2` in arguments\n",
    "    passed to function).\n",
    "\n",
    "    By default, return scores based only on Penn POS taggers.\n",
    "    If desired, also return scores using Stanford tagger with\n",
    "    `add_stanford_tags=True` and by providing appropriate\n",
    "    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and\n",
    "    `stan_lem2`.\n",
    "\n",
    "    By default, consider only bigram when calculating\n",
    "    similarity. If desired, this window may be expanded\n",
    "    by changing the `maxngram` argument value.\n",
    "\n",
    "    By default, remove exact duplicates when calculating\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired,\n",
    "    duplicates may be included when calculating scores by\n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty dictionaries for syntactic similarity\n",
    "    syntax_penn_tok = {}\n",
    "    syntax_penn_lem = {}\n",
    "\n",
    "    # if desired, generate Stanford-based scores\n",
    "    if add_stanford_tags:\n",
    "        syntax_stan_tok = {}\n",
    "        syntax_stan_lem = {}\n",
    "\n",
    "    # create empty dictionaries for lexical similarity\n",
    "    lexical_tok = {}\n",
    "    lexical_lem = {}\n",
    "\n",
    "    # cycle through all desired ngram lengths\n",
    "    for ngram in range(2,maxngram+1):\n",
    "\n",
    "        # calculate similarity for lexical ngrams (tokens and lemmas)\n",
    "        [vectorT1, vectorT2] = ngram_lexical(tok1,tok2,ngramsize=ngram)\n",
    "        [vectorL1, vectorL2] = ngram_lexical(lem1,lem2,ngramsize=ngram)\n",
    "        lexical_tok['lexical_tok{0}'.format(ngram)] = get_cosine(vectorT1,vectorT2)\n",
    "        lexical_lem['lexical_lem{0}'.format(ngram)] = get_cosine(vectorL1, vectorL2)\n",
    "\n",
    "        # calculate similarity for Penn POS ngrams (tokens)\n",
    "        [vector_penn_tok1, vector_penn_tok2] = ngram_pos(penn_tok1,penn_tok2,\n",
    "                                                ngramsize=ngram,\n",
    "                                                ignore_duplicates=ignore_duplicates)\n",
    "        syntax_penn_tok['syntax_penn_tok{0}'.format(ngram)] = get_cosine(vector_penn_tok1,\n",
    "                                                                                            vector_penn_tok2)\n",
    "        # calculate similarity for Penn POS ngrams (lemmas)\n",
    "        [vector_penn_lem1, vector_penn_lem2] = ngram_pos(penn_lem1,penn_lem2,\n",
    "                                                              ngramsize=ngram,\n",
    "                                                              ignore_duplicates=ignore_duplicates)\n",
    "        syntax_penn_lem['syntax_penn_lem{0}'.format(ngram)] = get_cosine(vector_penn_lem1,\n",
    "                                                                                            vector_penn_lem2)\n",
    "\n",
    "        # if desired, also calculate using Stanford POS\n",
    "        if add_stanford_tags:\n",
    "\n",
    "            # calculate similarity for Stanford POS ngrams (tokens)\n",
    "            [vector_stan_tok1, vector_stan_tok2] = ngram_pos(stan_tok1,stan_tok2,\n",
    "                                                                  ngramsize=ngram,\n",
    "                                                                  ignore_duplicates=ignore_duplicates)\n",
    "            syntax_stan_tok['syntax_stan_tok{0}'.format(ngram)] = get_cosine(vector_stan_tok1,\n",
    "                                                                                                vector_stan_tok2)\n",
    "\n",
    "            # calculate similarity for Stanford POS ngrams (lemmas)\n",
    "            [vector_stan_lem1, vector_stan_lem2] = ngram_pos(stan_lem1,stan_lem2,\n",
    "                                                                  ngramsize=ngram,\n",
    "                                                                  ignore_duplicates=ignore_duplicates)\n",
    "            syntax_stan_lem['syntax_stan_lem{0}'.format(ngram)] = get_cosine(vector_stan_lem1,\n",
    "                                                                                                vector_stan_lem2)\n",
    "\n",
    "    # return requested information\n",
    "    if add_stanford_tags:\n",
    "        dictionaries_list = [syntax_penn_tok, syntax_penn_lem,\n",
    "                             syntax_stan_tok, syntax_stan_lem,\n",
    "                             lexical_tok, lexical_lem]\n",
    "    else:\n",
    "        dictionaries_list = [syntax_penn_tok, syntax_penn_lem,\n",
    "                             lexical_tok, lexical_lem]\n",
    "\n",
    "    return dictionaries_list\n",
    "\n",
    "\n",
    "def conceptualAlignment(lem1, lem2, vocablist, highDimModel):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate conceptual alignment scores from list of lemmas\n",
    "    from between two interocutors (suffix `1` and `2` in arguments\n",
    "    passed to function) using `word2vec`.\n",
    "    \"\"\"\n",
    "\n",
    "    # aggregate composite high-dimensional vectors of all words in utterance\n",
    "    W2Vec1 = build_composite_semantic_vector(lem1,vocablist,highDimModel)\n",
    "    W2Vec2 = build_composite_semantic_vector(lem2,vocablist,highDimModel)\n",
    "\n",
    "    # return cosine distance alignment score\n",
    "    return 1 - spatial.distance.cosine(W2Vec1, W2Vec2)\n",
    "\n",
    "\n",
    "def returnMultilevelAlignment(cond_info,\n",
    "                                   partnerA,tok1,lem1,penn_tok1,penn_lem1,\n",
    "                                   partnerB,tok2,lem2,penn_tok2,penn_lem2,\n",
    "                                   vocablist, highDimModel,\n",
    "                                   stan_tok1=None,stan_lem1=None,\n",
    "                                   stan_tok2=None,stan_lem2=None,\n",
    "                                   add_stanford_tags=False,\n",
    "                                   maxngram=2,\n",
    "                                   ignore_duplicates=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate lexical, syntactic, and conceptual alignment\n",
    "    between a pair of turns by individual interlocutors\n",
    "    (suffix `1` and `2` in arguments passed to function),\n",
    "    including leading/following comparison directionality.\n",
    "\n",
    "    By default, return scores based only on Penn POS taggers.\n",
    "    If desired, also return scores using Stanford tagger with\n",
    "    `add_stanford_tags=True` and by providing appropriate\n",
    "    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and\n",
    "    `stan_lem2`.\n",
    "\n",
    "    By default, consider only bigrams when calculating\n",
    "    similarity. If desired, this window may be expanded\n",
    "    by changing the `maxngram` argument value.\n",
    "\n",
    "    By default, remove exact duplicates when calculating\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired,\n",
    "    duplicates may be included when calculating scores by\n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty dictionaries\n",
    "    partner_direction = {}\n",
    "    condition_info = {}\n",
    "    cosine_semanticL = {}\n",
    "\n",
    "    # calculate lexical and syntactic alignment\n",
    "    dictionaries_list = LexicalPOSAlignment(tok1=tok1,lem1=lem1,\n",
    "                                                 penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                 tok2=tok2,lem2=lem2,\n",
    "                                                 penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                 stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                 stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                 maxngram=maxngram,\n",
    "                                                 ignore_duplicates=ignore_duplicates,\n",
    "                                                 add_stanford_tags=add_stanford_tags)\n",
    "\n",
    "    # calculate conceptual alignment\n",
    "    cosine_semanticL['cosine_semanticL'] = conceptualAlignment(lem1,lem2,vocablist,highDimModel)\n",
    "    dictionaries_list.append(cosine_semanticL.copy())\n",
    "\n",
    "    # determine directionality of leading/following comparison\n",
    "    # Note: Partner B is the lagged partner, thus, B is following A\n",
    "    partner_direction['partner_direction'] = str(partnerA) + \">\" + str(partnerB)\n",
    "    dictionaries_list.append(partner_direction.copy())\n",
    "\n",
    "    # add condition information\n",
    "    condition_info['condition_info'] = cond_info\n",
    "    dictionaries_list.append(condition_info.copy())\n",
    "\n",
    "    # return alignment scores\n",
    "    return dictionaries_list\n",
    "\n",
    "\n",
    "def TurnByTurnAnalysis(dataframe,\n",
    "                            vocablist,\n",
    "                            highDimModel,\n",
    "                            delay=1,\n",
    "                            maxngram=2,\n",
    "                            add_stanford_tags=False,\n",
    "                            ignore_duplicates=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate lexical, syntactic, and conceptual alignment\n",
    "    between interlocutors over an entire conversation.\n",
    "    Automatically detect individual speakers by unique\n",
    "    speaker codes.\n",
    "\n",
    "    By default, compare only adjacent turns. If desired,\n",
    "    the comparison distance may be changed by increasing\n",
    "    the `delay` argument.\n",
    "\n",
    "    By default, include maximum n-gram comparison of 2. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "\n",
    "    By default, return scores based only on Penn POS taggers.\n",
    "    If desired, also return scores using Stanford tagger with\n",
    "    `add_stanford_tags=True`.\n",
    "\n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired,\n",
    "    duplicates may be included when calculating scores by\n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # if we don't want the Stanford tagger data, set defaults\n",
    "    if not add_stanford_tags:\n",
    "        stan_tok1=None\n",
    "        stan_lem1=None\n",
    "        stan_tok2=None\n",
    "        stan_lem2=None\n",
    "\n",
    "    # prepare the data to the appropriate type\n",
    "    dataframe['token'] = dataframe['token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['lemma'] = dataframe['lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_token'] = dataframe['tagged_token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_token'] = dataframe['tagged_token'].apply(lambda x: list(zip(x[0::2],x[1::2]))) # thanks to https://stackoverflow.com/a/4647086\n",
    "    dataframe['tagged_lemma'] = dataframe['tagged_lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_lemma'] = dataframe['tagged_lemma'].apply(lambda x: list(zip(x[0::2],x[1::2]))) # thanks to https://stackoverflow.com/a/4647086\n",
    "\n",
    "    # if desired, prepare the Stanford tagger data\n",
    "    if add_stanford_tags:\n",
    "        dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "        dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: list(zip(x[0::2],x[1::2]))) # thanks to https://stackoverflow.com/a/4647086\n",
    "        dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "        dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: list(zip(x[0::2],x[1::2]))) # thanks to https://stackoverflow.com/a/4647086\n",
    "\n",
    "    # create lagged version of the dataframe\n",
    "    df_original = dataframe.drop(dataframe.tail(delay).index,inplace=False)\n",
    "    df_lagged = dataframe.shift(-delay).drop(dataframe.tail(delay).index,inplace=False)\n",
    "\n",
    "    # cycle through each pair of turns\n",
    "    aggregated_df = pd.DataFrame()\n",
    "    for i in range(0,df_original.shape[0]):\n",
    "\n",
    "        # identify the condition for this dataframe\n",
    "        cond_info = dataframe['file'].unique()\n",
    "        if len(cond_info)==1:\n",
    "            cond_info = str(cond_info[0])\n",
    "\n",
    "        # break and flag error if we have more than 1 condition per dataframe\n",
    "        else:\n",
    "            raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)\n",
    "\n",
    "        # grab all of first participant's data\n",
    "        first_row = df_original.iloc[i]\n",
    "        first_partner = first_row['participant']\n",
    "        tok1=first_row['token']\n",
    "        lem1=first_row['lemma']\n",
    "        penn_tok1=first_row['tagged_token']\n",
    "        penn_lem1=first_row['tagged_lemma']\n",
    "\n",
    "        # grab all of lagged participant's data\n",
    "        lagged_row = df_lagged.iloc[i]\n",
    "        lagged_partner = lagged_row['participant']\n",
    "        tok2=lagged_row['token']\n",
    "        lem2=lagged_row['lemma']\n",
    "        penn_tok2=lagged_row['tagged_token']\n",
    "        penn_lem2=lagged_row['tagged_lemma']\n",
    "\n",
    "        # if desired, grab the Stanford tagger data for both participants\n",
    "        if add_stanford_tags:\n",
    "            stan_tok1=first_row['tagged_stan_token']\n",
    "            stan_lem1=first_row['tagged_stan_lemma']\n",
    "            stan_tok2=lagged_row['tagged_stan_token']\n",
    "            stan_lem2=lagged_row['tagged_stan_lemma']\n",
    "\n",
    "        # process multilevel alignment\n",
    "        dictionaries_list=returnMultilevelAlignment(cond_info=cond_info,\n",
    "                                                         partnerA=first_partner,\n",
    "                                                         tok1=tok1,lem1=lem1,\n",
    "                                                         penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                         partnerB=lagged_partner,\n",
    "                                                         tok2=tok2,lem2=lem2,\n",
    "                                                         penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                         vocablist=vocablist,\n",
    "                                                         highDimModel=highDimModel,\n",
    "                                                         stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                         stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                         maxngram = maxngram,\n",
    "                                                         ignore_duplicates = ignore_duplicates,\n",
    "                                                         add_stanford_tags = add_stanford_tags)\n",
    "\n",
    "        # sort columns so they are in order, append data to existing structures\n",
    "        next_df_line = pd.DataFrame.from_dict(OrderedDict(k for num, i in enumerate(d for d in dictionaries_list) for k in sorted(i.items())),\n",
    "                               orient='index').transpose()\n",
    "        aggregated_df = aggregated_df.append(next_df_line)\n",
    "\n",
    "    # reformat turn information and add index\n",
    "    aggregated_df = aggregated_df.reset_index(drop=True).reset_index().rename(columns={\"index\":\"time\"})\n",
    "\n",
    "    # give us our finished dataframe\n",
    "    return aggregated_df\n",
    "\n",
    "\n",
    "def ConvoByConvoAnalysis(dataframe,\n",
    "                          maxngram=2,\n",
    "                          ignore_duplicates=True,\n",
    "                          add_stanford_tags=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate analysis of multilevel similarity over\n",
    "    a conversation between two interlocutors from a\n",
    "    transcript dataframe prepared by Phase 1\n",
    "    of ALIGN. Automatically detect speakers by unique\n",
    "    speaker codes.\n",
    "\n",
    "    By default, include maximum n-gram comparison of 2. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "\n",
    "    By default, return scores based only on Penn POS taggers.\n",
    "    If desired, also return scores using Stanford tagger with\n",
    "    `add_stanford_tags=True`.\n",
    "\n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired,\n",
    "    duplicates may be included when calculating scores by\n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # identify the condition for this dataframe\n",
    "    cond_info = dataframe['file'].unique()\n",
    "    if len(cond_info)==1:\n",
    "        cond_info = str(cond_info[0])\n",
    "\n",
    "    # break and flag error if we have more than 1 condition per dataframe\n",
    "    else:\n",
    "        raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)\n",
    "\n",
    "    # if we don't want the Stanford info, set defaults\n",
    "    if not add_stanford_tags:\n",
    "        stan_tok1 = None\n",
    "        stan_lem1 = None\n",
    "        stan_tok2 = None\n",
    "        stan_lem2 = None\n",
    "\n",
    "    # identify individual interlocutors\n",
    "    df_A = dataframe.loc[dataframe['participant'] == dataframe['participant'].unique()[0]]\n",
    "    df_B = dataframe.loc[dataframe['participant'] == dataframe['participant'].unique()[1]]\n",
    "\n",
    "    # concatenate the token, lemma, and POS information for participant A\n",
    "    tok1 = [word for turn in df_A['token'] for word in turn]\n",
    "    lem1 = [word for turn in df_A['lemma'] for word in turn]\n",
    "    penn_tok1 = [POS for turn in df_A['tagged_token'] for POS in turn]\n",
    "    penn_lem1 = [POS for turn in df_A['tagged_lemma'] for POS in turn]\n",
    "    if add_stanford_tags:\n",
    "\n",
    "        if isinstance(df_A['tagged_stan_token'][0], list):\n",
    "            stan_tok1 = [POS for turn in df_A['tagged_stan_token'] for POS in turn]\n",
    "            stan_lem1 = [POS for turn in df_A['tagged_stan_lemma'] for POS in turn]\n",
    "\n",
    "        elif isinstance(df_A['tagged_stan_token'][0], str):\n",
    "            stan_tok1 = pd.Series(df_A['tagged_stan_token'].values).apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "            stan_tok1 = stan_tok1.apply(lambda x: list(zip(x[0::2],x[1::2])))\n",
    "            stan_tok1 = [POS for turn in stan_tok1 for POS in turn]\n",
    "            stan_lem1 = pd.Series(df_A['tagged_stan_lemma'].values).apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "            stan_lem1 = stan_lem1.apply(lambda x: list(zip(x[0::2],x[1::2])))\n",
    "            stan_lem1 = [POS for turn in stan_lem1 for POS in turn]\n",
    "\n",
    "    # concatenate the token, lemma, and POS information for participant B\n",
    "    tok2 = [word for turn in df_B['token'] for word in turn]\n",
    "    lem2 = [word for turn in df_B['lemma'] for word in turn]\n",
    "    penn_tok2 = [POS for turn in df_B['tagged_token'] for POS in turn]\n",
    "    penn_lem2 = [POS for turn in df_B['tagged_lemma'] for POS in turn]\n",
    "    if add_stanford_tags:\n",
    "\n",
    "        if isinstance(df_A['tagged_stan_token'][0],list):\n",
    "            stan_tok2 = [POS for turn in df_B['tagged_stan_token'] for POS in turn]\n",
    "            stan_lem2 = [POS for turn in df_B['tagged_stan_lemma'] for POS in turn]\n",
    "\n",
    "        elif isinstance(df_A['tagged_stan_token'][0], str):\n",
    "            stan_tok2 = pd.Series(df_B['tagged_stan_token'].values).apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "            stan_tok2 = stan_tok2.apply(lambda x: list(zip(x[0::2],x[1::2])))\n",
    "            stan_tok2 = [POS for turn in stan_tok2 for POS in turn]\n",
    "            stan_lem2 = pd.Series(df_B['tagged_stan_lemma'].values).apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "            stan_lem2 = stan_lem2.apply(lambda x: list(zip(x[0::2],x[1::2])))\n",
    "            stan_lem2 = [POS for turn in stan_lem2 for POS in turn]\n",
    "\n",
    "    # process multilevel alignment\n",
    "    dictionaries_list = LexicalPOSAlignment(tok1=tok1,lem1=lem1,\n",
    "                                                 penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                 tok2=tok2,lem2=lem2,\n",
    "                                                 penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                 stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                 stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                 maxngram=maxngram,\n",
    "                                                 ignore_duplicates=ignore_duplicates,\n",
    "                                                 add_stanford_tags=add_stanford_tags)\n",
    "\n",
    "    # append data to existing structures\n",
    "    dictionary_df = pd.DataFrame.from_dict(OrderedDict(k for num, i in enumerate(d for d in dictionaries_list) for k in sorted(i.items())),\n",
    "                       orient='index').transpose()\n",
    "    dictionary_df['condition_info'] = cond_info\n",
    "\n",
    "    # return the dataframe\n",
    "    return dictionary_df\n",
    "\n",
    "\n",
    "def GenerateSurrogate(original_conversation_list,\n",
    "                           surrogate_file_directory,\n",
    "                           all_surrogates=True,\n",
    "                           keep_original_turn_order=True,\n",
    "                           id_separator = '\\-',\n",
    "                           dyad_label='dyad',\n",
    "                           condition_label='cond'):\n",
    "\n",
    "    \"\"\"\n",
    "    Create transcripts for surrogate pairs of\n",
    "    participants (i.e., participants who did not\n",
    "    genuinely interact in the experiment), which\n",
    "    will later be used to generate baseline levels\n",
    "    of alignment. Store surrogate files in a new\n",
    "    folder each time the surrogate generation is run.\n",
    "\n",
    "    Returns a list of all surrogate files created.\n",
    "\n",
    "    By default, the separator between dyad ID and\n",
    "    condition ID is a hyphen ('\\-'). If desired,\n",
    "    this may be changed in the `id_separator`\n",
    "    argument.\n",
    "\n",
    "    By default, condition IDs will be identified as\n",
    "    any characters following `cond`. If desired,\n",
    "    this may be changed with the `condition_label`\n",
    "    argument.\n",
    "\n",
    "    By default, dyad IDs will be identified as\n",
    "    any characters following `dyad`. If desired,\n",
    "    this may be changed with the `dyad_label`\n",
    "    argument.\n",
    "\n",
    "    By default, generate surrogates from all possible\n",
    "    pairings. If desired, instead generate surrogates\n",
    "    only from a subset of all possible pairings\n",
    "    with `all_surrogates=False`.\n",
    "\n",
    "    By default, create surrogates by retaining the\n",
    "    original ordering of each surrogate partner's\n",
    "    data. If desired, create surrogates by shuffling\n",
    "    all turns within each surrogate partner's data\n",
    "    with `keep_original_turn_order = False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a subfolder for the new set of surrogates\n",
    "    import time\n",
    "    new_surrogate_path = surrogate_file_directory + 'surrogate_run-' + str(time.time()) +'/'\n",
    "    if not os.path.exists(new_surrogate_path):\n",
    "        os.makedirs(new_surrogate_path)\n",
    "\n",
    "    # grab condition types from each file name\n",
    "    file_info = [re.sub('\\.txt','',os.path.basename(file_name)) for file_name in original_conversation_list]\n",
    "    condition_ids = list(set([re.findall('[^'+id_separator+']*'+condition_label+'.*',metadata)[0] for metadata in file_info]))\n",
    "    files_conditions = {}\n",
    "    for unique_condition in condition_ids:\n",
    "        next_condition_files = [add_file for add_file in original_conversation_list if unique_condition in add_file]\n",
    "        files_conditions[unique_condition] = next_condition_files\n",
    "\n",
    "    # cycle through conditions\n",
    "    for condition in list(files_conditions.keys()):\n",
    "\n",
    "        # default: grab all possible pairs of conversations of this condition\n",
    "        paired_surrogates = [pair for pair in combinations(files_conditions[condition],2)]\n",
    "\n",
    "        # otherwise, if desired, randomly pull from all pairs to get target surrogate sample\n",
    "        if not all_surrogates:\n",
    "            import math\n",
    "            paired_surrogates = random.sample(paired_surrogates,\n",
    "                                              int(math.ceil(len(files_conditions[condition])/2)))\n",
    "\n",
    "        # cycle through surrogate pairings\n",
    "        for next_surrogate in paired_surrogates:\n",
    "\n",
    "            # read in the files\n",
    "            original_file1 = os.path.basename(next_surrogate[0])\n",
    "            original_file2 = os.path.basename(next_surrogate[1])\n",
    "            original_df1=pd.read_csv(next_surrogate[0], sep='\\t',encoding='utf-8')\n",
    "            original_df2=pd.read_csv(next_surrogate[1], sep='\\t',encoding='utf-8')\n",
    "\n",
    "            # get participants A and B from df1\n",
    "            participantA_1_code = min(original_df1['participant'].unique())\n",
    "            participantB_1_code = max(original_df1['participant'].unique())\n",
    "            participantA_1 = original_df1[original_df1['participant'] == participantA_1_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            participantB_1 = original_df1[original_df1['participant'] == participantB_1_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "\n",
    "            # get participants A and B from df2\n",
    "            participantA_2_code = min(original_df2['participant'].unique())\n",
    "            participantB_2_code = max(original_df2['participant'].unique())\n",
    "            participantA_2 = original_df2[original_df2['participant'] == participantA_2_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            participantB_2 = original_df2[original_df2['participant'] == participantB_2_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "\n",
    "            # identify truncation point for both surrogates (to have even number of turns)\n",
    "            surrogateX_turns=min([participantA_1.shape[0],\n",
    "                                  participantB_2.shape[0]])\n",
    "            surrogateY_turns=min([participantA_2.shape[0],\n",
    "                                  participantB_1.shape[0]])\n",
    "\n",
    "            # preserve original turn order for surrogate pairs\n",
    "            if keep_original_turn_order:\n",
    "                surrogateX_A1 = participantA_1.truncate(after=surrogateX_turns-1,\n",
    "                                                        copy=False)\n",
    "                surrogateX_B2 = participantB_2.truncate(after=surrogateX_turns-1,\n",
    "                                                        copy=False)\n",
    "                surrogateX = pd.concat(\n",
    "                    [surrogateX_A1, surrogateX_B2]).sort_index(\n",
    "                            kind=\"mergesort\").reset_index(\n",
    "                                    drop=True).rename(\n",
    "                                        columns={'index': 'original_index'})\n",
    "\n",
    "                surrogateY_A2 = participantA_2.truncate(after=surrogateY_turns-1,\n",
    "                                                        copy=False)\n",
    "                surrogateY_B1 = participantB_1.truncate(after=surrogateY_turns-1,\n",
    "                                                        copy=False)\n",
    "                surrogateY = pd.concat(\n",
    "                    [surrogateY_A2, surrogateY_B1]).sort_index(\n",
    "                            kind=\"mergesort\").reset_index(\n",
    "                                    drop=True).rename(\n",
    "                                            columns={'index': 'original_index'})\n",
    "\n",
    "            # otherwise, if desired, just shuffle all turns within participants\n",
    "            else:\n",
    "\n",
    "                # shuffle for first surrogate pairing\n",
    "                surrogateX_A1 = participantA_1.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateX_B2 = participantB_2.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateX = pd.concat([surrogateX_A1,surrogateX_B2]).sort_index(kind=\"mergesort\").reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "\n",
    "                # and for second surrogate pairing\n",
    "                surrogateY_A2 = participantA_2.truncate(after=surrogateY_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateY_B1 = participantB_1.truncate(after=surrogateY_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateY = pd.concat([surrogateY_A2,surrogateY_B1]).sort_index(kind=\"mergesort\").reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "\n",
    "            # create filename for our surrogate file\n",
    "            original_dyad1 = re.findall(dyad_label+'[^'+id_separator+']*',original_file1)[0]\n",
    "            original_dyad2 = re.findall(dyad_label+'[^'+id_separator+']*',original_file2)[0]\n",
    "            surrogateX['file'] = original_dyad1 + '-' + original_dyad2 + '-' + condition\n",
    "            surrogateY['file'] = original_dyad2 + '-' + original_dyad1 + '-' + condition\n",
    "            nameX='SurrogatePair-'+original_dyad1+'A'+'-'+original_dyad2+'B'+'-'+condition+'.txt'\n",
    "            nameY='SurrogatePair-'+original_dyad2+'A'+'-'+original_dyad1+'B'+'-'+condition+'.txt'\n",
    "\n",
    "            # save to file\n",
    "            surrogateX.to_csv(new_surrogate_path + nameX, encoding='utf-8',index=False,sep='\\t')\n",
    "            surrogateY.to_csv(new_surrogate_path + nameY, encoding='utf-8',index=False,sep='\\t')\n",
    "\n",
    "    # return list of all surrogate files\n",
    "    return glob.glob(new_surrogate_path+\"*.txt\")\n",
    "\n",
    "\n",
    "def calculate_alignment(input_files,\n",
    "                        output_file_directory,\n",
    "                        semantic_model_input_file,\n",
    "                        pretrained_input_file,\n",
    "                        high_sd_cutoff=3,\n",
    "                        low_n_cutoff=1,\n",
    "                        delay=1,\n",
    "                        maxngram=2,\n",
    "                        use_pretrained_vectors=True,\n",
    "                        ignore_duplicates=True,\n",
    "                        add_stanford_tags=False,\n",
    "                        input_as_directory=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate lexical, syntactic, and conceptual alignment between speakers.\n",
    "\n",
    "    Given a directory of individual .txt files and the\n",
    "    vocabulary list that have been generated by the `prepare_transcripts`\n",
    "    preparation stage, return multi-level alignment\n",
    "    scores with turn-by-turn and conversation-level metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    input_files : str (directory name) or list of str (file names)\n",
    "        Cleaned files to be analyzed. Behavior governed by `input_as_directory`\n",
    "        parameter as well.\n",
    "\n",
    "    output_file_directory : str\n",
    "        Name of directory where output for individual conversations will be\n",
    "        saved.\n",
    "\n",
    "    semantic_model_input_file : str\n",
    "        Name of file to be used for creating the semantic model. A compatible\n",
    "        file will be saved as an output of `prepare_transcripts()`.\n",
    "\n",
    "    pretrained_input_file : str or None\n",
    "        If using a pretrained vector to create the semantic model, use\n",
    "        name of model here. If not, use None. Behavior governed by\n",
    "        `use_pretrained_vectors` parameter as well.\n",
    "\n",
    "    high_sd_cutoff : int, optional (default: 3)\n",
    "        High-frequency cutoff (in SD over the mean) for lexical items\n",
    "        when creating the semantic model.\n",
    "\n",
    "    low_n_cutoff : int, optional (default: 1)\n",
    "        Low-frequency cutoff (in raw frequency) for lexical items when\n",
    "        creating the semantic models. Items with frequency less than or\n",
    "        equal to the number provided here will be removed. To remove the\n",
    "        low-frequency cutoff, set to 0.\n",
    "\n",
    "    delay : int, optional (default: 1)\n",
    "        Delay (or lag) at which to calculate similarity. A lag of 1 (default)\n",
    "        considers only adjacent turns.\n",
    "\n",
    "    maxngram : int, optional (default: 2)\n",
    "        Maximum n-gram size for calculations. Similarity scores for n-grams\n",
    "        from unigrams to the maximum size specified here will be calculated.\n",
    "\n",
    "    use_pretrained_vectors : boolean, optional (default: True)\n",
    "        Specify whether to use a pretrained gensim model for word2vec\n",
    "        analysis (True) or to construct a new model from the provided corpus\n",
    "        (False). If True, the file name of a valid model must be\n",
    "        provided to the `pretrained_input_file` parameter.\n",
    "\n",
    "    ignore_duplicates : boolean, optional (default: True)\n",
    "        Specify whether to remove exact duplicates when calculating\n",
    "        part-of-speech similarity scores (True) or to retain perfectly\n",
    "        mimicked lexical items for POS similarity calculation (False).\n",
    "\n",
    "    add_stanford_tags : boolean, optional (default: False)\n",
    "        Specify whether to return part-of-speech similarity scores based on\n",
    "        Stanford POS tagger in addition to the Penn POS tagger (True) or to\n",
    "        return only POS similarity scores from the Penn tagger (False). (Note:\n",
    "        Including Stanford POS tags will lead to a significant increase in\n",
    "        processing time.)\n",
    "\n",
    "    input_as_directory : boolean, optional (default: True)\n",
    "        Specify whether the value passed to `input_files` parameter should\n",
    "        be read as a directory (True) or a list of files to be processed\n",
    "        (False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    real_final_turn_df : Pandas DataFrame\n",
    "        A dataframe of lexical, syntactic, and conceptual alignment scores\n",
    "        between turns at specified delay. `NaN` values will be returned for\n",
    "        turns in which the speaker only produced words that were removed\n",
    "        from the corpus (e.g., too rare or too common words) or words that were\n",
    "        present in the corpus but not in the semantic model.\n",
    "\n",
    "    real_final_convo_df : Pandas DataFrame\n",
    "        A dataframe of lexical, syntactic, and conceptual alignment scores\n",
    "        between participants across the entire conversation.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # grab the files in the list\n",
    "    if not input_as_directory:\n",
    "        file_list = glob.glob(input_files)\n",
    "    else:\n",
    "        file_list = glob.glob(input_files+\"/*.txt\")\n",
    "\n",
    "    # build the semantic model to be used for all conversations\n",
    "    [vocablist, highDimModel] = BuildSemanticModel(semantic_model_input_file=semantic_model_input_file,\n",
    "                                                       pretrained_input_file=pretrained_input_file,\n",
    "                                                       use_pretrained_vectors=use_pretrained_vectors,\n",
    "                                                       high_sd_cutoff=high_sd_cutoff,\n",
    "                                                       low_n_cutoff=low_n_cutoff)\n",
    "\n",
    "    # create containers for alignment values\n",
    "    AlignmentT2T = pd.DataFrame()\n",
    "    AlignmentC2C = pd.DataFrame()\n",
    "\n",
    "    # cycle through each prepared file\n",
    "    for fileName in file_list:\n",
    "\n",
    "        # process the file if it's got a valid conversation\n",
    "        dataframe=pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        if len(dataframe) > 1:\n",
    "\n",
    "            # let us know which filename we're processing\n",
    "            print((\"Processing: \"+fileName))\n",
    "\n",
    "            # calculate turn-by-turn alignment scores\n",
    "            xT2T=TurnByTurnAnalysis(dataframe=dataframe,\n",
    "                                         delay=delay,\n",
    "                                         maxngram=maxngram,\n",
    "                                         vocablist=vocablist,\n",
    "                                         highDimModel=highDimModel,\n",
    "                                         add_stanford_tags=add_stanford_tags,\n",
    "                                         ignore_duplicates=ignore_duplicates)\n",
    "            AlignmentT2T=AlignmentT2T.append(xT2T)\n",
    "\n",
    "            # calculate conversation-level alignment scores\n",
    "            xC2C = ConvoByConvoAnalysis(dataframe=dataframe,\n",
    "                                             maxngram = maxngram,\n",
    "                                             ignore_duplicates=ignore_duplicates,\n",
    "                                             add_stanford_tags = add_stanford_tags)\n",
    "            AlignmentC2C=AlignmentC2C.append(xC2C)\n",
    "\n",
    "        # if it's invalid, let us know\n",
    "        else:\n",
    "            print((\"Invalid file: \"+fileName))\n",
    "\n",
    "    # update final dataframes\n",
    "    real_final_turn_df = AlignmentT2T.reset_index(drop=True)\n",
    "    real_final_convo_df = AlignmentC2C.reset_index(drop=True)\n",
    "\n",
    "    # export the final files\n",
    "    real_final_turn_df.to_csv(output_file_directory+\"AlignmentT2T.txt\",\n",
    "                      encoding='utf-8', index=False, sep='\\t')\n",
    "    real_final_convo_df.to_csv(output_file_directory+\"AlignmentC2C.txt\",\n",
    "                       encoding='utf-8', index=False, sep='\\t')\n",
    "\n",
    "    # display the info, too\n",
    "    return real_final_turn_df, real_final_convo_df\n",
    "\n",
    "\n",
    "def calculate_baseline_alignment(input_files,\n",
    "                                 surrogate_file_directory,\n",
    "                                 output_file_directory,\n",
    "                                 semantic_model_input_file,\n",
    "                                 pretrained_input_file,\n",
    "                                 high_sd_cutoff=3,\n",
    "                                 low_n_cutoff=1,\n",
    "                                 id_separator='\\-',\n",
    "                                 condition_label='cond',\n",
    "                                 dyad_label='dyad',\n",
    "                                 all_surrogates=True,\n",
    "                                 keep_original_turn_order=True,\n",
    "                                 delay=1,\n",
    "                                 maxngram=2,\n",
    "                                 use_pretrained_vectors=True,\n",
    "                                 ignore_duplicates=True,\n",
    "                                 add_stanford_tags=False,\n",
    "                                 input_as_directory=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate baselines for lexical, syntactic, and conceptual\n",
    "    alignment between speakers.\n",
    "\n",
    "    Given a directory of individual .txt files and the\n",
    "    vocab list that have been generated by the `prepare_transcripts`\n",
    "    preparation stage, return multi-level alignment\n",
    "    scores with turn-by-turn and conversation-level metrics\n",
    "    for surrogate baseline conversations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    input_files : str (directory name) or list of str (file names)\n",
    "        Cleaned files to be analyzed. Behavior governed by `input_as_directory`\n",
    "        parameter as well.\n",
    "\n",
    "    surrogate_file_directory : str\n",
    "        Name of directory where raw surrogate data will be saved.\n",
    "\n",
    "    output_file_directory : str\n",
    "        Name of directory where output for individual surrogate\n",
    "        conversations will be saved.\n",
    "\n",
    "    semantic_model_input_file : str\n",
    "        Name of file to be used for creating the semantic model. A compatible\n",
    "        file will be saved as an output of `prepare_transcripts()`.\n",
    "\n",
    "    pretrained_input_file : str or None\n",
    "        If using a pretrained vector to create the semantic model, use\n",
    "        name of model here. If not, use None. Behavior governed by\n",
    "        `use_pretrained_vectors` parameter as well.\n",
    "\n",
    "    high_sd_cutoff : int, optional (default: 3)\n",
    "        High-frequency cutoff (in SD over the mean) for lexical items\n",
    "        when creating the semantic model.\n",
    "\n",
    "    low_n_cutoff : int, optional (default: 1)\n",
    "        Low-frequency cutoff (in raw frequency) for lexical items when\n",
    "        creating the semantic models. Items with frequency less than or\n",
    "        equal to the number provided here will be removed. To remove the\n",
    "        low-frequency cutoff, set to 0.\n",
    "\n",
    "    id_separator : str, optional (default: '\\-')\n",
    "        Character separator between the dyad and condition IDs in\n",
    "        original data file names.\n",
    "\n",
    "    condition_label : str, optional (default: 'cond')\n",
    "        String preceding ID for each unique condition. Anything after this\n",
    "        label will be identified as a unique condition ID.\n",
    "\n",
    "    dyad_label : str, optional (default: 'dyad')\n",
    "        String preceding ID for each unique dyad. Anything after this label\n",
    "        will be identified as a unique dyad ID.\n",
    "\n",
    "    all_surrogates : boolean, optional (default: True)\n",
    "        Specify whether to generate all possible surrogates across original\n",
    "        dataset (True) or to generate only a subset of surrogates equal to\n",
    "        the real sample size drawn randomly from all possible surrogates\n",
    "        (False).\n",
    "\n",
    "    keep_original_turn_order : boolean, optional (default: True)\n",
    "        Specify whether to retain original turn ordering when pairing surrogate\n",
    "        dyads (True) or to pair surrogate partners' turns in random order\n",
    "        (False).\n",
    "\n",
    "    delay : int, optional (default: 1)\n",
    "        Delay (or lag) at which to calculate similarity. A lag of 1 (default)\n",
    "        considers only adjacent turns.\n",
    "\n",
    "    maxngram : int, optional (default: 2)\n",
    "        Maximum n-gram size for calculations. Similarity scores for n-grams\n",
    "        from unigrams to the maximum size specified here will be calculated.\n",
    "\n",
    "    use_pretrained_vectors : boolean, optional (default: True)\n",
    "        Specify whether to use a pretrained gensim model for word2vec\n",
    "        analysis. If True, the file name of a valid model must be\n",
    "        provided to the `pretrained_input_file` parameter.\n",
    "\n",
    "    ignore_duplicates : boolean, optional (default: True)\n",
    "        Specify whether to remove exact duplicates when calculating\n",
    "        part-of-speech similarity scores. By default, ignore perfectly\n",
    "        mimicked lexical items for POS similarity calculation.\n",
    "\n",
    "    add_stanford_tags : boolean, optional (default: False)\n",
    "        Specify whether to return part-of-speech similarity scores\n",
    "        based on Stanford POS tagger (in addition to the Penn POS\n",
    "        tagger).\n",
    "\n",
    "    input_as_directory : boolean, optional (default: True)\n",
    "        Specify whether the value passed to `input_files` parameter should\n",
    "        be read as a directory or a list of files to be processed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    surrogate_final_turn_df : Pandas DataFrame\n",
    "        A dataframe of lexical, syntactic, and conceptual alignment scores\n",
    "        between turns at specified delay for surrogate partners. `NaN` values\n",
    "        will be returned for turns in which the speaker only produced words\n",
    "        that were removed from the corpus (e.g., too rare or too common words)\n",
    "        or words that were present in the corpus but not in the semantic model.\n",
    "\n",
    "    surrogate_final_convo_df : Pandas DataFrame\n",
    "        A dataframe of lexical, syntactic, and conceptual alignment scores\n",
    "        between surrogate partners across the entire conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    # grab the files in the input list\n",
    "    if not input_as_directory:\n",
    "        file_list = glob.glob(input_files)\n",
    "    else:\n",
    "        file_list = glob.glob(input_files+\"/*.txt\")\n",
    "\n",
    "    # create a surrogate file list\n",
    "    surrogate_file_list = GenerateSurrogate(\n",
    "                            original_conversation_list=file_list,\n",
    "                            surrogate_file_directory=surrogate_file_directory,\n",
    "                            all_surrogates=all_surrogates,\n",
    "                            id_separator=id_separator,\n",
    "                            condition_label=condition_label,\n",
    "                            dyad_label=dyad_label,\n",
    "                            keep_original_turn_order=keep_original_turn_order)\n",
    "\n",
    "    # build the semantic model to be used for all conversations\n",
    "    [vocablist, highDimModel] = BuildSemanticModel(\n",
    "                            semantic_model_input_file=semantic_model_input_file,\n",
    "                            pretrained_input_file=pretrained_input_file,\n",
    "                            use_pretrained_vectors=use_pretrained_vectors,\n",
    "                            high_sd_cutoff=high_sd_cutoff,\n",
    "                            low_n_cutoff=low_n_cutoff)\n",
    "\n",
    "    # create containers for alignment values\n",
    "    AlignmentT2T = pd.DataFrame()\n",
    "    AlignmentC2C = pd.DataFrame()\n",
    "\n",
    "    # cycle through the files\n",
    "    for fileName in surrogate_file_list:\n",
    "\n",
    "        # process the file if it's got a valid conversation\n",
    "        dataframe=pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        if len(dataframe) > 1:\n",
    "\n",
    "            # let us know which filename we're processing\n",
    "            print((\"Processing: \"+fileName))\n",
    "\n",
    "            # calculate turn-by-turn alignment scores\n",
    "            xT2T=TurnByTurnAnalysis(dataframe=dataframe,\n",
    "                                         delay=delay,\n",
    "                                         maxngram=maxngram,\n",
    "                                         vocablist=vocablist,\n",
    "                                         highDimModel=highDimModel,\n",
    "                                         add_stanford_tags = add_stanford_tags,\n",
    "                                         ignore_duplicates = ignore_duplicates)\n",
    "            AlignmentT2T=AlignmentT2T.append(xT2T)\n",
    "\n",
    "            # calculate conversation-level alignment scores\n",
    "            xC2C = ConvoByConvoAnalysis(dataframe=dataframe,\n",
    "                                             maxngram = maxngram,\n",
    "                                             ignore_duplicates=ignore_duplicates,\n",
    "                                             add_stanford_tags = add_stanford_tags)\n",
    "            AlignmentC2C=AlignmentC2C.append(xC2C)\n",
    "\n",
    "        # if it's invalid, let us know\n",
    "        else:\n",
    "            print((\"Invalid file: \"+fileName))\n",
    "\n",
    "    # update final dataframes\n",
    "    surrogate_final_turn_df = AlignmentT2T.reset_index(drop=True)\n",
    "    surrogate_final_convo_df = AlignmentC2C.reset_index(drop=True)\n",
    "\n",
    "    # export the final files\n",
    "    surrogate_final_turn_df.to_csv(output_file_directory+\"AlignmentT2T_Surrogate.txt\",\n",
    "                      encoding='utf-8',index=False,sep='\\t')\n",
    "    surrogate_final_convo_df.to_csv(output_file_directory+\"AlignmentC2C_Surrogate.txt\",\n",
    "                       encoding='utf-8',index=False,sep='\\t')\n",
    "\n",
    "    # display the info, too\n",
    "    return surrogate_final_turn_df, surrogate_final_convo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN ALIGNMENT PHASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify path settings\n",
    "\n",
    "ALIGN will need to know where the raw transcripts are stored, where to store the processed data, and where to read in any additional files needed for optional ALIGN parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()\n",
    "\n",
    "CORPUS = os.path.join(BASE_PATH,\n",
    "                              'analysis/CHILDES/')\n",
    "\n",
    "PREPPED_TRANSCRIPTS = os.path.join(CORPUS,\n",
    "                                   'prepped_penn/')\n",
    "\n",
    "ANALYSIS_READY = os.path.join(CORPUS,\n",
    "                              'analysis/')\n",
    "\n",
    "SURROGATE_TRANSCRIPTS = os.path.join(CORPUS,\n",
    "                                     'surrogate/')\n",
    "\n",
    "OPTIONAL_PATHS = os.path.join(BASE_PATH,\n",
    "                             'optional_directories/')\n",
    "\n",
    "## GOOGLE NEWS PRETRAINED VECTORS\n",
    "PRETRAINED_INPUT_FILE = os.path.join(OPTIONAL_PATHS,\n",
    "                            'GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set standards to be used for real and surrogate\n",
    "INPUT_FILES = PREPPED_TRANSCRIPTS\n",
    "MAXNGRAM = 3\n",
    "USE_PRETRAINED_VECTORS = True\n",
    "SEMANTIC_MODEL_INPUT_FILE = os.path.join(CORPUS,\n",
    "                                         'align_concatenated_dataframe.txt')\n",
    "PRETRAINED_FILE_DRIRECTORY = PRETRAINED_INPUT_FILE\n",
    "ADD_STANFORD_TAGS = True\n",
    "IGNORE_DUPLICATES = False\n",
    "HIGH_SD_CUTOFF = 3\n",
    "LOW_N_CUTOFF = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickduran/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:139: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/Users/nickduran/opt/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time197-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time202-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time191-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time209-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time210-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time204-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time196-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time203-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time208-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time205-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time195-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time198-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time200-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time193-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time206-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time194-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time199-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time201-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time192-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/prepped_penn/time207-cond1.txt\n"
     ]
    }
   ],
   "source": [
    "start_phase2real = time.time()\n",
    "[turn_real,convo_real] = calculate_alignment(\n",
    "                            input_files=INPUT_FILES,\n",
    "                            maxngram=MAXNGRAM,   \n",
    "                            use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                            pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                            semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                            output_file_directory=ANALYSIS_READY,\n",
    "                            add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                            ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                            high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                            low_n_cutoff=LOW_N_CUTOFF)\n",
    "end_phase2real = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.930322885513306"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_phase2real - start_phase2real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>syntax_penn_tok2</th>\n",
       "      <th>syntax_penn_tok3</th>\n",
       "      <th>syntax_penn_lem2</th>\n",
       "      <th>syntax_penn_lem3</th>\n",
       "      <th>lexical_tok2</th>\n",
       "      <th>lexical_tok3</th>\n",
       "      <th>lexical_lem2</th>\n",
       "      <th>lexical_lem3</th>\n",
       "      <th>cosine_semanticL</th>\n",
       "      <th>partner_direction</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258996</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33429</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.154303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.534409</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.629952</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.546912</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.484909</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184623</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.117851</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452096</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225365</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.28817</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time syntax_penn_tok2 syntax_penn_tok3 syntax_penn_lem2 syntax_penn_lem3  \\\n",
       "0     0                0                0                0                0   \n",
       "1     1                0                0                0                0   \n",
       "2     2         0.154303                0                0                0   \n",
       "3     3                0                0                0                0   \n",
       "4     4         0.111111                0          0.09245                0   \n",
       "5     5         0.222222                0          0.27735                0   \n",
       "6     6                0                0                0                0   \n",
       "7     7         0.117851                0                0                0   \n",
       "8     8         0.288675                0         0.288675                0   \n",
       "9     9                0                0                0                0   \n",
       "\n",
       "  lexical_tok2 lexical_tok3 lexical_lem2 lexical_lem3 cosine_semanticL  \\\n",
       "0            0            0            0            0         0.258996   \n",
       "1            0            0            0            0          0.33429   \n",
       "2            0            0            0            0         0.534409   \n",
       "3            0            0            0            0         0.629952   \n",
       "4            0            0            0            0         0.546912   \n",
       "5            0            0            0            0         0.484909   \n",
       "6            0            0            0            0         0.184623   \n",
       "7            0            0            0            0         0.452096   \n",
       "8            0            0            0            0         0.225365   \n",
       "9            0            0            0            0          0.28817   \n",
       "\n",
       "  partner_direction     condition_info  \n",
       "0           cgv>kid  time197-cond1.txt  \n",
       "1           kid>cgv  time197-cond1.txt  \n",
       "2           cgv>kid  time197-cond1.txt  \n",
       "3           kid>cgv  time197-cond1.txt  \n",
       "4           cgv>kid  time197-cond1.txt  \n",
       "5           kid>cgv  time197-cond1.txt  \n",
       "6           cgv>kid  time197-cond1.txt  \n",
       "7           kid>cgv  time197-cond1.txt  \n",
       "8           cgv>kid  time197-cond1.txt  \n",
       "9           kid>cgv  time197-cond1.txt  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>syntax_penn_tok2</th>\n",
       "      <th>syntax_penn_tok3</th>\n",
       "      <th>syntax_penn_lem2</th>\n",
       "      <th>syntax_penn_lem3</th>\n",
       "      <th>lexical_tok2</th>\n",
       "      <th>lexical_tok3</th>\n",
       "      <th>lexical_lem2</th>\n",
       "      <th>lexical_lem3</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.757362</td>\n",
       "      <td>0.317063</td>\n",
       "      <td>0.764680</td>\n",
       "      <td>0.408463</td>\n",
       "      <td>0.099848</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.186072</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.840291</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>0.873396</td>\n",
       "      <td>0.568983</td>\n",
       "      <td>0.356901</td>\n",
       "      <td>0.096535</td>\n",
       "      <td>0.435932</td>\n",
       "      <td>0.128407</td>\n",
       "      <td>time202-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.815473</td>\n",
       "      <td>0.465478</td>\n",
       "      <td>0.834075</td>\n",
       "      <td>0.514817</td>\n",
       "      <td>0.311076</td>\n",
       "      <td>0.080344</td>\n",
       "      <td>0.360760</td>\n",
       "      <td>0.092467</td>\n",
       "      <td>time191-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.833976</td>\n",
       "      <td>0.456512</td>\n",
       "      <td>0.855974</td>\n",
       "      <td>0.507570</td>\n",
       "      <td>0.352856</td>\n",
       "      <td>0.121858</td>\n",
       "      <td>0.401046</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>time209-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.847184</td>\n",
       "      <td>0.447043</td>\n",
       "      <td>0.857351</td>\n",
       "      <td>0.532719</td>\n",
       "      <td>0.192589</td>\n",
       "      <td>0.054908</td>\n",
       "      <td>0.306480</td>\n",
       "      <td>0.073248</td>\n",
       "      <td>time210-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.835725</td>\n",
       "      <td>0.500279</td>\n",
       "      <td>0.865767</td>\n",
       "      <td>0.616747</td>\n",
       "      <td>0.313545</td>\n",
       "      <td>0.054253</td>\n",
       "      <td>0.367784</td>\n",
       "      <td>0.075877</td>\n",
       "      <td>time204-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.717362</td>\n",
       "      <td>0.318379</td>\n",
       "      <td>0.777429</td>\n",
       "      <td>0.455681</td>\n",
       "      <td>0.163734</td>\n",
       "      <td>0.040834</td>\n",
       "      <td>0.229057</td>\n",
       "      <td>0.040508</td>\n",
       "      <td>time196-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.835617</td>\n",
       "      <td>0.504704</td>\n",
       "      <td>0.870079</td>\n",
       "      <td>0.535710</td>\n",
       "      <td>0.285261</td>\n",
       "      <td>0.091068</td>\n",
       "      <td>0.316627</td>\n",
       "      <td>0.099707</td>\n",
       "      <td>time203-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.784656</td>\n",
       "      <td>0.407821</td>\n",
       "      <td>0.833618</td>\n",
       "      <td>0.539878</td>\n",
       "      <td>0.320858</td>\n",
       "      <td>0.109847</td>\n",
       "      <td>0.384379</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>time208-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.825618</td>\n",
       "      <td>0.431302</td>\n",
       "      <td>0.888448</td>\n",
       "      <td>0.554301</td>\n",
       "      <td>0.187845</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>0.231621</td>\n",
       "      <td>0.030184</td>\n",
       "      <td>time205-cond1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   syntax_penn_tok2  syntax_penn_tok3  syntax_penn_lem2  syntax_penn_lem3  \\\n",
       "0          0.757362          0.317063          0.764680          0.408463   \n",
       "1          0.840291          0.465278          0.873396          0.568983   \n",
       "2          0.815473          0.465478          0.834075          0.514817   \n",
       "3          0.833976          0.456512          0.855974          0.507570   \n",
       "4          0.847184          0.447043          0.857351          0.532719   \n",
       "5          0.835725          0.500279          0.865767          0.616747   \n",
       "6          0.717362          0.318379          0.777429          0.455681   \n",
       "7          0.835617          0.504704          0.870079          0.535710   \n",
       "8          0.784656          0.407821          0.833618          0.539878   \n",
       "9          0.825618          0.431302          0.888448          0.554301   \n",
       "\n",
       "   lexical_tok2  lexical_tok3  lexical_lem2  lexical_lem3     condition_info  \n",
       "0      0.099848      0.021056      0.186072      0.027283  time197-cond1.txt  \n",
       "1      0.356901      0.096535      0.435932      0.128407  time202-cond1.txt  \n",
       "2      0.311076      0.080344      0.360760      0.092467  time191-cond1.txt  \n",
       "3      0.352856      0.121858      0.401046      0.130100  time209-cond1.txt  \n",
       "4      0.192589      0.054908      0.306480      0.073248  time210-cond1.txt  \n",
       "5      0.313545      0.054253      0.367784      0.075877  time204-cond1.txt  \n",
       "6      0.163734      0.040834      0.229057      0.040508  time196-cond1.txt  \n",
       "7      0.285261      0.091068      0.316627      0.099707  time203-cond1.txt  \n",
       "8      0.320858      0.109847      0.384379      0.129500  time208-cond1.txt  \n",
       "9      0.187845      0.022437      0.231621      0.030184  time205-cond1.txt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_real.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run surrogate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickduran/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:139: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/Users/nickduran/opt/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time199A-time195B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time195A-time210B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time208A-time204B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time198A-time197B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time210A-time195B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time197A-time206B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time199A-time210B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time202A-time201B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time202A-time204B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time197A-time198B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time206A-time197B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time204A-time202B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time191A-time208B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time196A-time208B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time201A-time202B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time208A-time196B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time195A-time199B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time208A-time191B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time204A-time208B-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/surrogate/surrogate_run-1578946869.1576588/SurrogatePair-time210A-time199B-cond1.txt\n"
     ]
    }
   ],
   "source": [
    "# start_phase2real = time.time() # <<< this looks wrong\n",
    "[turn_surrogate,convo_surrogate] = calculate_baseline_alignment(\n",
    "                                    input_files=INPUT_FILES, \n",
    "                                    maxngram=MAXNGRAM,\n",
    "                                    use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                                    pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                                    semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                                    output_file_directory=ANALYSIS_READY,\n",
    "                                    add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                                    ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                                    high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                                    low_n_cutoff=LOW_N_CUTOFF,\n",
    "                                    surrogate_file_directory=SURROGATE_TRANSCRIPTS,\n",
    "                                    all_surrogates=False,\n",
    "                                    keep_original_turn_order=True,\n",
    "                                    ## >> This is where things can get screwy if not set up correctly for each corpus\n",
    "                                    id_separator='\\-',\n",
    "                                    dyad_label='time',\n",
    "                                    condition_label='cond')\n",
    "# end_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>syntax_penn_tok2</th>\n",
       "      <th>syntax_penn_tok3</th>\n",
       "      <th>syntax_penn_lem2</th>\n",
       "      <th>syntax_penn_lem3</th>\n",
       "      <th>lexical_tok2</th>\n",
       "      <th>lexical_tok3</th>\n",
       "      <th>lexical_lem2</th>\n",
       "      <th>lexical_lem3</th>\n",
       "      <th>cosine_semanticL</th>\n",
       "      <th>partner_direction</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.173897</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313822</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.560767</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.187882</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230933</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135741</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.269408</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0944911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167692</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.115728</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.294807</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.228085</td>\n",
       "      <td>cgv&gt;kid</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0778499</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0731272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.61994</td>\n",
       "      <td>kid&gt;cgv</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time syntax_penn_tok2 syntax_penn_tok3 syntax_penn_lem2 syntax_penn_lem3  \\\n",
       "0     0         0.288675                0         0.288675                0   \n",
       "1     1                0                0                0                0   \n",
       "2     2                0                0                0                0   \n",
       "3     3                0                0                0                0   \n",
       "4     4                0                0                0                0   \n",
       "5     5                0                0                0                0   \n",
       "6     6         0.269408                0        0.0944911                0   \n",
       "7     7                0                0         0.115728                0   \n",
       "8     8                0                0         0.140028                0   \n",
       "9     9        0.0778499                0        0.0731272                0   \n",
       "\n",
       "  lexical_tok2 lexical_tok3 lexical_lem2 lexical_lem3 cosine_semanticL  \\\n",
       "0            0            0            0            0         0.173897   \n",
       "1            0            0            0            0         0.313822   \n",
       "2            0            0            0            0         0.560767   \n",
       "3            0            0            0            0         0.187882   \n",
       "4            0            0            0            0         0.230933   \n",
       "5            0            0            0            0         0.135741   \n",
       "6            0            0            0            0         0.167692   \n",
       "7            0            0            0            0         0.294807   \n",
       "8            0            0            0            0         0.228085   \n",
       "9            0            0            0            0          0.61994   \n",
       "\n",
       "  partner_direction         condition_info  \n",
       "0           cgv>kid  time199-time195-cond1  \n",
       "1           kid>cgv  time199-time195-cond1  \n",
       "2           cgv>kid  time199-time195-cond1  \n",
       "3           kid>cgv  time199-time195-cond1  \n",
       "4           cgv>kid  time199-time195-cond1  \n",
       "5           kid>cgv  time199-time195-cond1  \n",
       "6           cgv>kid  time199-time195-cond1  \n",
       "7           kid>cgv  time199-time195-cond1  \n",
       "8           cgv>kid  time199-time195-cond1  \n",
       "9           kid>cgv  time199-time195-cond1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_surrogate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>syntax_penn_tok2</th>\n",
       "      <th>syntax_penn_tok3</th>\n",
       "      <th>syntax_penn_lem2</th>\n",
       "      <th>syntax_penn_lem3</th>\n",
       "      <th>lexical_tok2</th>\n",
       "      <th>lexical_tok3</th>\n",
       "      <th>lexical_lem2</th>\n",
       "      <th>lexical_lem3</th>\n",
       "      <th>condition_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.706511</td>\n",
       "      <td>0.328459</td>\n",
       "      <td>0.762799</td>\n",
       "      <td>0.395685</td>\n",
       "      <td>0.124388</td>\n",
       "      <td>0.028448</td>\n",
       "      <td>0.184657</td>\n",
       "      <td>0.034365</td>\n",
       "      <td>time199-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.789731</td>\n",
       "      <td>0.396927</td>\n",
       "      <td>0.849894</td>\n",
       "      <td>0.480171</td>\n",
       "      <td>0.146808</td>\n",
       "      <td>0.017779</td>\n",
       "      <td>0.221491</td>\n",
       "      <td>0.031007</td>\n",
       "      <td>time195-time210-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.731604</td>\n",
       "      <td>0.303947</td>\n",
       "      <td>0.746011</td>\n",
       "      <td>0.395191</td>\n",
       "      <td>0.149313</td>\n",
       "      <td>0.035518</td>\n",
       "      <td>0.198159</td>\n",
       "      <td>0.049023</td>\n",
       "      <td>time208-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.628028</td>\n",
       "      <td>0.249044</td>\n",
       "      <td>0.711974</td>\n",
       "      <td>0.374271</td>\n",
       "      <td>0.070131</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.126820</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>time198-time197-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.778837</td>\n",
       "      <td>0.444004</td>\n",
       "      <td>0.829510</td>\n",
       "      <td>0.547978</td>\n",
       "      <td>0.110791</td>\n",
       "      <td>0.016825</td>\n",
       "      <td>0.189151</td>\n",
       "      <td>0.025922</td>\n",
       "      <td>time210-time195-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.742307</td>\n",
       "      <td>0.372784</td>\n",
       "      <td>0.740943</td>\n",
       "      <td>0.409181</td>\n",
       "      <td>0.116393</td>\n",
       "      <td>0.014147</td>\n",
       "      <td>0.144304</td>\n",
       "      <td>0.020621</td>\n",
       "      <td>time197-time206-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.722624</td>\n",
       "      <td>0.336317</td>\n",
       "      <td>0.775274</td>\n",
       "      <td>0.460376</td>\n",
       "      <td>0.118675</td>\n",
       "      <td>0.030476</td>\n",
       "      <td>0.204248</td>\n",
       "      <td>0.051044</td>\n",
       "      <td>time199-time210-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.783974</td>\n",
       "      <td>0.425876</td>\n",
       "      <td>0.842856</td>\n",
       "      <td>0.538761</td>\n",
       "      <td>0.132311</td>\n",
       "      <td>0.014345</td>\n",
       "      <td>0.192521</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>time202-time201-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.768120</td>\n",
       "      <td>0.426555</td>\n",
       "      <td>0.808444</td>\n",
       "      <td>0.526934</td>\n",
       "      <td>0.259743</td>\n",
       "      <td>0.049913</td>\n",
       "      <td>0.317315</td>\n",
       "      <td>0.075054</td>\n",
       "      <td>time202-time204-cond1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.757926</td>\n",
       "      <td>0.298216</td>\n",
       "      <td>0.787155</td>\n",
       "      <td>0.354602</td>\n",
       "      <td>0.093848</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>0.126936</td>\n",
       "      <td>0.013535</td>\n",
       "      <td>time197-time198-cond1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   syntax_penn_tok2  syntax_penn_tok3  syntax_penn_lem2  syntax_penn_lem3  \\\n",
       "0          0.706511          0.328459          0.762799          0.395685   \n",
       "1          0.789731          0.396927          0.849894          0.480171   \n",
       "2          0.731604          0.303947          0.746011          0.395191   \n",
       "3          0.628028          0.249044          0.711974          0.374271   \n",
       "4          0.778837          0.444004          0.829510          0.547978   \n",
       "5          0.742307          0.372784          0.740943          0.409181   \n",
       "6          0.722624          0.336317          0.775274          0.460376   \n",
       "7          0.783974          0.425876          0.842856          0.538761   \n",
       "8          0.768120          0.426555          0.808444          0.526934   \n",
       "9          0.757926          0.298216          0.787155          0.354602   \n",
       "\n",
       "   lexical_tok2  lexical_tok3  lexical_lem2  lexical_lem3  \\\n",
       "0      0.124388      0.028448      0.184657      0.034365   \n",
       "1      0.146808      0.017779      0.221491      0.031007   \n",
       "2      0.149313      0.035518      0.198159      0.049023   \n",
       "3      0.070131      0.002362      0.126820      0.004677   \n",
       "4      0.110791      0.016825      0.189151      0.025922   \n",
       "5      0.116393      0.014147      0.144304      0.020621   \n",
       "6      0.118675      0.030476      0.204248      0.051044   \n",
       "7      0.132311      0.014345      0.192521      0.018324   \n",
       "8      0.259743      0.049913      0.317315      0.075054   \n",
       "9      0.093848      0.003947      0.126936      0.013535   \n",
       "\n",
       "          condition_info  \n",
       "0  time199-time195-cond1  \n",
       "1  time195-time210-cond1  \n",
       "2  time208-time204-cond1  \n",
       "3  time198-time197-cond1  \n",
       "4  time210-time195-cond1  \n",
       "5  time197-time206-cond1  \n",
       "6  time199-time210-cond1  \n",
       "7  time202-time201-cond1  \n",
       "8  time202-time204-cond1  \n",
       "9  time197-time198-cond1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_surrogate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
