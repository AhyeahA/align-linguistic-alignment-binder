{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes with Python 3 upgrade: \n",
    "\n",
    "* Initially ran 2to3 function to deal with most issues \n",
    "* Looks like \"string\" library has seen several changes from Python 2 and updates here were needed\n",
    "* issue with `file()` being changed to `open()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General notes on formatting of files/data\n",
    "\n",
    "### Preparing input data\n",
    "\n",
    "* Each input text file needs to contain a single conversation organized in an `N x 2` matrix\n",
    "    * Text file must be tab-delimited.\n",
    "* Each row must correspond to a single conversational turn from a speaker.\n",
    "    * Rows must be temporally ordered based on their occurrence in the conversation.\n",
    "    * Rows must alternate between speakers.\n",
    "* Speaker identifier and content for each turn are divided across two columns.\n",
    "    * Column 1 must have the header `participant`.\n",
    "        * Each cell specifies the speaker.\n",
    "        * Each speaker must have a unique label (e.g., `P1` and `P2`, `0` and `1`).\n",
    "    * Column 2 must have the header `content`.\n",
    "        * Each cell corresponds to the transcribed utterance from the speaker.\n",
    "        * Each cell must end with a newline character: `\\n`\n",
    "* See `examples` directory in Github repository for an example\n",
    "\n",
    "### Filename conventions\n",
    "\n",
    "* Each conversation text file must be regularly formatted, including a prefix for dyad and a prefix for conversation prior to the identifier for each that are separated by a unique character. By default, ALIGN looks for patterns that follow this convention: `dyad1-condA.txt`\n",
    "    * However, users may choose to include any label for dyad or condition so long as the two labels are distinct from one another and are not subsets of any possible dyad or condition labels. Users may also use any character as a separator so long as it does not occur anywhere else in the filename.\n",
    "    * The chosen file format **must** be used when saving **all** files for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import align packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Version Info:\n",
      "0.25.1\n",
      "Numpy Version Info:\n",
      "1.17.2\n",
      "Scipy Version Info:\n",
      "1.3.1\n",
      "NLTK Version Info:\n",
      "3.4.5\n",
      "Gensim Version Info:\n",
      "3.8.0\n",
      "Python and Conda Environment Info:\n",
      "3.7.4 (default, Aug 13 2019, 15:17:50) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nickduran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nickduran/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nickduran/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard libraries\n",
    "import os,re,math,csv,string,random,logging,glob,itertools,operator,sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "print(\"Pandas Version Info:\\n{}\".format(pd.__version__))\n",
    "print(\"Numpy Version Info:\\n{}\".format(np.__version__))\n",
    "print(\"Scipy Version Info:\\n{}\".format(scipy.__version__))\n",
    "print(\"NLTK Version Info:\\n{}\".format(nltk.__version__))\n",
    "print(\"Gensim Version Info:\\n{}\".format(gensim.__version__))\n",
    "print(\"Python and Conda Environment Info:\\n{}\".format(sys.version))\n",
    "\n",
    "# Import additional libraries that are used with the tutorials \n",
    "# >>> NOTE: why are these not downloaded with the align package?\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitialCleanup(dataframe,\n",
    "                   minwords=2,\n",
    "                   use_filler_list=None,\n",
    "                   filler_regex_and_list=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform basic text cleaning to prepare dataframe\n",
    "    for analysis. Remove non-letter/-space characters,\n",
    "    empty turns, turns below a minimum length, and\n",
    "    fillers.\n",
    "\n",
    "    By default, preserves turns 2 words or longer.\n",
    "    If desired, this may be changed by updating the\n",
    "    `minwords` argument.\n",
    "\n",
    "    By default, remove common fillers through regex.\n",
    "    If desired, remove other words by passing a list\n",
    "    of literal strings to `use_filler_list` argument,\n",
    "    and if both regex and list of additional literal\n",
    "    strings are to be used, update `filler_regex_and_list=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    # only allow strings, spaces, and newlines to pass\n",
    "    WHITELIST = string.ascii_letters + '\\'' + ' '\n",
    "\n",
    "    # remove inadvertent empty turns\n",
    "    dataframe = dataframe[pd.notnull(dataframe['content'])]\n",
    "\n",
    "    # internal function: remove fillers via regular expressions\n",
    "    def applyRegExpression(textFiller):\n",
    "        textClean = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\\s', ' ', textFiller) # at the start of a string\n",
    "        textClean = re.sub('\\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\\s', ' ', textClean) # within a string\n",
    "        textClean = re.sub('\\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', textClean) # end of a string\n",
    "        textClean = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', textClean) # if entire turn string\n",
    "        return textClean\n",
    "\n",
    "    # create a new column with only approved text before cleaning per user-specified settings\n",
    "    dataframe['clean_content'] = dataframe['content'].apply(lambda utterance: ''.join([char for char in utterance if char in WHITELIST]).lower())\n",
    "\n",
    "    # DEFAULT: remove typical speech fillers via regular expressions (examples: \"um, mm, oh, hm, uh, ha\")\n",
    "    if use_filler_list is None and not filler_regex_and_list:\n",
    "        dataframe['clean_content'] = dataframe['clean_content'].apply(applyRegExpression)\n",
    "\n",
    "    # OPTION 1: remove speech fillers or other words specified by user in a list\n",
    "    elif use_filler_list is not None and not filler_regex_and_list:\n",
    "        dataframe['clean_content'] = dataframe['clean_content'].apply(lambda utterance: ' '.join([word for word in utterance.split(\" \") if word not in use_filler_list]))\n",
    "\n",
    "    # OPTION 2: remove speech fillers via regular expression and any additional words from user-specified list\n",
    "    elif use_filler_list is not None and filler_regex_and_list:\n",
    "        dataframe['clean_content'] = dataframe['clean_content'].apply(applyRegExpression)\n",
    "        dataframe['clean_content'] = dataframe['clean_content'].apply(lambda utterance: ' '.join([word for word in utterance.split(\" \") if word not in use_filler_list]))\n",
    "        cleantext = \" \".join(cleantext)\n",
    "\n",
    "    # OPTION 3: nothing is filtered\n",
    "    else:\n",
    "        dataframe['clean_content'] = dataframe['clean_content']\n",
    "\n",
    "    # drop the old \"content\" column and rename the clean \"content\" column\n",
    "    dataframe = dataframe.drop(['content'],axis=1)\n",
    "    dataframe = dataframe.rename(index=str,\n",
    "                                 columns ={'clean_content': 'content'})\n",
    "\n",
    "    # remove rows that are now blank or do not meet `minwords` requirement, then drop length column\n",
    "    dataframe['utteranceLen'] = dataframe['content'].apply(lambda x: word_tokenize(x)).str.len()\n",
    "    dataframe = dataframe.drop(dataframe[dataframe.utteranceLen < int(minwords)].index).drop(['utteranceLen'],axis=1)\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "\n",
    "    # return the cleaned dataframe\n",
    "    return dataframe\n",
    "\n",
    "def AdjacentMerge(dataframe):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a dataframe of conversation turns,\n",
    "    merge adjacent turns by the same speaker.\n",
    "    \"\"\"\n",
    "\n",
    "    repeat=1\n",
    "    while repeat==1:\n",
    "        l1=len(dataframe)\n",
    "        DfMerge = []\n",
    "        k = 0\n",
    "        if len(dataframe) > 0:\n",
    "            while k < len(dataframe)-1:\n",
    "                if dataframe['participant'].iloc[k] != dataframe['participant'].iloc[k+1]:\n",
    "                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])\n",
    "                    k = k + 1\n",
    "                elif dataframe['participant'].iloc[k] == dataframe['participant'].iloc[k+1]:\n",
    "                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k] + \" \" + dataframe['content'].iloc[k+1]])\n",
    "                    k = k + 2\n",
    "            if k == len(dataframe)-1:\n",
    "                DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])\n",
    "\n",
    "        dataframe=pd.DataFrame(DfMerge,columns=('participant','content'))\n",
    "        if l1==len(dataframe):\n",
    "            repeat=0\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def Tokenize(text,nwords):\n",
    "    \"\"\"\n",
    "    Given list of text to be processed and a list\n",
    "    of known words, return a list of edited and\n",
    "    tokenized words.\n",
    "\n",
    "    Spell-checking is implemented using a\n",
    "    Bayesian spell-checking algorithm\n",
    "    (http://norvig.com/spell-correct.html).\n",
    "\n",
    "    By default, this is based on the Project Gutenberg\n",
    "    corpus, a collection of approximately 1 million texts\n",
    "    (http://www.gutenberg.org). A copy of this is included\n",
    "    within this package. If desired, users may specify a\n",
    "    different spell-check training corpus in the\n",
    "    `training_dictionary` argument of the\n",
    "    `prepare_transcripts()` function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # internal function: identify possible spelling errors for a given word\n",
    "    def edits1(word):\n",
    "        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes    = [a + b[1:] for a, b in splits if b]\n",
    "        transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "        replaces   = [a + c + b[1:] for a, b in splits for c in string.ascii_lowercase if b]\n",
    "        inserts    = [a + c + b     for a, b in splits for c in string.ascii_lowercase]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    # internal function: identify known edits\n",
    "    def known_edits2(word,nwords):\n",
    "        return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in nwords)\n",
    "\n",
    "    # internal function: identify known words\n",
    "    def known(words,nwords): return set(w for w in words if w in nwords)\n",
    "\n",
    "    # internal function: correct spelling\n",
    "    def correct(word,nwords):\n",
    "        candidates = known([word],nwords) or known(edits1(word),nwords) or known_edits2(word,nwords) or [word]\n",
    "        return max(candidates, key=nwords.get)\n",
    "\n",
    "    # expand out based on a fixed list of common contractions\n",
    "    contract_dict = { \"ain't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he had\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he'll've\": \"he will have\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'd've\": \"i would have\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'll've\": \"i will have\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it'll've\": \"it will have\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she'll've\": \"she will have\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so as\",\n",
    "        \"that'd\": \"that had\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they'll've\": \"they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what'll've\": \"what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who'll've\": \"who will have\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\" }\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(list(contract_dict.keys())))\n",
    "\n",
    "    # internal function:\n",
    "    def expand_contractions(text, contractions_re=contractions_re):\n",
    "        def replace(match):\n",
    "            return contract_dict[match.group(0)]\n",
    "        return contractions_re.sub(replace, text.lower())\n",
    "\n",
    "    # process all words in the text\n",
    "    cleantoken = []\n",
    "    text = expand_contractions(text)\n",
    "    token = word_tokenize(text)\n",
    "    for word in token:\n",
    "        if \"'\" not in word:\n",
    "            cleantoken.append(correct(word,nwords))\n",
    "        else:\n",
    "            cleantoken.append(word)\n",
    "    return cleantoken\n",
    "\n",
    "\n",
    "def pos_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert NLTK default tagger output into a format that Wordnet\n",
    "    can use in order to properly lemmatize the text.\n",
    "    \"\"\"\n",
    "\n",
    "    # create some inner functions for simplicity\n",
    "    def is_noun(tag):\n",
    "        return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    def is_verb(tag):\n",
    "        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    def is_adverb(tag):\n",
    "        return tag in ['RB', 'RBR', 'RBS']\n",
    "    def is_adjective(tag):\n",
    "        return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "    # check each tag against possible categories\n",
    "    if is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "\n",
    "def Lemmatize(tokenlist):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    defaultPos = nltk.pos_tag(tokenlist) # get the POS tags from NLTK default tagger\n",
    "    words_lemma = []\n",
    "    for item in defaultPos:\n",
    "        words_lemma.append(lemmatizer.lemmatize(item[0],pos_to_wn(item[1]))) # need to convert POS tags to a format (NOUN, VERB, ADV, ADJ) that wordnet uses to lemmatize\n",
    "    return words_lemma\n",
    "\n",
    "\n",
    "def ApplyPOSTagging(df,\n",
    "                    filename,\n",
    "                    add_stanford_tags=False,\n",
    "                    stanford_pos_path=None,\n",
    "                    stanford_language_path=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a dataframe of conversation turns, return a new\n",
    "    dataframe with part-of-speech tagging. Add filename\n",
    "    (given as string) as a new column in returned dataframe.\n",
    "\n",
    "    By default, return only tags from the NLTK default POS\n",
    "    tagger. Optionally, also return Stanford POS tagger\n",
    "    results by setting `add_stanford_tags=True`.\n",
    "\n",
    "    If Stanford POS tagging is desired, specify the\n",
    "    location of the Stanford POS tagger with the\n",
    "    `stanford_pos_path` argument. Also note that the\n",
    "    default language model for the Stanford tagger is\n",
    "    English (english-left3words-distsim.tagger). To change\n",
    "    language model, specify the location with the\n",
    "    `stanford_language_path` argument.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # if desired, import Stanford tagger\n",
    "    if add_stanford_tags:\n",
    "        if stanford_pos_path is None or stanford_language_path is None:\n",
    "            raise ValueError('Error! Specify path to Stanford POS tagger and language model using the `stanford_pos_path` and `stanford_language_path` arguments')\n",
    "        else:\n",
    "            stanford_tagger = StanfordPOSTagger(stanford_pos_path + stanford_language_path,\n",
    "                                                stanford_pos_path + 'stanford-postagger.jar')\n",
    "            \n",
    "    # add new columns to dataframe\n",
    "    df['tagged_token'] = df['token'].apply(nltk.pos_tag)\n",
    "    df['tagged_lemma'] = df['lemma'].apply(nltk.pos_tag)\n",
    "\n",
    "    # if desired, also tag with Stanford tagger\n",
    "    if add_stanford_tags:\n",
    "        df['tagged_stan_token'] = df['token'].apply(stanford_tagger.tag)\n",
    "        df['tagged_stan_lemma'] = df['lemma'].apply(stanford_tagger.tag)\n",
    "\n",
    "    df['file'] = filename\n",
    "\n",
    "    # return finished dataframe\n",
    "    return df\n",
    "\n",
    "def prepare_transcripts(input_files,\n",
    "                        output_file_directory,\n",
    "                        training_dictionary=None,\n",
    "                        minwords=2,\n",
    "                        use_filler_list=None,\n",
    "                        filler_regex_and_list=False,\n",
    "                        add_stanford_tags=False,\n",
    "                        stanford_pos_path=None,\n",
    "                        stanford_language_path=None,\n",
    "                        input_as_directory=True,\n",
    "                        save_concatenated_dataframe=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare transcripts for similarity analysis.\n",
    "\n",
    "    Given individual .txt files of conversations,\n",
    "    return a completely prepared dataframe of transcribed\n",
    "    conversations for later ALIGN analysis, including: text\n",
    "    cleaning, merging adjacent turns, spell-checking,\n",
    "    tokenization, lemmatization, and part-of-speech tagging.\n",
    "    The output serve as the input for later ALIGN\n",
    "    analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    input_files : str (directory name) or list of str (file names)\n",
    "        Raw files to be cleaned. Behavior governed by `input_as_directory`\n",
    "        parameter as well.\n",
    "\n",
    "    output_file_directory : str\n",
    "        Name of directory where output for individual conversations will be\n",
    "        saved.\n",
    "\n",
    "    training_dictionary : str, optional (default: None)\n",
    "        Specify whether to train the spell-checking dictionary using a\n",
    "        provided file name (str) or the default Project\n",
    "        Gutenberg corpus [http://www.gutenberg.org] (None).\n",
    "\n",
    "    minwords : int, optional (2)\n",
    "        Specify the minimum number of words in a turn. Any turns with fewer\n",
    "        than the minimum number of words will be removed from the corpus.\n",
    "        (Note: `minwords` must be equal to or greater than `maxngram` provided\n",
    "        to `calculate_alignment()` and `calculate_baseline_alignment` in later\n",
    "        steps.)\n",
    "\n",
    "    use_filler_list : list of str, optional (default: None)\n",
    "        Specify whether words should be filtered from all conversations using a\n",
    "        list of filler words (list of str) or using regular expressions to\n",
    "        filter out common filler words (None). Behavior governed by\n",
    "        `filler_regex_and_list` parameter as well.\n",
    "\n",
    "    filler_regex_and_list : boolean, optional (default: False)\n",
    "        If providing a list to `use_filler_list` parameter, specify whether to\n",
    "        use only the provided list (False) or to use both the provided list and\n",
    "        the regular expression filter (True).\n",
    "\n",
    "    add_stanford_tags : boolean, optional (default: False)\n",
    "        Specify whether to return part-of-speech similarity scores based on\n",
    "        Stanford POS tagger in addition to the Penn POS tagger (True) or to\n",
    "        return only POS similarity scores from the Penn tagger (False). (Note:\n",
    "        Including Stanford POS tags will lead to a significant increase in\n",
    "        processing time.)\n",
    "\n",
    "    stanford_pos_path : str, optional (default: None)\n",
    "        If Stanford POS tagging is desired, specify local path to Stanford POS\n",
    "        tagger.\n",
    "\n",
    "    stanford_language_path : str, optional (default: None)\n",
    "        If Stanford POS tagging is desired, specify local path to Stanford POS\n",
    "        tagger for the desired language (str) or use the default English tagger\n",
    "        (None).\n",
    "\n",
    "    input_as_directory : boolean, optional (default: True)\n",
    "        Specify whether the value passed to `input_files` parameter should\n",
    "        be read as a directory (True) or a list of files to be processed\n",
    "        (False).\n",
    "\n",
    "    save_concatenated_dataframe : boolean, optional (default: True)\n",
    "        Specify whether to save the individual conversation output data only\n",
    "        as individual files in the `output_file_directory` (False) or to save\n",
    "        the individual files as well as a single concatenated dataframe (True).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    prepped_df : Pandas DataFrame\n",
    "        A single concatenated dataframe of all transcripts, ready for\n",
    "        processing with `calculate_alignment()` and\n",
    "        `calculate_baseline_alignment()`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # create an internal function to train the model\n",
    "    def train(features):\n",
    "        model = defaultdict(lambda: 1)\n",
    "        for f in features:\n",
    "            model[f] += 1\n",
    "        return model\n",
    "\n",
    "    # if no training dictionary is specified, use the Gutenberg corpus\n",
    "    if training_dictionary is None:\n",
    "        \n",
    "########### NOTE: for personal use and testing\n",
    "        training_dictionary = os.path.join(BASE_PATH,\n",
    "                                           'dictionary/big.txt') \n",
    "\n",
    "########### NOTE: for upload to Github and pip, this needs to be uncommented       \n",
    "#         # first, get the name of the package directory\n",
    "#         module_path = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "#         # then construct the path to the text file\n",
    "#         training_dictionary = os.path.join(module_path, 'data/gutenberg.txt')\n",
    "###########\n",
    "\n",
    "    # train our spell-checking model\n",
    "    nwords = train(re.findall('[a-z]+', (open(training_dictionary).read().lower())))\n",
    "\n",
    "    # grab the appropriate files\n",
    "    if not input_as_directory:\n",
    "        file_list = glob.glob(input_files)\n",
    "    else:\n",
    "        file_list = glob.glob(input_files+\"/*.txt\")\n",
    "\n",
    "    # cycle through all files\n",
    "    prepped_df = pd.DataFrame()\n",
    "    for fileName in file_list:\n",
    "\n",
    "        # let us know which file we're processing\n",
    "        dataframe = pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        print((\"Processing: \"+fileName))\n",
    "\n",
    "        # clean up, merge, spellcheck, tokenize, lemmatize, and POS-tag\n",
    "        dataframe = InitialCleanup(dataframe,\n",
    "                                   minwords=minwords,\n",
    "                                   use_filler_list=use_filler_list,\n",
    "                                   filler_regex_and_list=filler_regex_and_list)\n",
    "        dataframe = AdjacentMerge(dataframe)\n",
    "\n",
    "        # tokenize and lemmatize\n",
    "        dataframe['token'] = dataframe['content'].apply(Tokenize,\n",
    "                                     args=(nwords,))\n",
    "        dataframe['lemma'] = dataframe['token'].apply(Lemmatize)\n",
    "\n",
    "        # apply part-of-speech tagging\n",
    "        dataframe = ApplyPOSTagging(dataframe,\n",
    "                                    filename=os.path.basename(fileName),\n",
    "                                    add_stanford_tags=add_stanford_tags,\n",
    "                                    stanford_pos_path=stanford_pos_path,\n",
    "                                    stanford_language_path=stanford_language_path)\n",
    "\n",
    "        # export the conversation's dataframe as a CSV\n",
    "        conversation_file = os.path.join(output_file_directory,os.path.basename(fileName))\n",
    "        dataframe.to_csv(conversation_file, encoding='utf-8',index=False,sep='\\t')\n",
    "        prepped_df = prepped_df.append(dataframe)\n",
    "\n",
    "    # save the concatenated dataframe\n",
    "    if save_concatenated_dataframe:\n",
    "        concatenated_file = os.path.join(output_file_directory,'../align_concatenated_dataframe.txt')\n",
    "        prepped_df.to_csv(concatenated_file,\n",
    "                    encoding='utf-8',index=False, sep='\\t')\n",
    "\n",
    "    # return the dataframe\n",
    "    return prepped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't change above as this is the code that is being used in the public version of ALIGN\n",
    "Note that I do toggle between gutenberg and big; gutenberg is in the public version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN PREPARATION PHASE\n",
    "\n",
    "First, we prepare our transcripts by reading in individual `.txt`\n",
    "files for each conversation, clean up undesired text and turns,\n",
    "spell-check, tokenize, lemmatize, and add POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify path settings\n",
    "\n",
    "ALIGN will need to know where the raw transcripts are stored, where to store the processed data, and where to read in any additional files needed for optional ALIGN parameters.\n",
    "\n",
    "Be sure to add the raw data and `stanford-postagger-full-2017-06-09` and `GoogleNews-vectors-negative300.bin` to the `optional_directories` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()\n",
    "\n",
    "CORPUS = os.path.join(BASE_PATH,\n",
    "                              'analysis/CHILDES/')\n",
    "if not os.path.exists(CORPUS):\n",
    "    os.makedirs(CORPUS)\n",
    "\n",
    "TRANSCRIPTS = os.path.join(CORPUS,\n",
    "                                   'raw/')\n",
    "if not os.path.exists(TRANSCRIPTS):\n",
    "    os.makedirs(TRANSCRIPTS)\n",
    "\n",
    "PREPPED_TRANSCRIPTS = os.path.join(CORPUS,\n",
    "                                   'prepped_penn/')\n",
    "if not os.path.exists(PREPPED_TRANSCRIPTS):\n",
    "    os.makedirs(PREPPED_TRANSCRIPTS)\n",
    "\n",
    "ANALYSIS_READY = os.path.join(CORPUS,\n",
    "                              'analysis/')\n",
    "if not os.path.exists(ANALYSIS_READY):\n",
    "    os.makedirs(ANALYSIS_READY)\n",
    "\n",
    "SURROGATE_TRANSCRIPTS = os.path.join(CORPUS,\n",
    "                                     'surrogate/')\n",
    "if not os.path.exists(SURROGATE_TRANSCRIPTS):\n",
    "    os.makedirs(SURROGATE_TRANSCRIPTS)\n",
    "    \n",
    "OPTIONAL_PATHS = os.path.join(BASE_PATH,\n",
    "                             'optional_directories/')\n",
    "if not os.path.exists(OPTIONAL_PATHS):\n",
    "    os.makedirs(OPTIONAL_PATHS)\n",
    "\n",
    "## STANFORD POS TAGGING\n",
    "STANFORD_POS_PATH = os.path.join(OPTIONAL_PATHS,\n",
    "                                 'stanford-postagger-full-2018-10-16/')\n",
    "STANFORD_LANGUAGE = os.path.join('models/english-left3words-distsim.tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/optional_directories/stanford-postagger-full-2018-10-16/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STANFORD_POS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/raw/time197-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/raw/time202-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/raw/time191-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/raw/time209-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/raw/time210-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/raw/time204-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/raw/time196-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/raw/time203-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/raw/time208-cond1.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/align-linguistic-alignment/align_working_python3/align_py3/analysis/CHILDES/raw/time205-cond1.txt\n"
     ]
    }
   ],
   "source": [
    "start_phase1 = time.time()\n",
    "model_store = prepare_transcripts(\n",
    "                        input_files=TRANSCRIPTS,\n",
    "                        output_file_directory=PREPPED_TRANSCRIPTS,\n",
    "                        minwords=2,\n",
    "                        use_filler_list=None,\n",
    "                        training_dictionary=os.path.join(BASE_PATH,'dictionary/big.txt'),\n",
    "                        add_stanford_tags=True,\n",
    "                        ### if you want to run the Stanford POS tagger, be sure to uncomment the next two lines\n",
    "                        stanford_pos_path=STANFORD_POS_PATH,\n",
    "                        stanford_language_path=STANFORD_LANGUAGE,\n",
    "                        ###    \n",
    "                        save_concatenated_dataframe=True)\n",
    "end_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase1 - start_phase1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_store.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
